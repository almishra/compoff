['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1481
19
<class 'numpy.dtype'> float64
1481
train batches:  67  validate samples: 118  test samples: 296
Epoch [0/150], Batch loss: 0.926
Epoch: 0 RMSE:  0.5515196973375213  MAPE: 1.1856498715286283  L2+L1 loss: 0.57
Epoch [1/150], Batch loss: 0.457
Epoch: 1 RMSE:  0.5483453151735677  MAPE: 0.8939444287576434  L2+L1 loss: 0.517
Epoch [2/150], Batch loss: 0.446
Epoch: 2 RMSE:  0.5478144208939865  MAPE: 0.9510658830628852  L2+L1 loss: 0.524
Epoch [3/150], Batch loss: 0.468
Epoch: 3 RMSE:  0.55270238366283  MAPE: 0.7595301558458802  L2+L1 loss: 0.504
Epoch [4/150], Batch loss: 0.441
Epoch: 4 RMSE:  0.5520204342782162  MAPE: 0.7731322941926256  L2+L1 loss: 0.505
Epoch [5/150], Batch loss: 0.47
Epoch: 5 RMSE:  0.5632386417972188  MAPE: 0.627321892039444  L2+L1 loss: 0.5
Epoch [6/150], Batch loss: 0.489
Epoch: 6 RMSE:  0.5484170165073234  MAPE: 0.8895157514367045  L2+L1 loss: 0.517
Epoch [7/150], Batch loss: 0.463
Epoch: 7 RMSE:  0.5498965021682422  MAPE: 0.827753481661796  L2+L1 loss: 0.511
Epoch [8/150], Batch loss: 0.46
Epoch: 8 RMSE:  0.5482795939256769  MAPE: 1.046157555174785  L2+L1 loss: 0.544
Epoch [9/150], Batch loss: 0.472
Epoch: 9 RMSE:  0.54819398185352  MAPE: 1.0391103545874705  L2+L1 loss: 0.542
Epoch [10/150], Batch loss: 0.466
Epoch: 10 RMSE:  0.5478478232521696  MAPE: 0.9965841910027318  L2+L1 loss: 0.533
Epoch [11/150], Batch loss: 0.472
Epoch: 11 RMSE:  0.5500454954992581  MAPE: 1.136031806195185  L2+L1 loss: 0.56
Epoch [12/150], Batch loss: 1.091
Epoch: 12 RMSE:  0.5478483973098148  MAPE: 0.9427905632553363  L2+L1 loss: 0.522
Epoch [13/150], Batch loss: 0.797
Epoch: 13 RMSE:  0.5982607552994651  MAPE: 0.48160730991704365  L2+L1 loss: 0.514
Epoch [14/150], Batch loss: 0.44
Epoch: 14 RMSE:  0.5490325113164295  MAPE: 0.8590864659748373  L2+L1 loss: 0.514
Epoch [15/150], Batch loss: 0.465
Epoch: 15 RMSE:  0.5487402264174521  MAPE: 0.8721984600590494  L2+L1 loss: 0.516
Epoch [16/150], Batch loss: 0.565
Epoch: 16 RMSE:  0.5852148410174898  MAPE: 0.5024249092153583  L2+L1 loss: 0.501
Epoch [17/150], Batch loss: 0.475
Epoch: 17 RMSE:  0.552219445912566  MAPE: 0.769004618475284  L2+L1 loss: 0.504
Epoch [18/150], Batch loss: 0.68
Epoch: 18 RMSE:  0.5485992945222792  MAPE: 1.0682129841605335  L2+L1 loss: 0.548
Epoch [19/150], Batch loss: 0.464
Epoch: 19 RMSE:  0.5478952416868919  MAPE: 0.9345573867779631  L2+L1 loss: 0.52
Epoch [20/150], Batch loss: 0.543
Epoch: 20 RMSE:  14.879644943570138  MAPE: 22.165847679037807  L2+L1 loss: 4.106
Epoch [21/150], Batch loss: 1.249
Epoch: 21 RMSE:  0.5480270528839598  MAPE: 0.9188451210804975  L2+L1 loss: 0.52
Epoch [22/150], Batch loss: 0.59
Epoch: 22 RMSE:  0.5489779101018387  MAPE: 0.861409004683148  L2+L1 loss: 0.515
Epoch [23/150], Batch loss: 0.883
Epoch: 23 RMSE:  0.5493471954783828  MAPE: 0.8465891969412495  L2+L1 loss: 0.513
Epoch [24/150], Batch loss: 0.45
Epoch: 24 RMSE:  0.550250648217668  MAPE: 0.8169240832284438  L2+L1 loss: 0.51
Epoch [25/150], Batch loss: 0.444
Epoch: 25 RMSE:  0.556230490679167  MAPE: 0.7036287572415106  L2+L1 loss: 0.502
Epoch [26/150], Batch loss: 0.458
Epoch: 26 RMSE:  0.5514662732614671  MAPE: 0.7853524363666394  L2+L1 loss: 0.506
Epoch [27/150], Batch loss: 0.471
Epoch: 27 RMSE:  0.5483233854759381  MAPE: 0.8953547508287866  L2+L1 loss: 0.517
Epoch [28/150], Batch loss: 0.545
Epoch: 28 RMSE:  0.5484958122216165  MAPE: 0.8849290940164056  L2+L1 loss: 0.517
Epoch [29/150], Batch loss: 0.459
Epoch: 29 RMSE:  0.5492913866172662  MAPE: 0.848707193408279  L2+L1 loss: 0.513
Epoch [30/150], Batch loss: 0.439
Epoch: 30 RMSE:  0.5478543665930203  MAPE: 0.9415931306555628  L2+L1 loss: 0.522
Epoch [31/150], Batch loss: 0.451
Epoch: 31 RMSE:  0.5479459790890262  MAPE: 0.9277462129992714  L2+L1 loss: 0.52
Epoch [32/150], Batch loss: 0.45
Epoch: 32 RMSE:  0.5477945767002256  MAPE: 0.980190037129828  L2+L1 loss: 0.53
Epoch [33/150], Batch loss: 0.43
Epoch: 33 RMSE:  0.5477934872463529  MAPE: 0.9592511368965395  L2+L1 loss: 0.525
Epoch [34/150], Batch loss: 0.454
Epoch: 34 RMSE:  0.5478433142009225  MAPE: 0.9955880074385108  L2+L1 loss: 0.533
Epoch [35/150], Batch loss: 0.466
Epoch: 35 RMSE:  0.5477920010185944  MAPE: 0.9601200649914635  L2+L1 loss: 0.525
Epoch [36/150], Batch loss: 0.464
Epoch: 36 RMSE:  0.5479050958779356  MAPE: 0.9331041063362957  L2+L1 loss: 0.52
Epoch [37/150], Batch loss: 0.467
Epoch: 37 RMSE:  0.5480356471890001  MAPE: 0.9179957647277754  L2+L1 loss: 0.52
Epoch [38/150], Batch loss: 0.466
Epoch: 38 RMSE:  0.5481072847227881  MAPE: 0.9114163470275215  L2+L1 loss: 0.519
Epoch [39/150], Batch loss: 0.611
Epoch: 39 RMSE:  0.5477853853428412  MAPE: 0.9726889722411095  L2+L1 loss: 0.528
Epoch [40/150], Batch loss: 0.456
Epoch: 40 RMSE:  0.5480862526526009  MAPE: 0.9132648109466946  L2+L1 loss: 0.519
Epoch [41/150], Batch loss: 0.464
Epoch: 41 RMSE:  0.5479900411470294  MAPE: 1.018615495200687  L2+L1 loss: 0.538
Epoch [42/150], Batch loss: 0.638
Epoch: 42 RMSE:  0.5478285321952632  MAPE: 0.9472440968152583  L2+L1 loss: 0.523
Epoch [43/150], Batch loss: 0.453
Epoch: 43 RMSE:  0.5478492691995691  MAPE: 0.9426122798842581  L2+L1 loss: 0.522
Epoch [44/150], Batch loss: 0.469
Epoch: 44 RMSE:  0.5481420892607817  MAPE: 0.9085244548559577  L2+L1 loss: 0.519
Epoch [45/150], Batch loss: 0.43
Epoch: 45 RMSE:  0.5479542712209964  MAPE: 1.0141003613029862  L2+L1 loss: 0.537
Epoch [46/150], Batch loss: 0.469
Epoch: 46 RMSE:  0.547982072486962  MAPE: 1.0176459643905291  L2+L1 loss: 0.538
Epoch [47/150], Batch loss: 0.468
Epoch: 47 RMSE:  0.5478030768628597  MAPE: 0.954890098767776  L2+L1 loss: 0.524
Epoch [48/150], Batch loss: 0.455
Epoch: 48 RMSE:  0.548334913920108  MAPE: 1.050390413825226  L2+L1 loss: 0.544
Epoch [49/150], Batch loss: 0.466
Epoch: 49 RMSE:  0.6513071119322251  MAPE: 0.935021031503349  L2+L1 loss: 0.502
Epoch [50/150], Batch loss: 0.505
Epoch: 50 RMSE:  0.5478245180573088  MAPE: 0.9482596018869975  L2+L1 loss: 0.523
Epoch [51/150], Batch loss: 0.465
Epoch: 51 RMSE:  0.5477958939295512  MAPE: 0.9579902691561845  L2+L1 loss: 0.525
Epoch [52/150], Batch loss: 0.477
Epoch: 52 RMSE:  0.5477923379682341  MAPE: 0.9789167214588614  L2+L1 loss: 0.529
Epoch [53/150], Batch loss: 0.467
Epoch: 53 RMSE:  0.5480564407886298  MAPE: 0.9159989640101147  L2+L1 loss: 0.519
Epoch [54/150], Batch loss: 0.799
Epoch: 54 RMSE:  1.743038240337029  MAPE: 3.8051845781920566  L2+L1 loss: 1.59
Epoch [55/150], Batch loss: 1.04
Epoch: 55 RMSE:  0.34542030308365557  MAPE: 0.6769315510728064  L2+L1 loss: 0.39
Epoch [56/150], Batch loss: 0.266
Epoch: 56 RMSE:  0.10707520303074419  MAPE: 0.2566765697296754  L2+L1 loss: 0.261
Epoch [57/150], Batch loss: 0.09
Epoch: 57 RMSE:  0.09793522988508602  MAPE: 0.20234975942333447  L2+L1 loss: 0.231
Epoch [58/150], Batch loss: 0.097
Epoch: 58 RMSE:  0.18125678747447832  MAPE: 0.2632062489546217  L2+L1 loss: 0.272
Epoch [59/150], Batch loss: 0.123
Epoch: 59 RMSE:  0.10438582245050673  MAPE: 0.21721320646455267  L2+L1 loss: 0.251
Epoch [60/150], Batch loss: 0.072
Epoch: 60 RMSE:  0.09351956557201223  MAPE: 0.20968019585446504  L2+L1 loss: 0.241
Epoch [61/150], Batch loss: 0.07
Epoch: 61 RMSE:  0.0992001523837821  MAPE: 0.20509386328524368  L2+L1 loss: 0.239
Epoch [62/150], Batch loss: 0.071
Epoch: 62 RMSE:  0.10005360998622871  MAPE: 0.2069514482265749  L2+L1 loss: 0.242
Epoch [63/150], Batch loss: 0.074
Epoch: 63 RMSE:  0.09226206911624335  MAPE: 0.2055050706665272  L2+L1 loss: 0.236
Epoch [64/150], Batch loss: 0.071
Epoch: 64 RMSE:  0.08940001919343799  MAPE: 0.20913905306459088  L2+L1 loss: 0.238
Epoch [65/150], Batch loss: 0.07
Epoch: 65 RMSE:  0.09973431125099  MAPE: 0.20247622452857506  L2+L1 loss: 0.241
Epoch [66/150], Batch loss: 0.069
Epoch: 66 RMSE:  0.08970951451647209  MAPE: 0.20472650608866555  L2+L1 loss: 0.238
Epoch [67/150], Batch loss: 0.069
Epoch: 67 RMSE:  0.09364630392108185  MAPE: 0.20884802955456708  L2+L1 loss: 0.242
Epoch [68/150], Batch loss: 0.071
Epoch: 68 RMSE:  0.09779498631046528  MAPE: 0.21488803235720125  L2+L1 loss: 0.239
Epoch [69/150], Batch loss: 0.072
Epoch: 69 RMSE:  0.08918132411229182  MAPE: 0.2041892570059464  L2+L1 loss: 0.231
Epoch [70/150], Batch loss: 0.069
Epoch: 70 RMSE:  0.10128144560493675  MAPE: 0.1985346285046042  L2+L1 loss: 0.242
Epoch [71/150], Batch loss: 0.074
Epoch: 71 RMSE:  0.09470902314933748  MAPE: 0.21256774452468688  L2+L1 loss: 0.242
Epoch [72/150], Batch loss: 0.074
Epoch: 72 RMSE:  0.09711214542010556  MAPE: 0.2065466718233413  L2+L1 loss: 0.247
Epoch [73/150], Batch loss: 0.069
Epoch: 73 RMSE:  0.12130315224581027  MAPE: 0.2150088936671977  L2+L1 loss: 0.255
Epoch [74/150], Batch loss: 0.073
Epoch: 74 RMSE:  0.10280984831121943  MAPE: 0.19496831479559829  L2+L1 loss: 0.242
Epoch [75/150], Batch loss: 0.072
Epoch: 75 RMSE:  0.11081447784205335  MAPE: 0.1966819176745313  L2+L1 loss: 0.242
Epoch [76/150], Batch loss: 0.072
Epoch: 76 RMSE:  0.09211947099624655  MAPE: 0.19818423769957869  L2+L1 loss: 0.238
Epoch [77/150], Batch loss: 0.073
Epoch: 77 RMSE:  0.09408079612465997  MAPE: 0.19612597344097224  L2+L1 loss: 0.235
Epoch [78/150], Batch loss: 0.072
Epoch: 78 RMSE:  0.11337333295843485  MAPE: 0.19147519103467758  L2+L1 loss: 0.237
Epoch [79/150], Batch loss: 0.068
Epoch: 79 RMSE:  0.08373740729106624  MAPE: 0.19651690856365234  L2+L1 loss: 0.232
Epoch [80/150], Batch loss: 0.069
Epoch: 80 RMSE:  0.09090858874258076  MAPE: 0.19787759260850663  L2+L1 loss: 0.232
Epoch [81/150], Batch loss: 0.07
Epoch: 81 RMSE:  0.0947071519396407  MAPE: 0.21111335935907802  L2+L1 loss: 0.244
Epoch [82/150], Batch loss: 0.069
Epoch: 82 RMSE:  0.09427393458006264  MAPE: 0.19896944467723732  L2+L1 loss: 0.241
Epoch [83/150], Batch loss: 0.069
Epoch: 83 RMSE:  0.08553760751621453  MAPE: 0.20943097841516442  L2+L1 loss: 0.233
Epoch [84/150], Batch loss: 0.068
Epoch: 84 RMSE:  0.08449785898251104  MAPE: 0.1900195656259049  L2+L1 loss: 0.225
Epoch [85/150], Batch loss: 0.067
Epoch: 85 RMSE:  0.10823456865168044  MAPE: 0.19884990883267012  L2+L1 loss: 0.239
Epoch [86/150], Batch loss: 0.068
Epoch: 86 RMSE:  0.09147269327967669  MAPE: 0.19709504944867384  L2+L1 loss: 0.235
Epoch [87/150], Batch loss: 0.068
Epoch: 87 RMSE:  0.08656289387641207  MAPE: 0.20694320271239308  L2+L1 loss: 0.235
Epoch [88/150], Batch loss: 0.07
Epoch: 88 RMSE:  0.08391973771546586  MAPE: 0.191614308085108  L2+L1 loss: 0.227
Epoch [89/150], Batch loss: 0.068
Epoch: 89 RMSE:  0.10873598798176165  MAPE: 0.2183214334722733  L2+L1 loss: 0.252
Epoch [90/150], Batch loss: 0.069
Epoch: 90 RMSE:  0.0947988031417834  MAPE: 0.18949628297144663  L2+L1 loss: 0.233
Epoch [91/150], Batch loss: 0.065
Epoch: 91 RMSE:  0.09087927316541573  MAPE: 0.18822385507912334  L2+L1 loss: 0.232
Epoch [92/150], Batch loss: 0.064
Epoch: 92 RMSE:  0.09045630038050516  MAPE: 0.18723216970132223  L2+L1 loss: 0.232
Epoch [93/150], Batch loss: 0.064
Epoch: 93 RMSE:  0.09033225462433855  MAPE: 0.1869254774866949  L2+L1 loss: 0.232
Epoch [94/150], Batch loss: 0.065
Epoch: 94 RMSE:  0.09063137740880804  MAPE: 0.18758102385258846  L2+L1 loss: 0.232
Epoch [95/150], Batch loss: 0.065
Epoch: 95 RMSE:  0.09117158004153822  MAPE: 0.1847287487569229  L2+L1 loss: 0.231
Epoch [96/150], Batch loss: 0.065
Epoch: 96 RMSE:  0.08980522570215459  MAPE: 0.18938185221479395  L2+L1 loss: 0.232
Epoch [97/150], Batch loss: 0.065
Epoch: 97 RMSE:  0.0887803525958007  MAPE: 0.18857584695385862  L2+L1 loss: 0.231
Epoch [98/150], Batch loss: 0.065
Epoch: 98 RMSE:  0.08878263500123974  MAPE: 0.19030921032799228  L2+L1 loss: 0.232
Epoch [99/150], Batch loss: 0.064
Epoch: 99 RMSE:  0.08786251301744723  MAPE: 0.18779834071004725  L2+L1 loss: 0.23
Epoch [100/150], Batch loss: 0.064
Epoch: 100 RMSE:  0.08739147444016274  MAPE: 0.1875989409470063  L2+L1 loss: 0.231
Epoch [101/150], Batch loss: 0.063
Epoch: 101 RMSE:  0.08995888363980042  MAPE: 0.1890282626447128  L2+L1 loss: 0.233
Epoch [102/150], Batch loss: 0.064
Epoch: 102 RMSE:  0.0893412983320116  MAPE: 0.18718096707830278  L2+L1 loss: 0.231
Epoch [103/150], Batch loss: 0.064
Epoch: 103 RMSE:  0.08650006310449249  MAPE: 0.18902822691773905  L2+L1 loss: 0.231
Epoch [104/150], Batch loss: 0.063
Epoch: 104 RMSE:  0.08809141364063794  MAPE: 0.18570291942661532  L2+L1 loss: 0.23
Epoch [105/150], Batch loss: 0.064
Epoch: 105 RMSE:  0.0891414165741001  MAPE: 0.18475823757149828  L2+L1 loss: 0.23
Epoch [106/150], Batch loss: 0.064
Epoch: 106 RMSE:  0.0864065493734236  MAPE: 0.18819093411524865  L2+L1 loss: 0.231
Epoch [107/150], Batch loss: 0.064
Epoch: 107 RMSE:  0.08912438063518434  MAPE: 0.18771320462328905  L2+L1 loss: 0.232
Epoch [108/150], Batch loss: 0.064
Epoch: 108 RMSE:  0.08820559146583176  MAPE: 0.18621050420264704  L2+L1 loss: 0.231
Epoch [109/150], Batch loss: 0.063
Epoch: 109 RMSE:  0.08883935591488172  MAPE: 0.18681914671846117  L2+L1 loss: 0.231
Epoch [110/150], Batch loss: 0.064
Epoch: 110 RMSE:  0.08985859517757379  MAPE: 0.18681537506834275  L2+L1 loss: 0.232
Epoch [111/150], Batch loss: 0.064
Epoch: 111 RMSE:  0.0880523125857947  MAPE: 0.18513628075621744  L2+L1 loss: 0.23
Epoch [112/150], Batch loss: 0.063
Epoch: 112 RMSE:  0.08700934908340618  MAPE: 0.1871145933684059  L2+L1 loss: 0.23
Epoch [113/150], Batch loss: 0.064
Epoch: 113 RMSE:  0.08910569565371712  MAPE: 0.18676301381587407  L2+L1 loss: 0.231
Epoch [114/150], Batch loss: 0.064
Epoch: 114 RMSE:  0.08745131782696883  MAPE: 0.18558803370366875  L2+L1 loss: 0.23
Epoch [115/150], Batch loss: 0.064
Epoch: 115 RMSE:  0.08894456897688073  MAPE: 0.18739770054054575  L2+L1 loss: 0.232
Epoch [116/150], Batch loss: 0.064
Epoch: 116 RMSE:  0.08798115096667068  MAPE: 0.18816291107941088  L2+L1 loss: 0.231
Epoch [117/150], Batch loss: 0.064
Epoch: 117 RMSE:  0.08693795226087156  MAPE: 0.18303526623683225  L2+L1 loss: 0.229
Epoch [118/150], Batch loss: 0.064
Epoch: 118 RMSE:  0.08568744606394771  MAPE: 0.18766654436404887  L2+L1 loss: 0.23
Epoch [119/150], Batch loss: 0.064
Epoch: 119 RMSE:  0.08618086881282386  MAPE: 0.1864577460107339  L2+L1 loss: 0.23
Epoch [120/150], Batch loss: 0.063
Epoch: 120 RMSE:  0.08650518183041991  MAPE: 0.18666030733644018  L2+L1 loss: 0.231
Epoch [121/150], Batch loss: 0.063
Epoch: 121 RMSE:  0.08665587324244278  MAPE: 0.1870789273813074  L2+L1 loss: 0.231
Epoch [122/150], Batch loss: 0.063
Epoch: 122 RMSE:  0.08682677147106863  MAPE: 0.18695085047095758  L2+L1 loss: 0.231
Epoch [123/150], Batch loss: 0.064
Epoch: 123 RMSE:  0.08710780302899704  MAPE: 0.18713031273009278  L2+L1 loss: 0.231
Epoch [124/150], Batch loss: 0.064
Epoch: 124 RMSE:  0.08688529157260692  MAPE: 0.18681307099029662  L2+L1 loss: 0.231
Epoch [125/150], Batch loss: 0.064
Epoch: 125 RMSE:  0.08734253026602921  MAPE: 0.1870543926984979  L2+L1 loss: 0.231
Epoch [126/150], Batch loss: 0.064
Epoch: 126 RMSE:  0.087151876397493  MAPE: 0.18711174857792126  L2+L1 loss: 0.231
Epoch [127/150], Batch loss: 0.063
Epoch: 127 RMSE:  0.08727652293292262  MAPE: 0.18688142825614562  L2+L1 loss: 0.231
Epoch [128/150], Batch loss: 0.063
Epoch: 128 RMSE:  0.08731509603400969  MAPE: 0.1868624628769052  L2+L1 loss: 0.231
Epoch [129/150], Batch loss: 0.063
Epoch: 129 RMSE:  0.08739094361555164  MAPE: 0.18675903160725715  L2+L1 loss: 0.231
Epoch [130/150], Batch loss: 0.063
Epoch: 130 RMSE:  0.08755776868028055  MAPE: 0.18704326725640985  L2+L1 loss: 0.231
Epoch [131/150], Batch loss: 0.063
Epoch: 131 RMSE:  0.08765912085241111  MAPE: 0.18684512333870013  L2+L1 loss: 0.231
Epoch [132/150], Batch loss: 0.063
Epoch: 132 RMSE:  0.0873965714204391  MAPE: 0.1867908633388051  L2+L1 loss: 0.231
Epoch [133/150], Batch loss: 0.063
Epoch: 133 RMSE:  0.08747079467838696  MAPE: 0.18683012270335234  L2+L1 loss: 0.231
Epoch [134/150], Batch loss: 0.063
Epoch: 134 RMSE:  0.08747071448963419  MAPE: 0.18691003591241753  L2+L1 loss: 0.231
Epoch [135/150], Batch loss: 0.063
Epoch: 135 RMSE:  0.08741635208897341  MAPE: 0.18680943561525168  L2+L1 loss: 0.231
Epoch [136/150], Batch loss: 0.063
Epoch: 136 RMSE:  0.08751969596097778  MAPE: 0.18678030140149796  L2+L1 loss: 0.231
Epoch [137/150], Batch loss: 0.064
Epoch: 137 RMSE:  0.08738668233112169  MAPE: 0.18680702050136874  L2+L1 loss: 0.231
Epoch [138/150], Batch loss: 0.063
Epoch: 138 RMSE:  0.08758164830579289  MAPE: 0.1869181501342039  L2+L1 loss: 0.231
Epoch [139/150], Batch loss: 0.063
Epoch: 139 RMSE:  0.08758929739124939  MAPE: 0.18681723675957665  L2+L1 loss: 0.231
Epoch [140/150], Batch loss: 0.064
Epoch: 140 RMSE:  0.08745237779356202  MAPE: 0.18687573703704424  L2+L1 loss: 0.231
Epoch [141/150], Batch loss: 0.063
Epoch: 141 RMSE:  0.08756664911761604  MAPE: 0.18673173122605358  L2+L1 loss: 0.231
Epoch [142/150], Batch loss: 0.063
Epoch: 142 RMSE:  0.08762385005331923  MAPE: 0.18674024947316045  L2+L1 loss: 0.231
Epoch [143/150], Batch loss: 0.063
Epoch: 143 RMSE:  0.08763199672650261  MAPE: 0.18689628951652973  L2+L1 loss: 0.231
Epoch [144/150], Batch loss: 0.063
Epoch: 144 RMSE:  0.08759949345448266  MAPE: 0.18686783891405825  L2+L1 loss: 0.231
Epoch [145/150], Batch loss: 0.063
Epoch: 145 RMSE:  0.08758174942785446  MAPE: 0.1867033302075726  L2+L1 loss: 0.231
Epoch [146/150], Batch loss: 0.064
Epoch: 146 RMSE:  0.08768924606821024  MAPE: 0.1868351948580251  L2+L1 loss: 0.231
Epoch [147/150], Batch loss: 0.063
Epoch: 147 RMSE:  0.08747711039703279  MAPE: 0.18676167236807237  L2+L1 loss: 0.231
Epoch [148/150], Batch loss: 0.063
Epoch: 148 RMSE:  0.08760344584126067  MAPE: 0.1869707662620711  L2+L1 loss: 0.231
Epoch [149/150], Batch loss: 0.063
Epoch: 149 RMSE:  0.08757817086092168  MAPE: 0.1867287169608855  L2+L1 loss: 0.231


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.10852599 , 0.2085
0.27596122 , 0.4152
0.7981503 , 1.0545
0.2833929 , 0.224
0.30923986 , 0.273
0.15791646 , 0.1146
0.25130826 , 0.2429
0.15221938 , 0.108
0.14106384 , 0.2533
0.14904588 , 0.1271
0.7266639 , 0.7571
0.43570977 , 0.4097
0.31228268 , 0.2589
0.3305626 , 0.2795
0.1863128 , 0.1188
0.9899114 , 0.8737
0.19970623 , 0.2673
0.64629686 , 0.5948
0.14157882 , 0.1134
0.15106761 , 0.1147
0.47794002 , 0.4373
0.13541678 , 0.1018
0.49296126 , 0.6488
0.13353702 , 0.2001
0.3222154 , 0.4559
0.38954058 , 0.4902
0.4221217 , 0.3795
0.65806174 , 0.5834
0.453903 , 0.4294
0.59845674 , 0.5555
0.12531668 , 0.1382
0.9959511 , 1.0232
0.32350105 , 0.2962
0.18036374 , 0.137
0.20464122 , 0.1771
0.2789625 , 0.2377
0.19435287 , 0.1461
0.39415357 , 0.3263
0.23946175 , 0.1988
0.46877882 , 0.4853
0.39233127 , 0.3192
7.386543 , 7.4578
0.11788967 , 0.1231
0.2852924 , 0.3166
0.16155979 , 0.1648
0.13847247 , 0.144
0.2695917 , 0.2773
0.3613733 , 0.2725
0.12105265 , 0.1062
0.38921738 , 0.3495
0.3763839 , 0.3109
0.35875982 , 0.3338
0.3251977 , 0.2336
0.36502725 , 0.444
0.32361957 , 0.2709
0.5042399 , 0.4489
0.39448678 , 0.3366
0.38767618 , 0.4209
0.12320009 , 0.1583
0.14448711 , 0.1009
0.689444 , 0.625
0.1373674 , 0.1417
0.5708281 , 0.5266
0.15357664 , 0.1436
0.112677276 , 0.011415
0.49969062 , 0.4838
0.88399935 , 0.8187
0.13501999 , 0.1734
0.35449156 , 0.3238
0.3913707 , 0.3158
0.30359524 , 0.34
0.18288529 , 0.1527
0.53269756 , 0.6767
0.6698314 , 0.6081
0.35967082 , 0.5875
0.0 , 0.003395
0.34438288 , 0.2998
0.18084261 , 0.1158
0.13254452 , 0.1582
0.14139286 , 0.109
0.26558843 , 0.2884
0.14806595 , 0.1797
0.15609905 , 0.1298
0.43728495 , 0.4022
0.41998446 , 0.3485
0.40868983 , 0.5298
0.70948684 , 0.775
0.3980294 , 0.4872
0.12521395 , 0.1096
0.14544159 , 0.1086
0.14984992 , 0.1091
0.56214213 , 0.5855
0.12867004 , 0.1742
6.5341496 , 6.3022
0.40360856 , 0.4503
0.29780442 , 0.2571
0.42173842 , 0.3681
0.13622999 , 0.1656
0.78961146 , 0.7038
0.5319705 , 0.5608
0.14096823 , 0.1117
0.15592596 , 0.1199
1.1714046 , 1.4706
0.68870366 , 0.6321
0.13867375 , 0.1884
6.7727532 , 6.7314
0.61173034 , 0.5899
0.15800825 , 0.1213
0.15148738 , 0.1249
0.15465099 , 0.1602
0.9710939 , 1.1077
0.38402689 , 0.4057
0.64247894 , 0.5377
0.9303321 , 0.8224
0.13741615 , 0.127
0.47479603 , 0.4079
0.60328686 , 0.7104
0.14631519 , 0.111
0.06919709 , 0.028212
0.14908254 , 0.1044
0.40811887 , 0.3662
0.20544776 , 0.1937
0.12060252 , 0.1337
0.14083752 , 0.1116
0.13736644 , 0.1047
0.16920266 , 0.1246
0.3397204 , 0.3513
0.22870716 , 0.172
0.13643694 , 0.1321
0.73785114 , 0.6607
0.19767419 , 0.3523
0.29082468 , 0.2486
0.2916077 , 0.243
0.1699337 , 0.1048
0.18726271 , 0.1457
0.59598213 , 0.5848
0.6571468 , 0.5875
0.16373652 , 0.1329
0.21271151 , 0.2214
0.32402295 , 0.2996
0.24046731 , 0.336
0.33339307 , 0.2839
0.559717 , 0.4814
0.20748302 , 0.2052
0.27030092 , 0.2746
0.38811415 , 0.5808
1.0119526 , 0.9949
0.12967595 , 0.1308
0.1773319 , 0.1289
0.28485203 , 0.2286
0.15094629 , 0.1438
0.39326003 , 0.3154
0.13130987 , 0.1015
0.22956446 , 0.23
0.31965348 , 0.3531
0.57473606 , 0.7901
0.18493453 , 0.1232
0.36136484 , 0.5147
0.33981282 , 0.4121
0.33354396 , 0.3643
0.14531061 , 0.1628
0.500223 , 0.5054
0.76385534 , 0.7978
0.15362927 , 0.1095
0.37020993 , 0.4253
0.29314065 , 0.3128
0.3037351 , 0.2755
0.4843484 , 0.4738
0.7803897 , 0.7814
0.11381185 , 0.1603
0.3559097 , 0.313
0.3211551 , 0.3132
0.12304026 , 0.1113
0.7778939 , 0.952
0.20803073 , 0.1746
0.30603772 , 0.2629
0.1542258 , 0.2582
0.53598326 , 0.6252
0.5013731 , 0.4878
0.63554233 , 0.5765
0.14412251 , 0.1297
0.23631033 , 0.2648
0.38996178 , 0.3134
0.12591335 , 0.1757
0.152634 , 0.1284
0.1487416 , 0.1685
0.14547443 , 0.1831
0.410946 , 0.4607
0.3060673 , 0.5275
0.19439676 , 0.1513
0.5158864 , 0.4779
0.14043543 , 0.1341
0.6682124 , 0.5982
0.26979658 , 0.2122
0.4962322 , 0.4429
0.21009013 , 0.1612
0.3966804 , 0.4309
0.9797934 , 1.0618
0.4044531 , 0.462
0.5570891 , 0.6537
0.33371672 , 0.3863
0.5496708 , 0.4572
0.24743068 , 0.174
0.13395932 , 0.1114
0.13633746 , 0.1089
0.14519843 , 0.1085
0.124816895 , 0.125
0.26523146 , 0.2209
0.121100634 , 0.192
0.22162616 , 0.1492
0.18101835 , 0.253
0.14986485 , 0.1135
0.136403 , 0.1201
0.14103884 , 0.1404
0.63229525 , 0.5741
0.1491124 , 0.1437
0.28858495 , 0.3131
0.4204424 , 0.4817
0.13361436 , 0.1091
0.14310616 , 0.1001
0.5534372 , 0.5037
0.15027884 , 0.1167
0.63146466 , 0.5675
0.13038558 , 0.14
0.35719818 , 0.2867
0.7925674 , 0.7399
0.3204762 , 0.3078
0.83372736 , 0.841
0.14034659 , 0.1098
0.38317505 , 0.4213
1.1357188 , 1.0141
0.7685491 , 0.8311
0.99891067 , 1.0263
0.76286536 , 0.9135
0.30438906 , 0.2656
0.21430609 , 0.2293
0.2934971 , 0.3945
0.18623063 , 0.1661
0.20590517 , 0.1842
0.2418386 , 0.1924
0.40774494 , 0.3496
0.17952356 , 0.1203
0.49631825 , 0.439
0.5777698 , 0.5573
0.25764477 , 0.2306
0.24658886 , 0.1983
0.62623185 , 0.5907
0.17049712 , 0.209
0.27847436 , 0.3474
0.7266364 , 0.6503
0.5505157 , 0.5527
0.835248 , 0.8413
0.14099416 , 0.1028
0.21493408 , 0.1643
0.23204869 , 0.1732
0.24543944 , 0.2023
0.23650935 , 0.2234
0.21835536 , 0.1812
0.20300749 , 0.1638
0.28103903 , 0.2364
0.38968307 , 0.3846
0.12381971 , 0.1115
0.6427145 , 0.785
0.7559632 , 0.6619
0.18276054 , 0.1847
0.116267264 , 0.1195
0.75756353 , 0.7466
0.06854412 , 0.008758
0.51639485 , 0.4906
0.27398747 , 0.2923
0.24935523 , 0.1789
0.4315754 , 0.5269
0.40564 , 0.3784
0.3294829 , 0.257
0.37612772 , 0.4181
0.20252481 , 0.3003
0.15097633 , 0.1792
0.6863199 , 0.604
0.482511 , 0.5578
0.4571868 , 0.456
0.21011296 , 0.1545
0.1324403 , 0.1426
0.33233118 , 0.3177
0.5696126 , 0.5559
0.19468972 , 0.1471
1.1519036 , 1.0753
0.27591518 , 0.2081
0.28892618 , 0.2723
0.38120717 , 0.3801
0.52694196 , 0.5758
0.20840007 , 0.1381
0.79821634 , 0.7966
0.9262253 , 0.988
0.12937441 , 0.1058
0.7509506 , 0.6537
0.14420524 , 0.1079
RMSE:  0.06720459885802176  MAPE: 0.232645642008894
5: ground truth total-  293  predicted total -  293
100: ground truth total-  3  predicted total -  3
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
0.008493006 , 0.007351
0.08310497 , 0.02796
0.007462859 , 0.006986
0.10039422 , 0.004627
0.10235286 , 0.074736
0.08441052 , 0.120303
0.10683584 , 0.184957
0.092274934 , 0.090149
0.08493778 , 0.21569
0.09831837 , 0.123302
0.10618287 , 0.094405
0.10744882 , 0.007502
0.098700106 , 0.193753
0.089663506 , 0.003833
0.104224145 , 0.00692
0.006244242 , 0.004772
0.08375791 , 0.063802
0.10942131 , 0.110989
0.09766537 , 0.064148
0.08114636 , 0.003237
0.084139675 , 0.091373
0.11006445 , 0.224873
0.08479267 , 0.191873
0.10300583 , 0.155169
0.091622174 , 0.044932
0.09570676 , 0.004235
0.0047529936 , 0.004073
0.09265694 , 0.135922
0.08462083 , 0.161347
0.09292802 , 0.194388
RMSE:  0.067751449621377  MAPE: 4.2013001901212865
