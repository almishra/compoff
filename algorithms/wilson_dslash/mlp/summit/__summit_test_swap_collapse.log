['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
2040
19
<class 'numpy.dtype'> float64
2040
train batches:  92  validate samples: 163  test samples: 408
Epoch [0/150], Batch loss: 37.954
Epoch: 0 RMSE:  6.200975820734729  MAPE: 4.926151852872043  L2+L1 loss: 3.928
Epoch [1/150], Batch loss: 3.549
Epoch: 1 RMSE:  3.6566415062578175  MAPE: 4.041333313087647  L2+L1 loss: 2.993
Epoch [2/150], Batch loss: 2.822
Epoch: 2 RMSE:  2.2906972540395136  MAPE: 2.8031009985483  L2+L1 loss: 2.126
Epoch [3/150], Batch loss: 3.19
Epoch: 3 RMSE:  2.585560822092138  MAPE: 1.7791527166873626  L2+L1 loss: 2.044
Epoch [4/150], Batch loss: 2.638
Epoch: 4 RMSE:  2.0117632189169337  MAPE: 0.5308791301363979  L2+L1 loss: 1.208
Epoch [5/150], Batch loss: 1.716
Epoch: 5 RMSE:  7.297372803102761  MAPE: 7.004554272103973  L2+L1 loss: 3.19
Epoch [6/150], Batch loss: 4.283
Epoch: 6 RMSE:  8.0290458982888  MAPE: 1.585444354445428  L2+L1 loss: 2.691
Epoch [7/150], Batch loss: 3.265
Epoch: 7 RMSE:  8.900813614344173  MAPE: 3.7510897324235843  L2+L1 loss: 3.247
Epoch [8/150], Batch loss: 6.582
Epoch: 8 RMSE:  8.660341428963408  MAPE: 1.437474628869146  L2+L1 loss: 2.497
Epoch [9/150], Batch loss: 3.964
Epoch: 9 RMSE:  8.195112031566953  MAPE: 0.9276011986271903  L2+L1 loss: 2.161
Epoch [10/150], Batch loss: 4.926
Epoch: 10 RMSE:  8.464356188489395  MAPE: 1.353525645040919  L2+L1 loss: 2.541
Epoch [11/150], Batch loss: 3.659
Epoch: 11 RMSE:  7.877748812534141  MAPE: 1.5143980432641493  L2+L1 loss: 2.203
Epoch [12/150], Batch loss: 7.138
Epoch: 12 RMSE:  9.233784632758052  MAPE: 4.088663474413266  L2+L1 loss: 2.747
Epoch [13/150], Batch loss: 5.016
Epoch: 13 RMSE:  8.88964920289732  MAPE: 4.266059022368581  L2+L1 loss: 3.389
Epoch [14/150], Batch loss: 4.733
Epoch: 14 RMSE:  8.886316651694793  MAPE: 4.915724920053154  L2+L1 loss: 3.571
Epoch [15/150], Batch loss: 4.847
Epoch: 15 RMSE:  8.886151390128214  MAPE: 4.6168548567994705  L2+L1 loss: 3.486
Epoch [16/150], Batch loss: 4.831
Epoch: 16 RMSE:  8.893129750831497  MAPE: 4.080488531006575  L2+L1 loss: 3.338
Epoch [17/150], Batch loss: 4.82
Epoch: 17 RMSE:  8.891423496097229  MAPE: 4.163969779944186  L2+L1 loss: 3.361
Epoch [18/150], Batch loss: 5.091
Epoch: 18 RMSE:  8.889388354395852  MAPE: 4.282975880648755  L2+L1 loss: 3.394
Epoch [19/150], Batch loss: 4.932
Epoch: 19 RMSE:  9.023768889631251  MAPE: 3.9888990660671624  L2+L1 loss: 3.57
Epoch [20/150], Batch loss: 5.452
Epoch: 20 RMSE:  8.888323232060804  MAPE: 5.1415583833095475  L2+L1 loss: 3.642
Epoch [21/150], Batch loss: 4.985
Epoch: 21 RMSE:  8.898064938092942  MAPE: 3.884161055278453  L2+L1 loss: 3.286
Epoch [22/150], Batch loss: 4.921
Epoch: 22 RMSE:  8.887631474912332  MAPE: 5.08124158600038  L2+L1 loss: 3.622
Epoch [23/150], Batch loss: 4.829
Epoch: 23 RMSE:  8.887387738543483  MAPE: 4.442448029677341  L2+L1 loss: 3.437
Epoch [24/150], Batch loss: 4.73
Epoch: 24 RMSE:  8.888786992419325  MAPE: 4.324544372062847  L2+L1 loss: 3.406
Epoch [25/150], Batch loss: 4.937
Epoch: 25 RMSE:  20.45477472474523  MAPE: 29.725852235477586  L2+L1 loss: 7.589
Epoch [26/150], Batch loss: 5.306
Epoch: 26 RMSE:  8.895837457522186  MAPE: 3.966152490563127  L2+L1 loss: 3.304
Epoch [27/150], Batch loss: 4.881
Epoch: 27 RMSE:  8.886098301165617  MAPE: 4.867976847473481  L2+L1 loss: 3.558
Epoch [28/150], Batch loss: 6.883
Epoch: 28 RMSE:  8.902293939862123  MAPE: 3.7474150985302552  L2+L1 loss: 3.254
Epoch [29/150], Batch loss: 9.967
Epoch: 29 RMSE:  8.949941433655672  MAPE: 2.7932482911485894  L2+L1 loss: 3.044
Epoch [30/150], Batch loss: 4.802
Epoch: 30 RMSE:  8.89501320132139  MAPE: 3.9990780580542924  L2+L1 loss: 3.314
Epoch [31/150], Batch loss: 4.784
Epoch: 31 RMSE:  8.888110204671792  MAPE: 4.37685740242493  L2+L1 loss: 3.42
Epoch [32/150], Batch loss: 4.876
Epoch: 32 RMSE:  8.88683371927959  MAPE: 4.504595915493614  L2+L1 loss: 3.455
Epoch [33/150], Batch loss: 4.794
Epoch: 33 RMSE:  8.887141470341504  MAPE: 4.468364995797922  L2+L1 loss: 3.444
Epoch [34/150], Batch loss: 4.798
Epoch: 34 RMSE:  8.888721668739139  MAPE: 4.329308355635805  L2+L1 loss: 3.407
Epoch [35/150], Batch loss: 4.8
Epoch: 35 RMSE:  8.8875200718077  MAPE: 4.4302483291511745  L2+L1 loss: 3.434
Epoch [36/150], Batch loss: 4.793
Epoch: 36 RMSE:  8.886530668787861  MAPE: 4.546550335864803  L2+L1 loss: 3.466
Epoch [37/150], Batch loss: 4.819
Epoch: 37 RMSE:  8.888361293815406  MAPE: 4.356631840856331  L2+L1 loss: 3.414
Epoch [38/150], Batch loss: 4.917
Epoch: 38 RMSE:  8.88812890617524  MAPE: 4.375313104691463  L2+L1 loss: 3.419
Epoch [39/150], Batch loss: 4.522
Epoch: 39 RMSE:  8.582233264504191  MAPE: 1.4847723119094314  L2+L1 loss: 2.381
Epoch [40/150], Batch loss: 3.686
Epoch: 40 RMSE:  8.361780221086793  MAPE: 1.3981009917969793  L2+L1 loss: 1.974
Epoch [41/150], Batch loss: 3.4
Epoch: 41 RMSE:  8.150899988018146  MAPE: 0.6969780517640218  L2+L1 loss: 1.504
Epoch [42/150], Batch loss: 3.293
Epoch: 42 RMSE:  7.9813171919751005  MAPE: 0.7220027250228823  L2+L1 loss: 1.452
Epoch [43/150], Batch loss: 3.429
Epoch: 43 RMSE:  5.562956403471486  MAPE: 0.9154860402065143  L2+L1 loss: 2.074
Epoch [44/150], Batch loss: 1.708
Epoch: 44 RMSE:  1.8781864786543685  MAPE: 0.43265309528252704  L2+L1 loss: 0.891
Epoch [45/150], Batch loss: 2.591
Epoch: 45 RMSE:  8.58387243643002  MAPE: 0.3902418094100711  L2+L1 loss: 1.823
Epoch [46/150], Batch loss: 3.714
Epoch: 46 RMSE:  8.518321578791262  MAPE: 0.45140106624944915  L2+L1 loss: 1.784
Epoch [47/150], Batch loss: 3.755
Epoch: 47 RMSE:  3.2394077650502093  MAPE: 2.416463053797453  L2+L1 loss: 1.973
Epoch [48/150], Batch loss: 1.549
Epoch: 48 RMSE:  1.6844679969859624  MAPE: 0.6701190383532121  L2+L1 loss: 0.955
Epoch [49/150], Batch loss: 1.197
Epoch: 49 RMSE:  0.6386700811639887  MAPE: 0.5927856014057789  L2+L1 loss: 0.523
Epoch [50/150], Batch loss: 1.098
Epoch: 50 RMSE:  1.0398857489376685  MAPE: 0.37447901691020247  L2+L1 loss: 0.598
Epoch [51/150], Batch loss: 0.63
Epoch: 51 RMSE:  1.700063926676643  MAPE: 0.36951523637460154  L2+L1 loss: 0.634
Epoch [52/150], Batch loss: 0.837
Epoch: 52 RMSE:  1.0817447098996495  MAPE: 0.7537773300554934  L2+L1 loss: 0.92
Epoch [53/150], Batch loss: 0.754
Epoch: 53 RMSE:  1.2158816527008114  MAPE: 1.0514845984367756  L2+L1 loss: 0.822
Epoch [54/150], Batch loss: 0.928
Epoch: 54 RMSE:  1.8575550886037069  MAPE: 1.7871613392194412  L2+L1 loss: 1.423
Epoch [55/150], Batch loss: 0.703
Epoch: 55 RMSE:  1.37423770071141  MAPE: 0.6219600797741062  L2+L1 loss: 0.788
Epoch [56/150], Batch loss: 0.778
Epoch: 56 RMSE:  3.8665313073495833  MAPE: 0.97662796285207  L2+L1 loss: 1.2
Epoch [57/150], Batch loss: 1.217
Epoch: 57 RMSE:  0.6661272703419596  MAPE: 0.5073181429067719  L2+L1 loss: 0.557
Epoch [58/150], Batch loss: 0.562
Epoch: 58 RMSE:  0.5245162617594805  MAPE: 0.3158603633019556  L2+L1 loss: 0.498
Epoch [59/150], Batch loss: 0.577
Epoch: 59 RMSE:  4.746181052933054  MAPE: 0.6998587121259823  L2+L1 loss: 1.242
Epoch [60/150], Batch loss: 0.625
Epoch: 60 RMSE:  0.2923132261257003  MAPE: 0.2933856059445073  L2+L1 loss: 0.389
Epoch [61/150], Batch loss: 0.236
Epoch: 61 RMSE:  0.263128096178075  MAPE: 0.33232265460822136  L2+L1 loss: 0.346
Epoch [62/150], Batch loss: 0.254
Epoch: 62 RMSE:  0.3420666480502416  MAPE: 0.29921758555236827  L2+L1 loss: 0.413
Epoch [63/150], Batch loss: 0.228
Epoch: 63 RMSE:  0.42926572145885716  MAPE: 0.34713353563213445  L2+L1 loss: 0.376
Epoch [64/150], Batch loss: 0.258
Epoch: 64 RMSE:  0.27704397987600154  MAPE: 0.19692273411994335  L2+L1 loss: 0.344
Epoch [65/150], Batch loss: 0.234
Epoch: 65 RMSE:  0.39939449963419504  MAPE: 0.2960810073069797  L2+L1 loss: 0.36
Epoch [66/150], Batch loss: 0.257
Epoch: 66 RMSE:  0.4725251685069238  MAPE: 0.3299472065351069  L2+L1 loss: 0.404
Epoch [67/150], Batch loss: 0.259
Epoch: 67 RMSE:  0.2426910311125848  MAPE: 0.3438823407861771  L2+L1 loss: 0.364
Epoch [68/150], Batch loss: 0.275
Epoch: 68 RMSE:  0.6636009796657071  MAPE: 0.20418728577770992  L2+L1 loss: 0.394
Epoch [69/150], Batch loss: 0.303
Epoch: 69 RMSE:  0.22324223703906346  MAPE: 0.4036834792379117  L2+L1 loss: 0.352
Epoch [70/150], Batch loss: 0.249
Epoch: 70 RMSE:  0.42960967876483763  MAPE: 0.2041397500305334  L2+L1 loss: 0.35
Epoch [71/150], Batch loss: 0.286
Epoch: 71 RMSE:  0.41042700607214316  MAPE: 0.43698292923553606  L2+L1 loss: 0.443
Epoch [72/150], Batch loss: 0.221
Epoch: 72 RMSE:  0.3550935847297229  MAPE: 0.3895646087038103  L2+L1 loss: 0.357
Epoch [73/150], Batch loss: 0.265
Epoch: 73 RMSE:  0.5034612091214588  MAPE: 0.2892264181821658  L2+L1 loss: 0.38
Epoch [74/150], Batch loss: 0.324
Epoch: 74 RMSE:  0.5192351626226951  MAPE: 0.16567607017133124  L2+L1 loss: 0.359
Epoch [75/150], Batch loss: 0.293
Epoch: 75 RMSE:  0.30896660023830735  MAPE: 0.48512209469958295  L2+L1 loss: 0.373
Epoch [76/150], Batch loss: 0.241
Epoch: 76 RMSE:  0.5796777892320256  MAPE: 0.3498649975626021  L2+L1 loss: 0.433
Epoch [77/150], Batch loss: 0.222
Epoch: 77 RMSE:  0.2974610937434068  MAPE: 0.4821773429448861  L2+L1 loss: 0.342
Epoch [78/150], Batch loss: 0.238
Epoch: 78 RMSE:  0.2548812800497644  MAPE: 0.17096262888483  L2+L1 loss: 0.322
Epoch [79/150], Batch loss: 0.367
Epoch: 79 RMSE:  0.294431451552145  MAPE: 0.3727881488871576  L2+L1 loss: 0.444
Epoch [80/150], Batch loss: 0.242
Epoch: 80 RMSE:  0.22257219450919546  MAPE: 0.3552205458074264  L2+L1 loss: 0.315
Epoch [81/150], Batch loss: 0.241
Epoch: 81 RMSE:  0.20986668646569606  MAPE: 0.2604789661255277  L2+L1 loss: 0.337
Epoch [82/150], Batch loss: 0.232
Epoch: 82 RMSE:  0.3041918048833689  MAPE: 0.4072509924135604  L2+L1 loss: 0.342
Epoch [83/150], Batch loss: 0.253
Epoch: 83 RMSE:  0.22779325889552016  MAPE: 0.2835502045840044  L2+L1 loss: 0.312
Epoch [84/150], Batch loss: 0.296
Epoch: 84 RMSE:  0.41285787199423785  MAPE: 0.28335209247774995  L2+L1 loss: 0.332
Epoch [85/150], Batch loss: 0.231
Epoch: 85 RMSE:  0.2680090970559733  MAPE: 0.4689381434058169  L2+L1 loss: 0.334
Epoch [86/150], Batch loss: 0.261
Epoch: 86 RMSE:  0.8167318765758014  MAPE: 0.3089316288894387  L2+L1 loss: 0.465
Epoch [87/150], Batch loss: 0.314
Epoch: 87 RMSE:  0.638601642639677  MAPE: 0.5559001690053647  L2+L1 loss: 0.407
Epoch [88/150], Batch loss: 0.243
Epoch: 88 RMSE:  0.7197331900839886  MAPE: 0.39160296937678973  L2+L1 loss: 0.434
Epoch [89/150], Batch loss: 0.31
Epoch: 89 RMSE:  0.29304324332294307  MAPE: 0.2093119290553066  L2+L1 loss: 0.326
Epoch [90/150], Batch loss: 0.176
Epoch: 90 RMSE:  0.2370575564741404  MAPE: 0.24166393036181588  L2+L1 loss: 0.3
Epoch [91/150], Batch loss: 0.178
Epoch: 91 RMSE:  0.29406678858458934  MAPE: 0.24257287513078787  L2+L1 loss: 0.31
Epoch [92/150], Batch loss: 0.173
Epoch: 92 RMSE:  0.2979988009950365  MAPE: 0.27733523491773493  L2+L1 loss: 0.314
Epoch [93/150], Batch loss: 0.174
Epoch: 93 RMSE:  0.264495075485453  MAPE: 0.28012015093746845  L2+L1 loss: 0.311
Epoch [94/150], Batch loss: 0.16
Epoch: 94 RMSE:  0.2079752907710457  MAPE: 0.2832746420301455  L2+L1 loss: 0.296
Epoch [95/150], Batch loss: 0.165
Epoch: 95 RMSE:  0.2357074035848584  MAPE: 0.25944605408259563  L2+L1 loss: 0.305
Epoch [96/150], Batch loss: 0.164
Epoch: 96 RMSE:  0.1984606893485067  MAPE: 0.2500291955361987  L2+L1 loss: 0.286
Epoch [97/150], Batch loss: 0.161
Epoch: 97 RMSE:  0.2585901801590297  MAPE: 0.2744389091222039  L2+L1 loss: 0.309
Epoch [98/150], Batch loss: 0.163
Epoch: 98 RMSE:  0.21765123417257484  MAPE: 0.27062451756636047  L2+L1 loss: 0.298
Epoch [99/150], Batch loss: 0.16
Epoch: 99 RMSE:  0.19549395866351352  MAPE: 0.25716783197348686  L2+L1 loss: 0.284
Epoch [100/150], Batch loss: 0.159
Epoch: 100 RMSE:  0.20135294627572406  MAPE: 0.2581157297497293  L2+L1 loss: 0.291
Epoch [101/150], Batch loss: 0.161
Epoch: 101 RMSE:  0.2210778962007479  MAPE: 0.24393582117885815  L2+L1 loss: 0.298
Epoch [102/150], Batch loss: 0.156
Epoch: 102 RMSE:  0.260554460166587  MAPE: 0.26112024563511077  L2+L1 loss: 0.318
Epoch [103/150], Batch loss: 0.155
Epoch: 103 RMSE:  0.237450185015743  MAPE: 0.27524825861960206  L2+L1 loss: 0.3
Epoch [104/150], Batch loss: 0.16
Epoch: 104 RMSE:  0.21897755912831568  MAPE: 0.261222159277432  L2+L1 loss: 0.296
Epoch [105/150], Batch loss: 0.165
Epoch: 105 RMSE:  0.2211367962047862  MAPE: 0.28643580166995875  L2+L1 loss: 0.297
Epoch [106/150], Batch loss: 0.15
Epoch: 106 RMSE:  0.2914031639435622  MAPE: 0.2717152303302727  L2+L1 loss: 0.31
Epoch [107/150], Batch loss: 0.153
Epoch: 107 RMSE:  0.33607491807086365  MAPE: 0.25610432834068503  L2+L1 loss: 0.316
Epoch [108/150], Batch loss: 0.159
Epoch: 108 RMSE:  0.21280733378128355  MAPE: 0.2438022373488451  L2+L1 loss: 0.292
Epoch [109/150], Batch loss: 0.155
Epoch: 109 RMSE:  0.2617969749251695  MAPE: 0.2616334085021506  L2+L1 loss: 0.306
Epoch [110/150], Batch loss: 0.153
Epoch: 110 RMSE:  0.22787252330377605  MAPE: 0.24561986265703914  L2+L1 loss: 0.3
Epoch [111/150], Batch loss: 0.152
Epoch: 111 RMSE:  0.2662919256099665  MAPE: 0.25851265446616184  L2+L1 loss: 0.308
Epoch [112/150], Batch loss: 0.145
Epoch: 112 RMSE:  0.2536126019553167  MAPE: 0.2751849544089431  L2+L1 loss: 0.307
Epoch [113/150], Batch loss: 0.157
Epoch: 113 RMSE:  0.2184784890269117  MAPE: 0.26482508243334413  L2+L1 loss: 0.298
Epoch [114/150], Batch loss: 0.155
Epoch: 114 RMSE:  0.23391184336787874  MAPE: 0.2699604801952493  L2+L1 loss: 0.3
Epoch [115/150], Batch loss: 0.155
Epoch: 115 RMSE:  0.2756193512313089  MAPE: 0.2549903054402554  L2+L1 loss: 0.31
Epoch [116/150], Batch loss: 0.157
Epoch: 116 RMSE:  0.20471681502489072  MAPE: 0.26268069671181127  L2+L1 loss: 0.298
Epoch [117/150], Batch loss: 0.149
Epoch: 117 RMSE:  0.23386123738311854  MAPE: 0.26132558260843686  L2+L1 loss: 0.299
Epoch [118/150], Batch loss: 0.15
Epoch: 118 RMSE:  0.20090993521435405  MAPE: 0.2684515209249772  L2+L1 loss: 0.291
Epoch [119/150], Batch loss: 0.162
Epoch: 119 RMSE:  0.2697633263795527  MAPE: 0.2599245439694147  L2+L1 loss: 0.311
Epoch [120/150], Batch loss: 0.146
Epoch: 120 RMSE:  0.24984269124723632  MAPE: 0.26110894861934203  L2+L1 loss: 0.305
Epoch [121/150], Batch loss: 0.145
Epoch: 121 RMSE:  0.24053330051738395  MAPE: 0.25946230081803423  L2+L1 loss: 0.301
Epoch [122/150], Batch loss: 0.147
Epoch: 122 RMSE:  0.2404946769706006  MAPE: 0.2587926692797417  L2+L1 loss: 0.3
Epoch [123/150], Batch loss: 0.147
Epoch: 123 RMSE:  0.23590375300158292  MAPE: 0.25669989184801184  L2+L1 loss: 0.298
Epoch [124/150], Batch loss: 0.147
Epoch: 124 RMSE:  0.2374940854925542  MAPE: 0.25964649513847937  L2+L1 loss: 0.298
Epoch [125/150], Batch loss: 0.145
Epoch: 125 RMSE:  0.23833429520496074  MAPE: 0.2569240511508632  L2+L1 loss: 0.299
Epoch [126/150], Batch loss: 0.147
Epoch: 126 RMSE:  0.2441850170202776  MAPE: 0.2594979749437453  L2+L1 loss: 0.3
Epoch [127/150], Batch loss: 0.144
Epoch: 127 RMSE:  0.2328738762200604  MAPE: 0.26021719782798386  L2+L1 loss: 0.297
Epoch [128/150], Batch loss: 0.143
Epoch: 128 RMSE:  0.2367866728880022  MAPE: 0.25913187829691786  L2+L1 loss: 0.299
Epoch [129/150], Batch loss: 0.147
Epoch: 129 RMSE:  0.2356198764821517  MAPE: 0.26003497331484665  L2+L1 loss: 0.298
Epoch [130/150], Batch loss: 0.144
Epoch: 130 RMSE:  0.23886303037387707  MAPE: 0.2611164203295165  L2+L1 loss: 0.299
Epoch [131/150], Batch loss: 0.144
Epoch: 131 RMSE:  0.24164940169454877  MAPE: 0.25881216744994906  L2+L1 loss: 0.3
Epoch [132/150], Batch loss: 0.145
Epoch: 132 RMSE:  0.23639925542677465  MAPE: 0.2596564370933069  L2+L1 loss: 0.298
Epoch [133/150], Batch loss: 0.145
Epoch: 133 RMSE:  0.2401878642201898  MAPE: 0.2614219511592986  L2+L1 loss: 0.3
Epoch [134/150], Batch loss: 0.145
Epoch: 134 RMSE:  0.2485188813320202  MAPE: 0.2600096216727889  L2+L1 loss: 0.301
Epoch [135/150], Batch loss: 0.147
Epoch: 135 RMSE:  0.2366793353100501  MAPE: 0.26079626921225585  L2+L1 loss: 0.298
Epoch [136/150], Batch loss: 0.145
Epoch: 136 RMSE:  0.23391240322241852  MAPE: 0.261680121211044  L2+L1 loss: 0.298
Epoch [137/150], Batch loss: 0.143
Epoch: 137 RMSE:  0.23552112348379586  MAPE: 0.2597750603017103  L2+L1 loss: 0.298
Epoch [138/150], Batch loss: 0.143
Epoch: 138 RMSE:  0.2406821085851706  MAPE: 0.2592457611651772  L2+L1 loss: 0.299
Epoch [139/150], Batch loss: 0.144
Epoch: 139 RMSE:  0.23486144493720157  MAPE: 0.26081419810304757  L2+L1 loss: 0.298
Epoch [140/150], Batch loss: 0.143
Epoch: 140 RMSE:  0.23393764068613832  MAPE: 0.25992504777430364  L2+L1 loss: 0.298
Epoch [141/150], Batch loss: 0.142
Epoch: 141 RMSE:  0.2327903970825993  MAPE: 0.2600164762931299  L2+L1 loss: 0.297
Epoch [142/150], Batch loss: 0.141
Epoch: 142 RMSE:  0.23468044338364794  MAPE: 0.26002438090975993  L2+L1 loss: 0.298
Epoch [143/150], Batch loss: 0.145
Epoch: 143 RMSE:  0.24329320227394732  MAPE: 0.26062430198749115  L2+L1 loss: 0.3
Epoch [144/150], Batch loss: 0.147
Epoch: 144 RMSE:  0.2353320277670828  MAPE: 0.25967506802751716  L2+L1 loss: 0.297
Epoch [145/150], Batch loss: 0.144
Epoch: 145 RMSE:  0.2399455201276249  MAPE: 0.2603532329296491  L2+L1 loss: 0.299
Epoch [146/150], Batch loss: 0.145
Epoch: 146 RMSE:  0.23276537372758496  MAPE: 0.25818571627463854  L2+L1 loss: 0.297
Epoch [147/150], Batch loss: 0.146
Epoch: 147 RMSE:  0.23618966763324034  MAPE: 0.26121076567398954  L2+L1 loss: 0.298
Epoch [148/150], Batch loss: 0.145
Epoch: 148 RMSE:  0.23901760866692082  MAPE: 0.2586345672383737  L2+L1 loss: 0.298
Epoch [149/150], Batch loss: 0.141
Epoch: 149 RMSE:  0.23974238925382696  MAPE: 0.25839822682748853  L2+L1 loss: 0.298


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.34667683 , 0.2963
0.1799798 , 0.172
0.22141409 , 0.2396
0.15391636 , 0.046204
0.29017735 , 0.2563
0.29155827 , 0.2692
3.4015906 , 3.3689
8.975343 , 8.8123
1.3281913 , 1.456
4.9248786 , 4.8895
0.2662258 , 0.2583
0.47365665 , 0.513
4.5492654 , 4.5695
1.2575364 , 1.3245
0.26684904 , 0.2988
0.43179488 , 0.3892
0.6929791 , 0.6457
0.18843842 , 0.167
4.0150623 , 4.1114
0.78238916 , 0.7116
1.3220367 , 1.2462
0.2403245 , 0.2955
0.6074722 , 0.596
2.1337435 , 2.1546
0.32580757 , 0.2547
3.8057892 , 3.6714
0.34272337 , 0.462
3.3718224 , 3.4409
4.767516 , 4.7246
0.23300171 , 0.2383
1.5182121 , 1.3127
0.41439724 , 0.399
4.8388076 , 4.8722
0.3122921 , 0.3065
1.334415 , 1.3026
0.3276739 , 0.3943
2.3233266 , 2.5386
7.791553 , 7.6387
0.25415087 , 0.3672
0.21727562 , 0.2392
1.3567963 , 1.5427
0.83249116 , 0.6794
1.1485014 , 1.1177
1.3648381 , 1.1576
3.9183972 , 3.9363
0.8052454 , 0.8122
0.24765301 , 0.3376
0.19541454 , 0.1736
0.18988895 , 0.1658
3.975867 , 4.0352
3.8499231 , 3.7152
4.6210575 , 4.519
6.943727 , 7.1966
0.29186153 , 0.3777
0.23154306 , 0.3004
1.2080951 , 1.0882
3.067 , 2.7125
4.254102 , 4.2773
2.682917 , 2.6195
2.9199188 , 3.0221
0.49935365 , 0.5152
0.43578124 , 0.4076
0.22336817 , 0.1309
3.3005395 , 3.1001
0.17667294 , 0.1519
0.4476576 , 0.5275
4.1288624 , 4.197
0.22078037 , 0.2726
0.860831 , 0.8849
0.46561408 , 0.3816
0.29710817 , 0.2664
1.1354296 , 1.0201
0.30525303 , 0.3433
9.034998 , 9.221
5.298153 , 5.3118
3.492075 , 3.3708
0.4009905 , 0.4007
1.2004356 , 1.208
2.4087358 , 2.5168
0.3631754 , 0.4135
0.45348907 , 0.45
0.40220594 , 0.3983
4.6817465 , 4.7083
4.286694 , 4.5183
4.897934 , 4.9671
1.1918571 , 0.9525
1.1442494 , 1.4143
0.20436287 , 0.2365
1.5615778 , 1.6615
4.7840652 , 4.8821
1.8293254 , 2.1258
6.8163176 , 6.7361
1.6426771 , 1.7279
4.224171 , 4.2837
0.26299858 , 0.2625
0.35322905 , 0.357
1.7238147 , 1.6622
2.6029434 , 2.5725
0.5442567 , 0.516
1.8432989 , 1.9729
0.46522427 , 0.422
1.5148766 , 1.5185
0.39592028 , 0.4006
2.2439766 , 2.2376
5.498831 , 5.5134
0.17000341 , 0.2152
0.17833519 , 0.1567
1.4453459 , 1.3509
0.4929142 , 0.5241
0.38054323 , 0.3103
0.23477173 , 0.2166
0.39904594 , 0.3285
0.25773144 , 0.3155
4.1922426 , 4.2145
1.2225142 , 1.2503
0.3246355 , 0.3684
2.4931657 , 2.5404
1.9628232 , 2.1717
0.4267025 , 0.4116
4.968735 , 5.1725
6.1259336 , 5.9904
0.38018036 , 0.3667
0.25664806 , 0.2692
3.6158192 , 3.6725
2.4033823 , 2.5289
0.39958286 , 0.399
1.0578413 , 0.9972
7.20277 , 7.2654
2.0434685 , 2.0122
3.615685 , 3.5042
0.5466056 , 0.5619
1.5975964 , 1.6643
1.0135541 , 1.0935
0.23987055 , 0.2151
0.1947422 , 0.1751
0.0 , 0.1083
0.3396001 , 0.2774
0.50516343 , 0.5239
3.5791223 , 3.4726
0.32422066 , 0.2684
2.074972 , 2.3391
3.9492471 , 3.9702
2.9696734 , 2.8804
0.38338757 , 0.341
0.537508 , 0.6373
0.43069887 , 0.4404
0.34686136 , 0.2612
0.2647667 , 0.2983
0.27428627 , 0.2602
1.3605762 , 1.4791
0.88932395 , 0.9992
0.2650957 , 0.3146
0.5348542 , 0.4802
4.16663 , 4.1817
0.28094006 , 0.2963
2.240808 , 2.3143
2.3293157 , 2.2963
0.42851543 , 0.4931
0.54375505 , 0.4657
0.1944809 , 0.1665
5.392695 , 5.5749
1.7991257 , 1.7436
0.52543855 , 0.5103
0.17845154 , 0.1561
5.1861176 , 5.2275
0.19289923 , 0.2668
0.4512112 , 0.4264
3.6929195 , 3.6361
1.8399863 , 1.8089
0.56014943 , 0.5941
1.9103022 , 1.9941
2.7180579 , 2.7688
0.22701931 , 0.2474
0.35470533 , 0.3103
0.94660854 , 0.8567
1.6314969 , 1.7192
0.39972448 , 0.382
0.3817644 , 0.38
1.8999445 , 1.905
0.9053323 , 0.9247
1.5737288 , 1.4254
1.7131205 , 1.4953
1.7182136 , 1.6464
0.46027017 , 0.3868
4.089577 , 3.9465
7.667207 , 7.8402
0.33216047 , 0.2845
2.0522206 , 2.0709
3.331952 , 3.336
3.0911772 , 3.1694
3.878652 , 3.8075
2.3476686 , 2.2724
2.5674815 , 2.6984
0.23173189 , 0.2692
0.26901293 , 0.3122
0.9286339 , 0.9295
0.25914 , 0.2714
4.123598 , 4.1048
0.41156292 , 0.516
1.5547614 , 1.5079
11.743105 , 11.4969
1.2103226 , 1.1415
1.6341181 , 1.7741
0.44336295 , 0.4341
0.29164314 , 0.294
2.215507 , 2.226
5.580984 , 5.6928
9.715074 , 10.0007
0.21341515 , 0.1945
0.81041694 , 0.8233
1.0436587 , 1.1087
2.0668647 , 2.0781
0.32240152 , 0.3596
0.23146486 , 0.2144
1.2613473 , 1.137
2.6284945 , 2.7541
4.8744845 , 4.9218
2.2015598 , 2.2553
0.20696831 , 0.1923
2.9442677 , 2.7078
0.40671635 , 0.4611
0.44998312 , 0.524
0.36220598 , 0.3628
1.3655498 , 1.077
1.6193838 , 1.6564
0.7929065 , 0.757
3.777838 , 3.702
3.5904286 , 3.6043
0.3922329 , 0.2973
3.7153115 , 3.9666
0.30252838 , 0.2928
0.38253546 , 0.332
2.0472786 , 2.0998
4.8707185 , 4.9421
8.285189 , 8.2586
5.1415215 , 5.2293
0.36116934 , 0.333
0.21824455 , 0.2259
0.26031303 , 0.2931
7.0216694 , 7.0981
1.1518991 , 1.0857
0.45079684 , 0.5186
0.33551264 , 0.3511
1.4160092 , 1.539
0.32136106 , 0.2729
0.957752 , 0.8806
6.1078825 , 5.929
6.1109214 , 6.1571
0.2830429 , 0.2759
0.3003831 , 0.281
0.4062953 , 0.3968
1.1649742 , 0.351687
1.4732027 , 1.4295
0.24555588 , 0.2324
3.5995357 , 3.6685
6.927742 , 6.8055
3.9816198 , 3.8863
0.22572136 , 0.202
2.9949284 , 2.9766
0.22385311 , 0.2002
0.39312553 , 0.3205
1.6937647 , 1.7204
4.0327997 , 3.9505
2.3493798 , 2.2578
0.2005887 , 0.2131
0.4932804 , 0.4971
4.430073 , 4.5542
8.961857 , 8.7774
0.46245193 , 0.4433
5.5229206 , 5.4898
1.6524031 , 1.6212
0.167768 , 0.168
0.29211235 , 0.2552
0.31827354 , 0.2763
0.29028034 , 0.3366
1.865732 , 1.646
0.22801733 , 0.2958
3.020822 , 3.1111
2.6970067 , 2.6655
0.15310383 , 0.2417
2.6276002 , 2.4566
1.709713 , 1.6139
0.17497015 , 0.1815
0.93160605 , 0.9297
0.3903947 , 0.3462
8.294054 , 8.2723
2.9032035 , 2.8882
0.28121233 , 0.278
0.24498224 , 0.2103
0.23140192 , 0.248
1.0283248 , 0.9854
3.1072984 , 2.9796
0.1674676 , 0.1633
0.39237309 , 0.3721
1.2578373 , 1.2915
0.4507358 , 0.4084
8.951191 , 9.087
0.5140984 , 0.5128
1.5893741 , 1.3833
4.104513 , 4.1469
0.26613283 , 0.3422
0.24854946 , 0.241
0.15650797 , 0.2052
0.29615498 , 0.2566
0.41879892 , 0.4041
0.31740093 , 0.3145
2.5075207 , 2.4978
0.2079587 , 0.1894
1.4832594 , 1.3638
0.1932106 , 0.208
1.2911608 , 1.1992
0.49086213 , 0.4816
1.3401289 , 1.3522
0.40965748 , 0.3866
0.2339344 , 0.2445
1.6215606 , 1.6641
4.2777762 , 4.3308
0.30615044 , 0.2912
2.3619797 , 1.9768
1.4723668 , 1.4369
7.334009 , 7.3858
0.3773985 , 0.4133
2.699079 , 2.5852
3.9729435 , 4.2037
0.14259577 , 0.1377
0.44531894 , 0.4016
0.3497405 , 0.2975
0.266232 , 0.2462
0.16140366 , 0.1494
0.5995631 , 0.6234
0.20973253 , 0.2079
1.7642446 , 1.9532
5.2040424 , 5.2973
0.27297544 , 0.2654
3.0375133 , 3.1932
0.6989353 , 0.638
2.6284041 , 2.7371
6.1147003 , 6.4066
3.7064369 , 3.6491
0.2065258 , 0.2006
5.199664 , 5.2287
1.1185472 , 0.193853
0.8928182 , 0.9036
0.5736449 , 0.5347
3.7080758 , 3.6826
0.41990757 , 0.4089
3.0395634 , 3.0598
0.25853634 , 0.3157
0.45811486 , 0.4205
0.8084326 , 0.7686
0.18584585 , 0.2777
0.1790781 , 0.194
0.42265677 , 0.4041
3.6775959 , 3.6928
4.4100256 , 4.4071
2.7731738 , 2.9709
0.5444999 , 0.7267
4.830489 , 4.9426
0.16326809 , 0.1956
1.2886336 , 1.2606
3.1383286 , 3.2308
0.47650838 , 0.4884
0.31626034 , 0.3192
1.3467276 , 1.2652
7.6879754 , 7.7233
0.47371316 , 0.4635
1.1115246 , 0.9034
4.2444377 , 4.1982
1.0952277 , 1.0294
5.8331566 , 6.0788
1.7884095 , 1.8449
4.842618 , 4.8009
0.41941857 , 0.4265
0.38777924 , 0.4105
0.22057295 , 0.2316
0.3928771 , 0.3579
2.480747 , 2.6227
0.58743334 , 0.5169
0.81864524 , 0.8871
0.27031708 , 0.2981
0.35936785 , 0.3043
2.0291774 , 1.9143
8.74929 , 8.9148
1.2230248 , 1.1808
3.29371 , 3.2983
0.22528172 , 0.3317
2.1366494 , 2.0613
9.462122 , 9.7493
2.674757 , 2.8147
3.9354239 , 3.8505
2.679369 , 2.7975
0.38393497 , 0.3727
6.452729 , 6.2399
3.5937996 , 3.5984
2.6872702 , 2.6243
4.172315 , 4.2541
0.48797894 , 0.4801
0.5198114 , 0.6598
0.3234768 , 0.2861
2.9258187 , 2.8464
1.0831096 , 1.156
0.63345957 , 0.522
3.138693 , 3.2553
0.3500433 , 0.3368
0.53767395 , 0.5582
2.720729 , 2.5913
3.2519336 , 3.1092
6.9488053 , 6.8635
RMSE:  0.11277416964350363  MAPE: 0.0968133061919351
5: ground truth total-  372  predicted total -  372
100: ground truth total-  36  predicted total -  36
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
0.80375385 , 0.729948
0.36380625 , 0.045928
0.25133848 , 0.045988
0.6912885 , 0.735312
1.1765852 , 1.457218
1.037894 , 0.726571
0.93488455 , 0.364232
0.8252063 , 0.364531
0.5813012 , 0.367084
0.9321165 , 0.729025
1.0981514 , 1.094628
0.616956 , 0.045664
1.2470562 , 2.54928
1.111376 , 0.727164
1.1495211 , 1.09287
0.49681997 , 0.045811
0.7278311 , 0.045643
1.0368586 , 0.364837
1.2325573 , 2.185884
0.69376564 , 0.366018
RMSE:  0.5128807716312254  MAPE: 2.871546735660585
