['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1709
19
<class 'numpy.dtype'> float64
1709
train batches:  77  validate samples: 136  test samples: 341
Epoch [0/150], Batch loss: 2.08
Epoch: 0 RMSE:  0.43307502107234397  MAPE: 0.9655423409762094  L2+L1 loss: 0.566
Epoch [1/150], Batch loss: 1.044
Epoch: 1 RMSE:  0.47565076049986954  MAPE: 1.8901517326392148  L2+L1 loss: 0.722
Epoch [2/150], Batch loss: 1.033
Epoch: 2 RMSE:  0.518329123504985  MAPE: 2.2137345246844204  L2+L1 loss: 0.779
Epoch [3/150], Batch loss: 1.072
Epoch: 3 RMSE:  0.46316923199184973  MAPE: 1.7765073485344158  L2+L1 loss: 0.703
Epoch [4/150], Batch loss: 1.087
Epoch: 4 RMSE:  0.46164057412961174  MAPE: 1.7686852170713312  L2+L1 loss: 0.7
Epoch [5/150], Batch loss: 1.012
Epoch: 5 RMSE:  0.5190681049760562  MAPE: 2.218824394382789  L2+L1 loss: 0.78
Epoch [6/150], Batch loss: 1.085
Epoch: 6 RMSE:  0.47361737264433  MAPE: 1.8725509398244877  L2+L1 loss: 0.719
Epoch [7/150], Batch loss: 1.071
Epoch: 7 RMSE:  0.537520569545896  MAPE: 2.3419530048966344  L2+L1 loss: 0.804
Epoch [8/150], Batch loss: 1.058
Epoch: 8 RMSE:  0.4604866944173153  MAPE: 1.7498373987657527  L2+L1 loss: 0.699
Epoch [9/150], Batch loss: 1.082
Epoch: 9 RMSE:  0.4677166604479988  MAPE: 1.8196753298440744  L2+L1 loss: 0.71
Epoch [10/150], Batch loss: 1.062
Epoch: 10 RMSE:  0.4438602942283578  MAPE: 1.5550362728113092  L2+L1 loss: 0.666
Epoch [11/150], Batch loss: 1.062
Epoch: 11 RMSE:  0.445501959299622  MAPE: 1.5777033907049265  L2+L1 loss: 0.67
Epoch [12/150], Batch loss: 1.048
Epoch: 12 RMSE:  0.4305959192675935  MAPE: 1.2821985615900022  L2+L1 loss: 0.617
Epoch [13/150], Batch loss: 1.066
Epoch: 13 RMSE:  0.6036522540599599  MAPE: 2.747151552595168  L2+L1 loss: 0.901
Epoch [14/150], Batch loss: 1.045
Epoch: 14 RMSE:  0.4312800942303765  MAPE: 1.3072294999807559  L2+L1 loss: 0.621
Epoch [15/150], Batch loss: 1.04
Epoch: 15 RMSE:  0.5765857948361304  MAPE: 2.587776015420508  L2+L1 loss: 0.863
Epoch [16/150], Batch loss: 1.094
Epoch: 16 RMSE:  0.44021993147606986  MAPE: 1.5000410434124414  L2+L1 loss: 0.655
Epoch [17/150], Batch loss: 1.057
Epoch: 17 RMSE:  0.480673019037007  MAPE: 1.9322735427211004  L2+L1 loss: 0.728
Epoch [18/150], Batch loss: 1.696
Epoch: 18 RMSE:  0.47736504967528803  MAPE: 1.9047367472924275  L2+L1 loss: 0.724
Epoch [19/150], Batch loss: 1.085
Epoch: 19 RMSE:  0.4823698275745255  MAPE: 1.946109357399314  L2+L1 loss: 0.73
Epoch [20/150], Batch loss: 1.057
Epoch: 20 RMSE:  0.5415768402635824  MAPE: 2.368317979036567  L2+L1 loss: 0.809
Epoch [21/150], Batch loss: 1.085
Epoch: 21 RMSE:  0.613515071457107  MAPE: 2.803342411564346  L2+L1 loss: 0.914
Epoch [22/150], Batch loss: 1.094
Epoch: 22 RMSE:  0.4300197999298411  MAPE: 1.2566432391958435  L2+L1 loss: 0.613
Epoch [23/150], Batch loss: 1.054
Epoch: 23 RMSE:  17.221938030685763  MAPE: 63.00306759602702  L2+L1 loss: 6.558
Epoch [24/150], Batch loss: 1.133
Epoch: 24 RMSE:  0.43628612653164733  MAPE: 1.4308356362740489  L2+L1 loss: 0.643
Epoch [25/150], Batch loss: 1.075
Epoch: 25 RMSE:  0.4656021568373788  MAPE: 1.799898659143429  L2+L1 loss: 0.707
Epoch [26/150], Batch loss: 1.421
Epoch: 26 RMSE:  0.5001223848923404  MAPE: 2.0833919342434477  L2+L1 loss: 0.753
Epoch [27/150], Batch loss: 1.083
Epoch: 27 RMSE:  0.4914932993420104  MAPE: 2.018218258832116  L2+L1 loss: 0.741
Epoch [28/150], Batch loss: 1.066
Epoch: 28 RMSE:  0.4650999158370201  MAPE: 1.7951277555619276  L2+L1 loss: 0.706
Epoch [29/150], Batch loss: 1.085
Epoch: 29 RMSE:  0.45232880677524623  MAPE: 1.6621986169943008  L2+L1 loss: 0.684
Epoch [30/150], Batch loss: 1.304
Epoch: 30 RMSE:  0.4486473185107625  MAPE: 1.0258610057944006  L2+L1 loss: 0.589
Epoch [31/150], Batch loss: 1.075
Epoch: 31 RMSE:  0.47021817736734606  MAPE: 1.8424692083124168  L2+L1 loss: 0.714
Epoch [32/150], Batch loss: 1.058
Epoch: 32 RMSE:  0.4505504254660319  MAPE: 1.6414472069802155  L2+L1 loss: 0.681
Epoch [33/150], Batch loss: 1.069
Epoch: 33 RMSE:  0.45287641609748264  MAPE: 1.6684398166709637  L2+L1 loss: 0.685
Epoch [34/150], Batch loss: 1.056
Epoch: 34 RMSE:  0.5185017468252422  MAPE: 2.214924906740446  L2+L1 loss: 0.78
Epoch [35/150], Batch loss: 1.041
Epoch: 35 RMSE:  0.47682953727884425  MAPE: 1.9002046943066953  L2+L1 loss: 0.723
Epoch [36/150], Batch loss: 1.076
Epoch: 36 RMSE:  0.47653166175173434  MAPE: 1.8976744065560784  L2+L1 loss: 0.723
Epoch [37/150], Batch loss: 1.075
Epoch: 37 RMSE:  0.4722713414629653  MAPE: 1.8607305858192684  L2+L1 loss: 0.717
Epoch [38/150], Batch loss: 1.066
Epoch: 38 RMSE:  0.4716927529457694  MAPE: 1.8556232428457744  L2+L1 loss: 0.716
Epoch [39/150], Batch loss: 1.06
Epoch: 39 RMSE:  0.46532937933901636  MAPE: 1.7973111024636812  L2+L1 loss: 0.707
Epoch [40/150], Batch loss: 1.073
Epoch: 40 RMSE:  0.4748969539607677  MAPE: 1.883666022104537  L2+L1 loss: 0.72
Epoch [41/150], Batch loss: 1.081
Epoch: 41 RMSE:  0.47676522402379456  MAPE: 1.8996589589493538  L2+L1 loss: 0.723
Epoch [42/150], Batch loss: 1.078
Epoch: 42 RMSE:  0.4662267252546965  MAPE: 1.8057913929009695  L2+L1 loss: 0.708
Epoch [43/150], Batch loss: 1.056
Epoch: 43 RMSE:  0.47444742887931746  MAPE: 1.8797765818808658  L2+L1 loss: 0.72
Epoch [44/150], Batch loss: 1.059
Epoch: 44 RMSE:  0.46742297616204365  MAPE: 1.816957496111625  L2+L1 loss: 0.71
Epoch [45/150], Batch loss: 1.078
Epoch: 45 RMSE:  0.4672495798011284  MAPE: 1.8153485525786002  L2+L1 loss: 0.71
Epoch [46/150], Batch loss: 1.08
Epoch: 46 RMSE:  0.46545349469891456  MAPE: 1.7984895176222067  L2+L1 loss: 0.707
Epoch [47/150], Batch loss: 1.069
Epoch: 47 RMSE:  0.4634340708198029  MAPE: 1.7790887642083124  L2+L1 loss: 0.704
Epoch [48/150], Batch loss: 1.03
Epoch: 48 RMSE:  0.47496204651330554  MAPE: 1.8842278669164216  L2+L1 loss: 0.721
Epoch [49/150], Batch loss: 1.067
Epoch: 49 RMSE:  0.47442338035901377  MAPE: 1.8795680416816392  L2+L1 loss: 0.72
Epoch [50/150], Batch loss: 1.025
Epoch: 50 RMSE:  0.38989767525092023  MAPE: 1.5298342854973659  L2+L1 loss: 0.635
Epoch [51/150], Batch loss: 0.959
Epoch: 51 RMSE:  0.44238322291830406  MAPE: 1.5335999085000354  L2+L1 loss: 0.661
Epoch [52/150], Batch loss: 0.994
Epoch: 52 RMSE:  0.45613756248504106  MAPE: 1.7045222952020511  L2+L1 loss: 0.691
Epoch [53/150], Batch loss: 1.073
Epoch: 53 RMSE:  0.46614785128046715  MAPE: 1.8050496471227948  L2+L1 loss: 0.708
Epoch [54/150], Batch loss: 1.06
Epoch: 54 RMSE:  0.4671968210720583  MAPE: 1.8148583687186886  L2+L1 loss: 0.71
Epoch [55/150], Batch loss: 1.051
Epoch: 55 RMSE:  0.4611942483622223  MAPE: 1.704188086551675  L2+L1 loss: 0.702
Epoch [56/150], Batch loss: 0.988
Epoch: 56 RMSE:  0.4455673365399034  MAPE: 1.3141987722945994  L2+L1 loss: 0.625
Epoch [57/150], Batch loss: 0.85
Epoch: 57 RMSE:  0.7353636182199984  MAPE: 1.356550851555221  L2+L1 loss: 0.695
Epoch [58/150], Batch loss: 0.717
Epoch: 58 RMSE:  0.40334597752137924  MAPE: 1.1244893410276173  L2+L1 loss: 0.586
Epoch [59/150], Batch loss: 0.75
Epoch: 59 RMSE:  0.332127865831594  MAPE: 1.2174825440756423  L2+L1 loss: 0.532
Epoch [60/150], Batch loss: 0.647
Epoch: 60 RMSE:  0.31329609357465427  MAPE: 0.7604621175223139  L2+L1 loss: 0.485
Epoch [61/150], Batch loss: 0.766
Epoch: 61 RMSE:  0.4439466055989934  MAPE: 0.8104170009813222  L2+L1 loss: 0.551
Epoch [62/150], Batch loss: 0.961
Epoch: 62 RMSE:  0.44368941900742176  MAPE: 1.5526100413339672  L2+L1 loss: 0.665
Epoch [63/150], Batch loss: 1.061
Epoch: 63 RMSE:  0.4681353388295066  MAPE: 1.8235342940566663  L2+L1 loss: 0.711
Epoch [64/150], Batch loss: 1.005
Epoch: 64 RMSE:  0.3250132532485236  MAPE: 0.9506794772526022  L2+L1 loss: 0.458
Epoch [65/150], Batch loss: 0.814
Epoch: 65 RMSE:  0.43409576696818014  MAPE: 1.4781788470384283  L2+L1 loss: 0.637
Epoch [66/150], Batch loss: 1.003
Epoch: 66 RMSE:  0.49756608488301324  MAPE: 1.017618539685866  L2+L1 loss: 0.639
Epoch [67/150], Batch loss: 0.827
Epoch: 67 RMSE:  0.30750140632379863  MAPE: 0.39197295425440976  L2+L1 loss: 0.397
Epoch [68/150], Batch loss: 1.04
Epoch: 68 RMSE:  0.478566827826976  MAPE: 1.0492369186105617  L2+L1 loss: 0.641
Epoch [69/150], Batch loss: 0.91
Epoch: 69 RMSE:  0.35055440470730304  MAPE: 0.5067058141812233  L2+L1 loss: 0.475
Epoch [70/150], Batch loss: 0.684
Epoch: 70 RMSE:  0.46309359359134783  MAPE: 0.5503574833748159  L2+L1 loss: 0.513
Epoch [71/150], Batch loss: 0.623
Epoch: 71 RMSE:  0.49992051096015905  MAPE: 0.5752183059944432  L2+L1 loss: 0.526
Epoch [72/150], Batch loss: 0.668
Epoch: 72 RMSE:  0.4858405723553926  MAPE: 0.5845353907362362  L2+L1 loss: 0.526
Epoch [73/150], Batch loss: 0.649
Epoch: 73 RMSE:  0.4193490663612243  MAPE: 0.5826942219957607  L2+L1 loss: 0.508
Epoch [74/150], Batch loss: 0.631
Epoch: 74 RMSE:  0.5361242779955036  MAPE: 0.6513140025716602  L2+L1 loss: 0.546
Epoch [75/150], Batch loss: 0.643
Epoch: 75 RMSE:  0.40176748084246566  MAPE: 0.6482681435740645  L2+L1 loss: 0.506
Epoch [76/150], Batch loss: 0.64
Epoch: 76 RMSE:  0.3812478195304515  MAPE: 0.6592709990341276  L2+L1 loss: 0.496
Epoch [77/150], Batch loss: 0.624
Epoch: 77 RMSE:  0.33422945490219824  MAPE: 0.6827182953049172  L2+L1 loss: 0.494
Epoch [78/150], Batch loss: 0.678
Epoch: 78 RMSE:  0.3695355454610989  MAPE: 0.7053752056111214  L2+L1 loss: 0.508
Epoch [79/150], Batch loss: 0.652
Epoch: 79 RMSE:  0.33085935847738757  MAPE: 0.7318242401754458  L2+L1 loss: 0.5
Epoch [80/150], Batch loss: 0.617
Epoch: 80 RMSE:  0.44721713662745677  MAPE: 0.7839618937935429  L2+L1 loss: 0.536
Epoch [81/150], Batch loss: 0.642
Epoch: 81 RMSE:  0.33214494392252264  MAPE: 0.7587498207252449  L2+L1 loss: 0.505
Epoch [82/150], Batch loss: 0.676
Epoch: 82 RMSE:  0.3305610753530546  MAPE: 0.7788549485007766  L2+L1 loss: 0.507
Epoch [83/150], Batch loss: 0.63
Epoch: 83 RMSE:  0.5043998317231083  MAPE: 1.0521808687475769  L2+L1 loss: 0.585
Epoch [84/150], Batch loss: 0.62
Epoch: 84 RMSE:  0.3223565799681258  MAPE: 0.762661997880104  L2+L1 loss: 0.49
Epoch [85/150], Batch loss: 0.597
Epoch: 85 RMSE:  0.43009208185622105  MAPE: 0.9281529831850758  L2+L1 loss: 0.546
Epoch [86/150], Batch loss: 0.653
Epoch: 86 RMSE:  0.3440082001256025  MAPE: 0.7981643046347509  L2+L1 loss: 0.493
Epoch [87/150], Batch loss: 0.65
Epoch: 87 RMSE:  0.3053771482245028  MAPE: 0.681873025860232  L2+L1 loss: 0.458
Epoch [88/150], Batch loss: 0.581
Epoch: 88 RMSE:  0.29168313326419404  MAPE: 0.7110748163986262  L2+L1 loss: 0.459
Epoch [89/150], Batch loss: 0.591
Epoch: 89 RMSE:  0.30581347694341776  MAPE: 0.6558959154101857  L2+L1 loss: 0.444
Epoch [90/150], Batch loss: 0.55
Epoch: 90 RMSE:  0.27129251549967687  MAPE: 0.6617607659580996  L2+L1 loss: 0.436
Epoch [91/150], Batch loss: 0.576
Epoch: 91 RMSE:  0.26503867461354125  MAPE: 0.6321660548777415  L2+L1 loss: 0.419
Epoch [92/150], Batch loss: 0.547
Epoch: 92 RMSE:  0.2724647938951897  MAPE: 0.621815302934496  L2+L1 loss: 0.42
Epoch [93/150], Batch loss: 0.582
Epoch: 93 RMSE:  0.25639885496307857  MAPE: 0.5318796617654321  L2+L1 loss: 0.386
Epoch [94/150], Batch loss: 0.515
Epoch: 94 RMSE:  0.3667715765299711  MAPE: 0.349715539043285  L2+L1 loss: 0.357
Epoch [95/150], Batch loss: 0.595
Epoch: 95 RMSE:  0.24110198660082466  MAPE: 0.5427742232926803  L2+L1 loss: 0.39
Epoch [96/150], Batch loss: 0.544
Epoch: 96 RMSE:  0.2279115455189719  MAPE: 0.4226871906392178  L2+L1 loss: 0.368
Epoch [97/150], Batch loss: 0.558
Epoch: 97 RMSE:  0.2216105718828565  MAPE: 0.3198519785629429  L2+L1 loss: 0.338
Epoch [98/150], Batch loss: 0.536
Epoch: 98 RMSE:  0.356343020769988  MAPE: 0.5531472825539374  L2+L1 loss: 0.452
Epoch [99/150], Batch loss: 0.528
Epoch: 99 RMSE:  0.4507062701926626  MAPE: 0.48199446527294754  L2+L1 loss: 0.458
Epoch [100/150], Batch loss: 0.513
Epoch: 100 RMSE:  0.24791142024144389  MAPE: 0.292391146463925  L2+L1 loss: 0.334
Epoch [101/150], Batch loss: 0.506
Epoch: 101 RMSE:  0.2374419990443119  MAPE: 0.25454207073701424  L2+L1 loss: 0.313
Epoch [102/150], Batch loss: 0.504
Epoch: 102 RMSE:  0.19710717232614633  MAPE: 0.21803161107099656  L2+L1 loss: 0.292
Epoch [103/150], Batch loss: 0.504
Epoch: 103 RMSE:  0.21349112908926607  MAPE: 0.23485881838175918  L2+L1 loss: 0.307
Epoch [104/150], Batch loss: 0.492
Epoch: 104 RMSE:  0.2096920973923897  MAPE: 0.32943148114716436  L2+L1 loss: 0.33
Epoch [105/150], Batch loss: 0.472
Epoch: 105 RMSE:  0.20811687993235597  MAPE: 0.24301892042528753  L2+L1 loss: 0.309
Epoch [106/150], Batch loss: 0.463
Epoch: 106 RMSE:  0.2139559519523135  MAPE: 0.23890119918393135  L2+L1 loss: 0.312
Epoch [107/150], Batch loss: 0.477
Epoch: 107 RMSE:  0.20961422959701267  MAPE: 0.23561449268986945  L2+L1 loss: 0.308
Epoch [108/150], Batch loss: 0.491
Epoch: 108 RMSE:  0.21168080961268512  MAPE: 0.2373663807946269  L2+L1 loss: 0.309
Epoch [109/150], Batch loss: 0.476
Epoch: 109 RMSE:  0.2094963967006842  MAPE: 0.22955999458003623  L2+L1 loss: 0.306
Epoch [110/150], Batch loss: 0.473
Epoch: 110 RMSE:  0.2111649450481344  MAPE: 0.2341108868728981  L2+L1 loss: 0.308
Epoch [111/150], Batch loss: 0.487
Epoch: 111 RMSE:  0.21021350123416394  MAPE: 0.23357154143124154  L2+L1 loss: 0.306
Epoch [112/150], Batch loss: 0.494
Epoch: 112 RMSE:  0.20899134406610848  MAPE: 0.22916485447081994  L2+L1 loss: 0.305
Epoch [113/150], Batch loss: 0.493
Epoch: 113 RMSE:  0.20974613010042234  MAPE: 0.22863019708936022  L2+L1 loss: 0.305
Epoch [114/150], Batch loss: 0.489
Epoch: 114 RMSE:  0.20453296111828254  MAPE: 0.2281809086009189  L2+L1 loss: 0.302
Epoch [115/150], Batch loss: 0.485
Epoch: 115 RMSE:  0.20612211730605265  MAPE: 0.23247016818872387  L2+L1 loss: 0.303
Epoch [116/150], Batch loss: 0.47
Epoch: 116 RMSE:  0.21065136572173243  MAPE: 0.23467569462261056  L2+L1 loss: 0.307
Epoch [117/150], Batch loss: 0.472
Epoch: 117 RMSE:  0.2121163631290112  MAPE: 0.2379541092057752  L2+L1 loss: 0.308
Epoch [118/150], Batch loss: 0.477
Epoch: 118 RMSE:  0.21389339805487848  MAPE: 0.23210483138596993  L2+L1 loss: 0.308
Epoch [119/150], Batch loss: 0.479
Epoch: 119 RMSE:  0.2111385497641707  MAPE: 0.22850691238321533  L2+L1 loss: 0.305
Epoch [120/150], Batch loss: 0.472
Epoch: 120 RMSE:  0.21017800229970393  MAPE: 0.23705483683699474  L2+L1 loss: 0.307
Epoch [121/150], Batch loss: 0.47
Epoch: 121 RMSE:  0.20991274728356335  MAPE: 0.22784722892389664  L2+L1 loss: 0.305
Epoch [122/150], Batch loss: 0.469
Epoch: 122 RMSE:  0.2104950384371678  MAPE: 0.228977832168204  L2+L1 loss: 0.305
Epoch [123/150], Batch loss: 0.483
Epoch: 123 RMSE:  0.21132657256878176  MAPE: 0.22787907328598078  L2+L1 loss: 0.306
Epoch [124/150], Batch loss: 0.467
Epoch: 124 RMSE:  0.2080106642358359  MAPE: 0.2264482013983629  L2+L1 loss: 0.303
Epoch [125/150], Batch loss: 0.479
Epoch: 125 RMSE:  0.21043625487529502  MAPE: 0.22673402981791407  L2+L1 loss: 0.306
Epoch [126/150], Batch loss: 0.474
Epoch: 126 RMSE:  0.2091354173538314  MAPE: 0.2239573237227983  L2+L1 loss: 0.304
Epoch [127/150], Batch loss: 0.487
Epoch: 127 RMSE:  0.2082857020071163  MAPE: 0.22057962890631422  L2+L1 loss: 0.302
Epoch [128/150], Batch loss: 0.483
Epoch: 128 RMSE:  0.20757141411295857  MAPE: 0.22204747919621695  L2+L1 loss: 0.302
Epoch [129/150], Batch loss: 0.465
Epoch: 129 RMSE:  0.20800213261615794  MAPE: 0.2249035459907858  L2+L1 loss: 0.303
Epoch [130/150], Batch loss: 0.496
Epoch: 130 RMSE:  0.20579071421484907  MAPE: 0.2214614176372652  L2+L1 loss: 0.301
Epoch [131/150], Batch loss: 0.474
Epoch: 131 RMSE:  0.20716142338642507  MAPE: 0.2257609107692966  L2+L1 loss: 0.302
Epoch [132/150], Batch loss: 0.491
Epoch: 132 RMSE:  0.20786491498456275  MAPE: 0.2217222573580367  L2+L1 loss: 0.302
Epoch [133/150], Batch loss: 0.485
Epoch: 133 RMSE:  0.21174397078801663  MAPE: 0.22872179591831918  L2+L1 loss: 0.307
Epoch [134/150], Batch loss: 0.495
Epoch: 134 RMSE:  0.2075635121263974  MAPE: 0.22042705200610402  L2+L1 loss: 0.301
Epoch [135/150], Batch loss: 0.468
Epoch: 135 RMSE:  0.20723060089006085  MAPE: 0.22347401506695613  L2+L1 loss: 0.302
Epoch [136/150], Batch loss: 0.467
Epoch: 136 RMSE:  0.21060698055299326  MAPE: 0.22990499685461155  L2+L1 loss: 0.307
Epoch [137/150], Batch loss: 0.464
Epoch: 137 RMSE:  0.20800405858874677  MAPE: 0.22049988205818444  L2+L1 loss: 0.302
Epoch [138/150], Batch loss: 0.462
Epoch: 138 RMSE:  0.20902478077450648  MAPE: 0.23086521145915526  L2+L1 loss: 0.306
Epoch [139/150], Batch loss: 0.484
Epoch: 139 RMSE:  0.20681066413454954  MAPE: 0.2208769620307811  L2+L1 loss: 0.301
Epoch [140/150], Batch loss: 0.454
Epoch: 140 RMSE:  0.2069892188226846  MAPE: 0.22140833010626743  L2+L1 loss: 0.302
Epoch [141/150], Batch loss: 0.452
Epoch: 141 RMSE:  0.20704436928595882  MAPE: 0.22172475257054094  L2+L1 loss: 0.302
Epoch [142/150], Batch loss: 0.47
Epoch: 142 RMSE:  0.20680240807467998  MAPE: 0.2213978517253746  L2+L1 loss: 0.301
Epoch [143/150], Batch loss: 0.473
Epoch: 143 RMSE:  0.2069798684035384  MAPE: 0.2221609353745723  L2+L1 loss: 0.302
Epoch [144/150], Batch loss: 0.471
Epoch: 144 RMSE:  0.20721411551334945  MAPE: 0.2226783664118828  L2+L1 loss: 0.302
Epoch [145/150], Batch loss: 0.473
Epoch: 145 RMSE:  0.2071116695560966  MAPE: 0.22267408709416456  L2+L1 loss: 0.302
Epoch [146/150], Batch loss: 0.469
Epoch: 146 RMSE:  0.2071617234022962  MAPE: 0.2228552651785994  L2+L1 loss: 0.302
Epoch [147/150], Batch loss: 0.473
Epoch: 147 RMSE:  0.20727537424998338  MAPE: 0.22299220007483472  L2+L1 loss: 0.302
Epoch [148/150], Batch loss: 0.471
Epoch: 148 RMSE:  0.20730708916505236  MAPE: 0.22288459869259866  L2+L1 loss: 0.302
Epoch [149/150], Batch loss: 0.481
Epoch: 149 RMSE:  0.2073423902991535  MAPE: 0.22316325035748122  L2+L1 loss: 0.303


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.6768148 , 0.4907
0.15842795 , 0.1083
0.8448647 , 1.36
0.16673073 , 0.184
0.16660237 , 0.1798
0.1486381 , 0.1171
0.8797835 , 0.6732
0.9309695 , 0.6987
0.15713057 , 0.3055
0.2919502 , 0.3093
1.4932536 , 1.2568
0.14437188 , 0.2119
0.14603402 , 0.1448
0.76201344 , 1.3602
1.2173727 , 1.1399
0.35795802 , 0.3892
1.5129731 , 1.1596
1.4917116 , 2.7378
0.5569014 , 0.514
0.2026418 , 0.3253
0.15320921 , 0.1351
0.38303992 , 0.2944
0.18052506 , 0.2668
0.97362006 , 0.7148
0.22904448 , 0.2481
0.55683446 , 0.5806
0.1547252 , 0.1264
0.2178466 , 0.1704
0.14862722 , 0.1128
1.3873043 , 2.2934
0.2908398 , 0.2402
0.82814884 , 0.9081
0.4604221 , 0.5153
0.15773842 , 0.204
0.72336495 , 0.5742
0.43115392 , 0.5704
0.14600576 , 0.1187
1.0711571 , 1.6339
0.5641005 , 0.4557
0.695632 , 0.618
0.6044821 , 0.5024
0.15327364 , 0.1283
1.1650794 , 1.6718
0.25742474 , 0.1892
0.4476725 , 0.7617
0.15456064 , 0.1528
2.6119678 , 3.9868
1.3762909 , 2.1896
0.14176299 , 0.1594
0.28996187 , 0.2795
0.23719864 , 0.233
0.46015334 , 0.5696
0.25173035 , 0.211
0.1399496 , 0.1188
0.46257246 , 0.3116
0.6193321 , 0.6359
0.23240758 , 0.3526
0.6735394 , 0.6884
0.14284708 , 0.1639
0.7955072 , 0.6743
0.22840804 , 0.1494
0.3523348 , 0.3209
0.14548115 , 0.1836
0.16234232 , 0.1475
0.15237203 , 0.2565
0.15716901 , 0.1434
0.15615998 , 0.1911
0.66245615 , 0.6485
0.24674627 , 0.3563
0.14971247 , 0.1118
0.1454684 , 0.1146
0.504386 , 0.3795
0.78301096 , 0.5449
1.023754 , 1.0943
0.13982002 , 0.1116
1.3077512 , 0.8747
0.14392881 , 0.1973
0.13010654 , 0.2174
0.38207853 , 0.6338
0.14976336 , 0.1273
0.95766795 , 0.9507
0.17926677 , 0.2262
0.15702549 , 0.1936
0.1497622 , 0.1111
0.43307734 , 0.4163
0.1494659 , 0.1741
0.41584617 , 0.3183
0.39895162 , 0.366
0.14798132 , 0.104
0.15947309 , 0.2478
0.14545642 , 0.1107
0.3500348 , 0.2711
0.6276872 , 0.6311
0.17243345 , 0.1308
0.13974209 , 0.3163
0.14255649 , 0.1129
0.5404582 , 0.7668
1.122617 , 1.0636
0.29652944 , 0.2626
0.14107461 , 0.1083
0.1480187 , 0.1321
1.1343753 , 0.7833
0.88173056 , 0.9192
0.149752 , 0.1452
0.58862454 , 0.3978
0.77097476 , 0.6221
0.44220182 , 0.3711
0.32218346 , 0.2769
0.8279226 , 0.6157
0.15793023 , 0.1292
0.15167342 , 0.1265
0.15646419 , 0.1368
0.15233544 , 0.1403
0.2959981 , 0.2572
1.2706143 , 1.0667
0.4617647 , 0.4819
0.9567015 , 1.2372
0.9436704 , 0.8164
0.25661486 , 0.2064
0.4431028 , 0.3938
0.13638914 , 0.1001
0.018598795 , 0.01815
0.21722658 , 0.2397
0.33759966 , 0.3799
0.15723883 , 0.1239
0.26004237 , 0.2748
15.137686 , 12.7723
0.4615492 , 0.4312
0.1349547 , 0.1322
0.14856479 , 0.1423
16.893045 , 32.9235
0.41065598 , 0.4541
0.16966648 , 0.2133
0.38923505 , 0.3478
0.17088926 , 0.1994
0.34513745 , 0.3085
0.22813421 , 0.302
0.34659684 , 0.5449
0.16975123 , 0.1008
0.12954059 , 0.124
0.41973424 , 0.39
0.52646935 , 0.6425
0.49602026 , 0.5758
0.21969967 , 0.2201
1.0685818 , 1.0729
0.37194425 , 0.3759
0.5571146 , 0.558
0.21228397 , 0.1534
1.0054688 , 1.218
1.3989366 , 1.0054
0.5392529 , 0.4289
0.7009617 , 0.9506
0.062227488 , 0.07656
0.14614396 , 0.1954
1.0317532 , 1.052
0.25652453 , 0.217
0.5394709 , 0.4381
0.14510098 , 0.1416
0.16258976 , 0.1198
0.203763 , 0.2342
0.9538394 , 1.1096
0.36278865 , 0.429
0.16730042 , 0.2405
0.16869938 , 0.1851
0.14566413 , 0.1305
0.14708316 , 0.1297
0.16248055 , 0.2984
0.16561005 , 0.1765
0.1523287 , 0.1352
0.597677 , 0.5001
0.32197154 , 0.3113
0.17150213 , 0.1261
0.17888778 , 0.2051
0.8048996 , 0.6711
0.34140742 , 0.2515
0.39295584 , 0.4445
0.35531077 , 0.2231
0.13546917 , 0.1986
0.16337666 , 0.1442
0.17931032 , 0.2289
0.5220518 , 0.4244
1.8175291 , 1.4574
0.13446495 , 0.1621
0.6355405 , 0.5218
0.15706594 , 0.1202
0.3120812 , 0.3454
0.14314504 , 0.2111
0.8489231 , 0.6121
0.15514944 , 0.1401
0.68655795 , 0.6614
0.36906186 , 0.3845
1.136501 , 1.8818
1.4485933 , 2.527
0.15192018 , 0.1058
0.157733 , 0.1606
0.1432889 , 0.1045
0.48300928 , 0.9539
0.14826761 , 0.1752
0.40052667 , 0.5009
0.16407728 , 0.1142
0.943329 , 0.9875
0.15880875 , 0.1001
0.53649306 , 0.5062
0.14542446 , 0.1011
0.74795634 , 0.5359
0.69968474 , 0.5608
0.21960336 , 0.1416
0.1460723 , 0.2426
0.72578007 , 0.7137
0.15271778 , 0.2249
0.17458302 , 0.2214
0.64032876 , 0.4659
0.16127107 , 0.1886
0.15513073 , 0.1238
0.3454334 , 0.3814
0.50211483 , 0.4752
0.32271683 , 0.5191
0.23501799 , 0.336
0.33493972 , 0.2856
0.4410293 , 0.5001
0.15995899 , 0.2731
0.8152939 , 0.6759
0.34513095 , 0.4391
0.1673697 , 0.1725
0.7924639 , 0.8929
1.2386805 , 1.4003
0.1598193 , 0.1452
1.5828667 , 3.2483
0.120028466 , 0.1287
1.1817056 , 0.953
0.9126179 , 1.2487
0.3172178 , 0.2914
0.62301517 , 0.52
0.17933404 , 0.1454
0.5200108 , 0.4252
1.069536 , 1.6461
0.70680964 , 0.9945
0.25950706 , 0.326
0.63988876 , 0.5808
2.5310824 , 1.721
0.76808715 , 0.8008
0.16348018 , 0.1237
0.15583608 , 0.1641
0.2313984 , 0.2282
0.17878157 , 0.1623
1.184172 , 0.8161
0.17103063 , 0.123
0.15257064 , 0.1236
0.47536218 , 0.499
0.4162355 , 0.3477
1.467356 , 1.1143
0.5835587 , 0.4308
0.86778843 , 0.7115
0.81007755 , 0.8475
0.13415137 , 0.2251
0.15907551 , 0.1558
0.21299982 , 0.1298
0.15412523 , 0.1154
0.18036735 , 0.1879
0.53874946 , 0.4136
0.46034005 , 0.3557
0.15435989 , 0.1653
0.8538674 , 1.1237
0.14414157 , 0.1118
0.620761 , 0.4775
0.13926724 , 0.1943
0.25801528 , 0.2118
0.15490349 , 0.1082
0.79181993 , 1.2222
0.54499424 , 0.6894
0.81435025 , 0.65
0.88951707 , 0.6707
0.99502826 , 1.4332
0.2459195 , 0.2592
0.19625024 , 0.1536
0.4207917 , 0.3742
0.5754639 , 0.4202
0.15678932 , 0.1556
0.16011003 , 0.1374
0.559605 , 0.5123
0.14537187 , 0.1174
0.16359952 , 0.1732
0.74929434 , 0.6411
0.96507084 , 0.712
0.13944541 , 0.1535
0.5030292 , 0.4891
0.16746369 , 0.1258
0.60913056 , 0.7591
0.91988945 , 1.1414
0.14895059 , 0.1298
1.2665883 , 1.7596
0.9046067 , 0.6195
0.15223609 , 0.1102
0.24373952 , 0.2483
0.53287923 , 0.4218
0.4165839 , 0.3576
0.8456304 , 0.593
0.3228397 , 0.3605
0.14949036 , 0.1115
0.503814 , 0.4021
0.4953509 , 0.5492
0.43547195 , 0.4073
0.14004214 , 0.1552
0.5823325 , 0.5014
0.13924578 , 0.1291
0.46251953 , 0.5313
1.3047726 , 1.969
0.40389934 , 0.4323
0.16208503 , 0.1859
1.5807239 , 1.2125
0.14759521 , 0.1939
1.6802574 , 1.3277
0.15180178 , 0.1214
0.32163396 , 0.4626
0.30023333 , 0.2593
0.346832 , 0.2957
0.84289026 , 0.9817
1.2120032 , 0.8354
0.29848853 , 0.4573
0.82427824 , 1.3338
0.30623892 , 0.2951
0.15059096 , 0.1647
0.1856518 , 0.1611
0.29132202 , 0.255
0.14418842 , 0.1121
0.5677489 , 0.4706
0.13714026 , 0.1066
0.49153024 , 0.4247
1.1703792 , 1.6859
0.20465134 , 0.1631
0.11047126 , 0.1227
0.14759375 , 0.1475
0.19927981 , 0.2181
0.5043217 , 0.5205
0.13727959 , 0.1129
0.24091396 , 0.3004
0.13396461 , 0.137
0.14292002 , 0.1104
0.61052215 , 0.8093
0.39200395 , 0.4362
0.31618848 , 0.2482
RMSE:  0.9066824250109462  MAPE: 0.21983308293914694
5: ground truth total-  339  predicted total -  339
100: ground truth total-  2  predicted total -  1
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
0.05414407 , 0.148345
0.055382147 , 0.045721
0.06666812 , 0.066343
0.0 , 0.047982
0.08465466 , 0.152069
0.07610917 , 0.129675
0.0026129186 , 0.015652
0.0 , 0.007316
0.0 , 0.027159
0.0 , 0.031873
0.0043539405 , 0.00528
0.0 , 0.002866
0.0 , 0.015986
0.04734142 , 0.061996
0.0 , 0.038308
0.07277501 , 0.08693
0.05855736 , 0.11859
0.006417215 , 0.013198
0.069960475 , 0.064909
0.082987115 , 0.131686
0.05806084 , 0.044611
0.069315955 , 0.086326
0.040436476 , 0.023774
0.07758057 , 0.098146
0.051877186 , 0.050998
0.0104056895 , 0.01077
0.04292816 , 0.072149
0.0142222345 , 0.010294
0.08138238 , 0.130844
0.07751201 , 0.109889
0.054236606 , 0.034373
0.063549474 , 0.101086
0.012428373 , 0.007779
0.0 , 0.022433
0.0 , 0.011066
RMSE:  0.03244397487176688  MAPE: 0.5156879269044589
