['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1770
19
<class 'numpy.dtype'> float64
1770
train batches:  80  validate samples: 141  test samples: 354
Epoch [0/150], Batch loss: 2.789
Epoch: 0 RMSE:  0.9015851580237806  MAPE: 0.9811615124415118  L2+L1 loss: 0.615
Epoch [1/150], Batch loss: 0.454
Epoch: 1 RMSE:  0.8967771790988746  MAPE: 1.1991465483367538  L2+L1 loss: 0.647
Epoch [2/150], Batch loss: 0.464
Epoch: 2 RMSE:  0.8977173553347252  MAPE: 1.1264516741535602  L2+L1 loss: 0.636
Epoch [3/150], Batch loss: 0.459
Epoch: 3 RMSE:  0.8967394734659905  MAPE: 1.1963789490387913  L2+L1 loss: 0.647
Epoch [4/150], Batch loss: 0.458
Epoch: 4 RMSE:  0.8971025135332542  MAPE: 1.166243934483008  L2+L1 loss: 0.642
Epoch [5/150], Batch loss: 0.455
Epoch: 5 RMSE:  0.8968159055508417  MAPE: 1.189379543458601  L2+L1 loss: 0.646
Epoch [6/150], Batch loss: 0.475
Epoch: 6 RMSE:  0.8980286338762878  MAPE: 1.1094490504777503  L2+L1 loss: 0.633
Epoch [7/150], Batch loss: 1.672
Epoch: 7 RMSE:  6.434204865144481  MAPE: 10.641314664110615  L2+L1 loss: 1.962
Epoch [8/150], Batch loss: 0.914
Epoch: 8 RMSE:  0.9079647501247519  MAPE: 1.8392308974934544  L2+L1 loss: 0.744
Epoch [9/150], Batch loss: 0.47
Epoch: 9 RMSE:  0.8965401039935692  MAPE: 1.217229300944254  L2+L1 loss: 0.649
Epoch [10/150], Batch loss: 0.457
Epoch: 10 RMSE:  0.8966808840902727  MAPE: 1.2020665074715304  L2+L1 loss: 0.647
Epoch [11/150], Batch loss: 0.459
Epoch: 11 RMSE:  0.8970170669456062  MAPE: 1.1727172514843336  L2+L1 loss: 0.643
Epoch [12/150], Batch loss: 0.599
Epoch: 12 RMSE:  0.8977843792350043  MAPE: 1.4951886536517116  L2+L1 loss: 0.685
Epoch [13/150], Batch loss: 1.006
Epoch: 13 RMSE:  0.6029036023961023  MAPE: 1.1907604837527606  L2+L1 loss: 0.731
Epoch [14/150], Batch loss: 0.428
Epoch: 14 RMSE:  0.860458644199452  MAPE: 0.35833492873617456  L2+L1 loss: 0.437
Epoch [15/150], Batch loss: 0.386
Epoch: 15 RMSE:  0.8505759366450839  MAPE: 0.42016661642160585  L2+L1 loss: 0.442
Epoch [16/150], Batch loss: 0.379
Epoch: 16 RMSE:  0.8352474825908274  MAPE: 0.5982415121934072  L2+L1 loss: 0.477
Epoch [17/150], Batch loss: 0.385
Epoch: 17 RMSE:  0.8412471635471811  MAPE: 0.4771057276550188  L2+L1 loss: 0.451
Epoch [18/150], Batch loss: 0.381
Epoch: 18 RMSE:  0.7576406089466519  MAPE: 0.31631278668091967  L2+L1 loss: 0.39
Epoch [19/150], Batch loss: 0.397
Epoch: 19 RMSE:  0.5458454164858624  MAPE: 0.48917091733081747  L2+L1 loss: 0.451
Epoch [20/150], Batch loss: 0.447
Epoch: 20 RMSE:  0.8966249970197847  MAPE: 1.404001874041646  L2+L1 loss: 0.67
Epoch [21/150], Batch loss: 0.463
Epoch: 21 RMSE:  0.9048646477849633  MAPE: 0.8944956996582566  L2+L1 loss: 0.604
Epoch [22/150], Batch loss: 4.254
Epoch: 22 RMSE:  18.328200646693226  MAPE: 4.920703078151004  L2+L1 loss: 2.41
Epoch [23/150], Batch loss: 0.569
Epoch: 23 RMSE:  0.8961663701924069  MAPE: 1.3025544623029883  L2+L1 loss: 0.658
Epoch [24/150], Batch loss: 0.667
Epoch: 24 RMSE:  2.4034084247555603  MAPE: 6.221248540012047  L2+L1 loss: 1.258
Epoch [25/150], Batch loss: 0.638
Epoch: 25 RMSE:  0.8968196469448074  MAPE: 1.4095706470966438  L2+L1 loss: 0.671
Epoch [26/150], Batch loss: 0.455
Epoch: 26 RMSE:  0.8998128847103706  MAPE: 1.5949948378496503  L2+L1 loss: 0.702
Epoch [27/150], Batch loss: 3.107
Epoch: 27 RMSE:  1.697166178019306  MAPE: 7.02037614840394  L2+L1 loss: 1.737
Epoch [28/150], Batch loss: 1.079
Epoch: 28 RMSE:  0.9475247771279082  MAPE: 0.4878086648140244  L2+L1 loss: 0.584
Epoch [29/150], Batch loss: 0.723
Epoch: 29 RMSE:  0.8971803793221577  MAPE: 1.1605973712736266  L2+L1 loss: 0.642
Epoch [30/150], Batch loss: 0.457
Epoch: 30 RMSE:  0.8970049318794584  MAPE: 1.173662458304948  L2+L1 loss: 0.643
Epoch [31/150], Batch loss: 0.446
Epoch: 31 RMSE:  0.8969165579044681  MAPE: 1.1807626899750188  L2+L1 loss: 0.644
Epoch [32/150], Batch loss: 0.446
Epoch: 32 RMSE:  0.8968337353460903  MAPE: 1.1878069183424185  L2+L1 loss: 0.645
Epoch [33/150], Batch loss: 0.446
Epoch: 33 RMSE:  0.8970685998347486  MAPE: 1.1687763715976598  L2+L1 loss: 0.643
Epoch [34/150], Batch loss: 0.457
Epoch: 34 RMSE:  0.8971026918729116  MAPE: 1.1662307394602562  L2+L1 loss: 0.642
Epoch [35/150], Batch loss: 0.441
Epoch: 35 RMSE:  0.8964903263800533  MAPE: 1.223262448102646  L2+L1 loss: 0.65
Epoch [36/150], Batch loss: 0.456
Epoch: 36 RMSE:  0.8970492773670418  MAPE: 1.1702405154962587  L2+L1 loss: 0.643
Epoch [37/150], Batch loss: 0.456
Epoch: 37 RMSE:  0.8964601403660974  MAPE: 1.227149037017846  L2+L1 loss: 0.65
Epoch [38/150], Batch loss: 0.531
Epoch: 38 RMSE:  0.8963487431388036  MAPE: 1.3666885758362897  L2+L1 loss: 0.664
Epoch [39/150], Batch loss: 0.45
Epoch: 39 RMSE:  0.896435049812396  MAPE: 1.230534215068487  L2+L1 loss: 0.651
Epoch [40/150], Batch loss: 0.461
Epoch: 40 RMSE:  0.8974016687171265  MAPE: 1.145638143762589  L2+L1 loss: 0.639
Epoch [41/150], Batch loss: 0.455
Epoch: 41 RMSE:  0.8972355256739184  MAPE: 1.1567283085718527  L2+L1 loss: 0.641
Epoch [42/150], Batch loss: 0.453
Epoch: 42 RMSE:  0.8966828750661368  MAPE: 1.2018680785034348  L2+L1 loss: 0.647
Epoch [43/150], Batch loss: 0.456
Epoch: 43 RMSE:  0.8982525946571301  MAPE: 1.098184809890605  L2+L1 loss: 0.632
Epoch [44/150], Batch loss: 0.454
Epoch: 44 RMSE:  0.8974284131301276  MAPE: 1.1439242009599915  L2+L1 loss: 0.639
Epoch [45/150], Batch loss: 0.456
Epoch: 45 RMSE:  0.8966320665380383  MAPE: 1.207057751459373  L2+L1 loss: 0.648
Epoch [46/150], Batch loss: 0.454
Epoch: 46 RMSE:  1.4082879589204964  MAPE: 1.8237228207974578  L2+L1 loss: 0.635
Epoch [47/150], Batch loss: 0.432
Epoch: 47 RMSE:  0.8348174180619196  MAPE: 0.3200163543761891  L2+L1 loss: 0.382
Epoch [48/150], Batch loss: 0.491
Epoch: 48 RMSE:  0.9348318529430113  MAPE: 0.8821778096932041  L2+L1 loss: 0.491
Epoch [49/150], Batch loss: 0.341
Epoch: 49 RMSE:  0.8496542396253257  MAPE: 0.586454788808702  L2+L1 loss: 0.409
Epoch [50/150], Batch loss: 0.375
Epoch: 50 RMSE:  0.2804130938417556  MAPE: 0.3597955486979529  L2+L1 loss: 0.389
Epoch [51/150], Batch loss: 0.224
Epoch: 51 RMSE:  0.20166340935494048  MAPE: 0.2853016770259522  L2+L1 loss: 0.283
Epoch [52/150], Batch loss: 0.202
Epoch: 52 RMSE:  1.2510714593630243  MAPE: 0.33392196188079476  L2+L1 loss: 0.451
Epoch [53/150], Batch loss: 0.32
Epoch: 53 RMSE:  0.5585969881327933  MAPE: 0.36639105748012374  L2+L1 loss: 0.331
Epoch [54/150], Batch loss: 0.167
Epoch: 54 RMSE:  0.12921213540985893  MAPE: 0.25631967503663  L2+L1 loss: 0.26
Epoch [55/150], Batch loss: 0.145
Epoch: 55 RMSE:  0.21026435248340325  MAPE: 0.2828873950180493  L2+L1 loss: 0.274
Epoch [56/150], Batch loss: 0.129
Epoch: 56 RMSE:  0.11422030958394765  MAPE: 0.3043057750939676  L2+L1 loss: 0.271
Epoch [57/150], Batch loss: 0.179
Epoch: 57 RMSE:  0.14384463055180086  MAPE: 0.3745924468948857  L2+L1 loss: 0.308
Epoch [58/150], Batch loss: 0.139
Epoch: 58 RMSE:  0.17859564805023773  MAPE: 0.22878560729443692  L2+L1 loss: 0.26
Epoch [59/150], Batch loss: 0.152
Epoch: 59 RMSE:  0.2092504265694325  MAPE: 0.35963697616667134  L2+L1 loss: 0.338
Epoch [60/150], Batch loss: 0.136
Epoch: 60 RMSE:  0.15291906071332945  MAPE: 0.23887667998459775  L2+L1 loss: 0.26
Epoch [61/150], Batch loss: 0.117
Epoch: 61 RMSE:  0.13320640177780574  MAPE: 0.24290641030743648  L2+L1 loss: 0.26
Epoch [62/150], Batch loss: 0.117
Epoch: 62 RMSE:  0.14124927496195497  MAPE: 0.26962831259596287  L2+L1 loss: 0.269
Epoch [63/150], Batch loss: 0.115
Epoch: 63 RMSE:  0.17677558944574037  MAPE: 0.27627183791969895  L2+L1 loss: 0.274
Epoch [64/150], Batch loss: 0.117
Epoch: 64 RMSE:  0.12250790094848585  MAPE: 0.23239171405793352  L2+L1 loss: 0.258
Epoch [65/150], Batch loss: 0.116
Epoch: 65 RMSE:  0.12456788731694575  MAPE: 0.2468407457697083  L2+L1 loss: 0.261
Epoch [66/150], Batch loss: 0.116
Epoch: 66 RMSE:  0.11866870496161451  MAPE: 0.2744190410023629  L2+L1 loss: 0.269
Epoch [67/150], Batch loss: 0.121
Epoch: 67 RMSE:  0.12664396066288322  MAPE: 0.2462154595781494  L2+L1 loss: 0.26
Epoch [68/150], Batch loss: 0.114
Epoch: 68 RMSE:  0.12079956284487918  MAPE: 0.24700819550824193  L2+L1 loss: 0.26
Epoch [69/150], Batch loss: 0.113
Epoch: 69 RMSE:  0.12861456766051468  MAPE: 0.2294022459862292  L2+L1 loss: 0.261
Epoch [70/150], Batch loss: 0.105
Epoch: 70 RMSE:  0.1269041466461855  MAPE: 0.2057476428040751  L2+L1 loss: 0.257
Epoch [71/150], Batch loss: 0.102
Epoch: 71 RMSE:  0.12588603906567195  MAPE: 0.18927564139706196  L2+L1 loss: 0.252
Epoch [72/150], Batch loss: 0.102
Epoch: 72 RMSE:  0.12042207344296757  MAPE: 0.21687613266086472  L2+L1 loss: 0.26
Epoch [73/150], Batch loss: 0.105
Epoch: 73 RMSE:  0.1319693927991929  MAPE: 0.19944133313232576  L2+L1 loss: 0.254
Epoch [74/150], Batch loss: 0.103
Epoch: 74 RMSE:  0.10892699837604845  MAPE: 0.23382438243616857  L2+L1 loss: 0.262
Epoch [75/150], Batch loss: 0.1
Epoch: 75 RMSE:  0.13205499720780484  MAPE: 0.19762206866266666  L2+L1 loss: 0.257
Epoch [76/150], Batch loss: 0.102
Epoch: 76 RMSE:  0.12448816267579273  MAPE: 0.19289314656798826  L2+L1 loss: 0.253
Epoch [77/150], Batch loss: 0.103
Epoch: 77 RMSE:  0.11507191939634519  MAPE: 0.1999584810982806  L2+L1 loss: 0.253
Epoch [78/150], Batch loss: 0.103
Epoch: 78 RMSE:  0.1361625578193758  MAPE: 0.19870238069067953  L2+L1 loss: 0.256
Epoch [79/150], Batch loss: 0.1
Epoch: 79 RMSE:  0.1427407198456496  MAPE: 0.20694951688235727  L2+L1 loss: 0.257
Epoch [80/150], Batch loss: 0.099
Epoch: 80 RMSE:  0.10988381215917609  MAPE: 0.20526372878022903  L2+L1 loss: 0.258
Epoch [81/150], Batch loss: 0.102
Epoch: 81 RMSE:  0.13665208648814967  MAPE: 0.19377997608977335  L2+L1 loss: 0.255
Epoch [82/150], Batch loss: 0.105
Epoch: 82 RMSE:  0.11256767189554029  MAPE: 0.21174951325365612  L2+L1 loss: 0.254
Epoch [83/150], Batch loss: 0.104
Epoch: 83 RMSE:  0.19879193688863717  MAPE: 0.20799266411859677  L2+L1 loss: 0.263
Epoch [84/150], Batch loss: 0.103
Epoch: 84 RMSE:  0.16993988275689956  MAPE: 0.20450845852832455  L2+L1 loss: 0.26
Epoch [85/150], Batch loss: 0.101
Epoch: 85 RMSE:  0.14991790050070555  MAPE: 0.19458293082600128  L2+L1 loss: 0.257
Epoch [86/150], Batch loss: 0.099
Epoch: 86 RMSE:  0.1368536576757079  MAPE: 0.20093337348408272  L2+L1 loss: 0.255
Epoch [87/150], Batch loss: 0.101
Epoch: 87 RMSE:  0.16502565885826706  MAPE: 0.2104007360282753  L2+L1 loss: 0.261
Epoch [88/150], Batch loss: 0.102
Epoch: 88 RMSE:  0.14815978162937996  MAPE: 0.2040965564441791  L2+L1 loss: 0.257
Epoch [89/150], Batch loss: 0.101
Epoch: 89 RMSE:  0.12395250784346083  MAPE: 0.2126517067046198  L2+L1 loss: 0.257
Epoch [90/150], Batch loss: 0.096
Epoch: 90 RMSE:  0.12372327187368314  MAPE: 0.20159795874453534  L2+L1 loss: 0.254
Epoch [91/150], Batch loss: 0.097
Epoch: 91 RMSE:  0.1255423425890242  MAPE: 0.20103074862503154  L2+L1 loss: 0.254
Epoch [92/150], Batch loss: 0.097
Epoch: 92 RMSE:  0.12957590415531078  MAPE: 0.20179061875918397  L2+L1 loss: 0.254
Epoch [93/150], Batch loss: 0.096
Epoch: 93 RMSE:  0.1258076859678183  MAPE: 0.20275172035285932  L2+L1 loss: 0.255
Epoch [94/150], Batch loss: 0.097
Epoch: 94 RMSE:  0.12653083084314024  MAPE: 0.19874841911188762  L2+L1 loss: 0.254
Epoch [95/150], Batch loss: 0.097
Epoch: 95 RMSE:  0.12725148074994636  MAPE: 0.20260955368813385  L2+L1 loss: 0.255
Epoch [96/150], Batch loss: 0.097
Epoch: 96 RMSE:  0.1256800441053745  MAPE: 0.19671846930419612  L2+L1 loss: 0.253
Epoch [97/150], Batch loss: 0.097
Epoch: 97 RMSE:  0.12613207529680806  MAPE: 0.1974847608233442  L2+L1 loss: 0.253
Epoch [98/150], Batch loss: 0.097
Epoch: 98 RMSE:  0.1266434501398217  MAPE: 0.1968266942512493  L2+L1 loss: 0.253
Epoch [99/150], Batch loss: 0.097
Epoch: 99 RMSE:  0.12911388318921604  MAPE: 0.2014343573158395  L2+L1 loss: 0.254
Epoch [100/150], Batch loss: 0.096
Epoch: 100 RMSE:  0.12945995512109998  MAPE: 0.19885638178863352  L2+L1 loss: 0.254
Epoch [101/150], Batch loss: 0.097
Epoch: 101 RMSE:  0.12746583621024654  MAPE: 0.19518956164164464  L2+L1 loss: 0.253
Epoch [102/150], Batch loss: 0.097
Epoch: 102 RMSE:  0.12298380281907918  MAPE: 0.20360989769315407  L2+L1 loss: 0.255
Epoch [103/150], Batch loss: 0.097
Epoch: 103 RMSE:  0.12258424681671978  MAPE: 0.20590126967998434  L2+L1 loss: 0.255
Epoch [104/150], Batch loss: 0.097
Epoch: 104 RMSE:  0.12752558170652314  MAPE: 0.19471315528893735  L2+L1 loss: 0.252
Epoch [105/150], Batch loss: 0.097
Epoch: 105 RMSE:  0.1243588921162602  MAPE: 0.20473874620689167  L2+L1 loss: 0.255
Epoch [106/150], Batch loss: 0.098
Epoch: 106 RMSE:  0.12566379874432748  MAPE: 0.19796063233048686  L2+L1 loss: 0.253
Epoch [107/150], Batch loss: 0.097
Epoch: 107 RMSE:  0.1275037537069311  MAPE: 0.2012804501854911  L2+L1 loss: 0.254
Epoch [108/150], Batch loss: 0.097
Epoch: 108 RMSE:  0.13015582473269455  MAPE: 0.19761305830844506  L2+L1 loss: 0.253
Epoch [109/150], Batch loss: 0.097
Epoch: 109 RMSE:  0.1293212670897699  MAPE: 0.19972993860607793  L2+L1 loss: 0.254
Epoch [110/150], Batch loss: 0.097
Epoch: 110 RMSE:  0.12817669638316678  MAPE: 0.19903034638846842  L2+L1 loss: 0.253
Epoch [111/150], Batch loss: 0.098
Epoch: 111 RMSE:  0.12352620020355813  MAPE: 0.20115806497588865  L2+L1 loss: 0.254
Epoch [112/150], Batch loss: 0.097
Epoch: 112 RMSE:  0.12729828387487546  MAPE: 0.1963687115437618  L2+L1 loss: 0.253
Epoch [113/150], Batch loss: 0.097
Epoch: 113 RMSE:  0.12651780699608814  MAPE: 0.2001049055568489  L2+L1 loss: 0.254
Epoch [114/150], Batch loss: 0.098
Epoch: 114 RMSE:  0.12967664861482775  MAPE: 0.20087160938610688  L2+L1 loss: 0.254
Epoch [115/150], Batch loss: 0.097
Epoch: 115 RMSE:  0.1256091670410576  MAPE: 0.20134245722854077  L2+L1 loss: 0.254
Epoch [116/150], Batch loss: 0.097
Epoch: 116 RMSE:  0.12536274272645961  MAPE: 0.19950338683972468  L2+L1 loss: 0.253
Epoch [117/150], Batch loss: 0.098
Epoch: 117 RMSE:  0.1224332977603679  MAPE: 0.1981690505622696  L2+L1 loss: 0.253
Epoch [118/150], Batch loss: 0.097
Epoch: 118 RMSE:  0.12591889209962437  MAPE: 0.2032233162777954  L2+L1 loss: 0.254
Epoch [119/150], Batch loss: 0.097
Epoch: 119 RMSE:  0.12606048658392566  MAPE: 0.20171767962592446  L2+L1 loss: 0.253
Epoch [120/150], Batch loss: 0.096
Epoch: 120 RMSE:  0.12509054616404422  MAPE: 0.20136302506965006  L2+L1 loss: 0.253
Epoch [121/150], Batch loss: 0.096
Epoch: 121 RMSE:  0.12473271845095106  MAPE: 0.2010334448686819  L2+L1 loss: 0.253
Epoch [122/150], Batch loss: 0.096
Epoch: 122 RMSE:  0.1244405891664755  MAPE: 0.2013430494072464  L2+L1 loss: 0.253
Epoch [123/150], Batch loss: 0.096
Epoch: 123 RMSE:  0.12399590683395569  MAPE: 0.20121686947167847  L2+L1 loss: 0.253
Epoch [124/150], Batch loss: 0.096
Epoch: 124 RMSE:  0.124084131988033  MAPE: 0.20072986682740673  L2+L1 loss: 0.253
Epoch [125/150], Batch loss: 0.096
Epoch: 125 RMSE:  0.12398435819899638  MAPE: 0.20067214152644186  L2+L1 loss: 0.253
Epoch [126/150], Batch loss: 0.096
Epoch: 126 RMSE:  0.1237498576453414  MAPE: 0.2004411615233987  L2+L1 loss: 0.253
Epoch [127/150], Batch loss: 0.096
Epoch: 127 RMSE:  0.12378303446323248  MAPE: 0.20070326063107258  L2+L1 loss: 0.253
Epoch [128/150], Batch loss: 0.096
Epoch: 128 RMSE:  0.12380695703578877  MAPE: 0.2006058539073195  L2+L1 loss: 0.253
Epoch [129/150], Batch loss: 0.095
Epoch: 129 RMSE:  0.12375559412349327  MAPE: 0.20060071230837614  L2+L1 loss: 0.253
Epoch [130/150], Batch loss: 0.095
Epoch: 130 RMSE:  0.12369325359480694  MAPE: 0.20046261380863983  L2+L1 loss: 0.253
Epoch [131/150], Batch loss: 0.096
Epoch: 131 RMSE:  0.12391074289305888  MAPE: 0.20034829906914858  L2+L1 loss: 0.253
Epoch [132/150], Batch loss: 0.096
Epoch: 132 RMSE:  0.12378314489052576  MAPE: 0.20014345052754215  L2+L1 loss: 0.253
Epoch [133/150], Batch loss: 0.096
Epoch: 133 RMSE:  0.12378252435560781  MAPE: 0.20020944936207327  L2+L1 loss: 0.253
Epoch [134/150], Batch loss: 0.096
Epoch: 134 RMSE:  0.12371178610289436  MAPE: 0.20042059126161094  L2+L1 loss: 0.253
Epoch [135/150], Batch loss: 0.096
Epoch: 135 RMSE:  0.12395433783779186  MAPE: 0.2005820468795496  L2+L1 loss: 0.253
Epoch [136/150], Batch loss: 0.096
Epoch: 136 RMSE:  0.12397814943321057  MAPE: 0.20042620143563816  L2+L1 loss: 0.253
Epoch [137/150], Batch loss: 0.096
Epoch: 137 RMSE:  0.12415434832781537  MAPE: 0.20034945206476898  L2+L1 loss: 0.253
Epoch [138/150], Batch loss: 0.096
Epoch: 138 RMSE:  0.12437041522316374  MAPE: 0.20056894422621427  L2+L1 loss: 0.253
Epoch [139/150], Batch loss: 0.095
Epoch: 139 RMSE:  0.12402894269168782  MAPE: 0.2003972811385587  L2+L1 loss: 0.253
Epoch [140/150], Batch loss: 0.096
Epoch: 140 RMSE:  0.12383279410909723  MAPE: 0.2005303907616344  L2+L1 loss: 0.253
Epoch [141/150], Batch loss: 0.096
Epoch: 141 RMSE:  0.12424682170361301  MAPE: 0.20052158656168786  L2+L1 loss: 0.253
Epoch [142/150], Batch loss: 0.096
Epoch: 142 RMSE:  0.12424621128196904  MAPE: 0.20039223912111445  L2+L1 loss: 0.253
Epoch [143/150], Batch loss: 0.096
Epoch: 143 RMSE:  0.12432639297750475  MAPE: 0.20050903626064195  L2+L1 loss: 0.253
Epoch [144/150], Batch loss: 0.096
Epoch: 144 RMSE:  0.12410763635132628  MAPE: 0.20040950608995817  L2+L1 loss: 0.253
Epoch [145/150], Batch loss: 0.097
Epoch: 145 RMSE:  0.12411622634406078  MAPE: 0.20032297481065262  L2+L1 loss: 0.253
Epoch [146/150], Batch loss: 0.096
Epoch: 146 RMSE:  0.12412915361427725  MAPE: 0.20040210805180064  L2+L1 loss: 0.253
Epoch [147/150], Batch loss: 0.096
Epoch: 147 RMSE:  0.12423273757818407  MAPE: 0.2003492252708614  L2+L1 loss: 0.253
Epoch [148/150], Batch loss: 0.096
Epoch: 148 RMSE:  0.12427555266840541  MAPE: 0.2004942678944786  L2+L1 loss: 0.253
Epoch [149/150], Batch loss: 0.096
Epoch: 149 RMSE:  0.1244424993651023  MAPE: 0.20045616014333412  L2+L1 loss: 0.253


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
1.2548873 , 1.4023
0.36377478 , 0.2926
0.30258453 , 0.2629
0.14740977 , 0.1391
0.6840683 , 0.7334
0.65701425 , 0.8275
0.5255667 , 0.455
1.3505994 , 1.5416
0.41766882 , 0.4274
0.3090145 , 0.3233
0.95606506 , 1.2431
0.59077096 , 0.5565
1.0765315 , 1.3382
0.17317748 , 0.1683
0.4588375 , 0.35
0.2589819 , 0.17
0.65510917 , 0.7153
0.15824044 , 0.1212
0.35066143 , 0.4431
0.4166052 , 0.267
1.0449812 , 0.8481
0.5552215 , 0.6546
0.9288614 , 0.7755
0.45943147 , 0.362
0.3403585 , 0.5324
0.14878929 , 0.1714
0.21955901 , 0.2322
0.5352236 , 0.4826
0.43170112 , 0.4129
0.15520811 , 0.154
0.30219644 , 0.5402
0.2037046 , 0.1694
1.0501752 , 0.9044
0.41364476 , 0.4028
0.20533875 , 0.1522
0.9558612 , 1.2191
0.33930996 , 0.2376
0.7549147 , 0.5985
0.5223771 , 0.5757
0.17606643 , 0.1279
0.28851694 , 0.3491
0.23359376 , 0.2828
0.36132544 , 0.6231
0.14645684 , 0.1197
9.095782 , 9.6637
0.6392959 , 0.5544
0.5342251 , 0.6483
0.12741268 , 0.1029
0.13949409 , 0.1785
0.5900681 , 0.8153
0.85773754 , 0.6277
0.40168726 , 0.404
0.50049555 , 0.4534
0.19792426 , 0.2738
0.14255235 , 0.1388
1.0305774 , 0.9549
0.1452604 , 0.1495
0.17985764 , 0.1396
0.06410527 , 0.098774
0.56837445 , 0.8175
0.41988236 , 0.4579
0.343346 , 0.406
0.13087791 , 0.1341
0.11575842 , 0.1095
0.14981619 , 0.2138
0.31952164 , 0.4648
0.3215771 , 0.3822
0.1451295 , 0.1011
0.15768331 , 0.1227
0.8241985 , 0.8553
0.21575093 , 0.25
0.1305537 , 0.1504
0.5169496 , 0.4081
1.2385995 , 1.6483
0.32430634 , 0.2798
0.85296446 , 0.8666
0.6374362 , 0.5927
0.40832478 , 0.3596
0.14265639 , 0.102
0.4947366 , 0.7311
0.30795553 , 0.3897
0.9024135 , 0.806
0.19110045 , 0.1612
0.15285292 , 0.1384
0.14487126 , 0.1296
0.4827016 , 0.4906
0.41738826 , 0.6594
0.15031439 , 0.1094
0.14367592 , 0.1074
0.31482255 , 0.206
0.150675 , 0.1925
0.49534822 , 0.5867
0.3372264 , 0.4463
1.2068771 , 1.0161
0.5791757 , 0.5864
0.13766876 , 0.1071
0.24916384 , 0.1825
7.8534446 , 7.7907
0.37793568 , 0.2972
0.15318736 , 0.1531
0.20776892 , 0.1384
0.6780249 , 0.6574
0.44640046 , 0.5031
0.2915742 , 0.1945
0.14560825 , 0.149
0.1736941 , 0.1339
0.2566842 , 0.2413
0.16136488 , 0.1434
0.14112794 , 0.1178
0.13613072 , 0.1452
0.36966723 , 0.491
0.4195035 , 0.3634
1.1478887 , 1.0964
0.31376317 , 0.411
0.36718744 , 0.5141
0.21515998 , 0.1544
0.14739317 , 0.1404
0.71034956 , 0.5655
1.0162095 , 0.7641
0.1431615 , 0.1817
0.5551498 , 0.5745
0.15738308 , 0.1051
0.14178345 , 0.1095
0.91673326 , 0.7623
0.31775513 , 0.2616
0.48445046 , 0.4657
0.4683438 , 0.37
0.15463352 , 0.1074
0.8418691 , 0.7991
0.6099881 , 0.5706
0.14937589 , 0.1759
0.16334304 , 0.1366
0.15907425 , 0.1182
0.3601622 , 0.2474
0.13779342 , 0.136
0.14287964 , 0.1194
0.15357873 , 0.109
0.22084779 , 0.2939
0.13270205 , 0.1531
0.14356741 , 0.1108
0.36572802 , 0.431
0.17739853 , 0.2226
0.18384778 , 0.19
0.54603124 , 0.5198
0.28754333 , 0.2652
0.15813184 , 0.1098
0.31947994 , 0.274
0.2563428 , 0.1939
0.29575592 , 0.2329
0.42707917 , 0.3635
1.394939 , 1.297
0.63853043 , 0.4357
0.13192424 , 0.1717
0.9096978 , 0.8599
0.5872533 , 0.6343
0.062754214 , 0.088253
0.1464726 , 0.1069
0.17910862 , 0.132
0.46340162 , 0.4453
0.3142434 , 0.3072
0.5348914 , 0.5749
0.14440876 , 0.1572
0.19190556 , 0.2404
0.5560079 , 0.4738
0.14926592 , 0.134
0.75761133 , 0.7237
0.43410924 , 0.6464
0.14339337 , 0.1009
0.86796594 , 0.8425
0.1362943 , 0.1021
0.14092296 , 0.139
0.565087 , 0.5091
1.3328484 , 1.1011
0.43799853 , 0.4395
1.0797075 , 0.8998
0.17822653 , 0.1374
0.14685664 , 0.1054
0.16249791 , 0.196
0.43276918 , 0.4058
0.38059813 , 0.3702
0.14814344 , 0.1037
0.14340892 , 0.1615
0.13341618 , 0.1453
0.15203205 , 0.1913
0.3304919 , 0.5548
0.2514572 , 0.2128
0.34024116 , 0.2389
1.279542 , 1.4281
1.0295298 , 0.8596
0.1481272 , 0.1483
0.36464453 , 0.3341
0.16211477 , 0.1692
0.26346543 , 0.2578
0.78724277 , 0.8096
0.15364867 , 0.1094
0.56944233 , 0.6809
0.3365433 , 0.3665
0.13404289 , 0.1168
0.28795692 , 0.1783
0.30750245 , 0.2953
8.54208 , 8.9574
0.17402917 , 0.1219
0.4799716 , 0.4015
0.25223443 , 0.2526
0.9216631 , 0.8094
1.1882873 , 1.4087
0.13486841 , 0.1038
0.359135 , 0.3717
0.15447652 , 0.1112
0.20539948 , 0.1591
0.22237843 , 0.177
0.1428375 , 0.1359
0.58702755 , 0.8092
0.55970764 , 0.5126
0.60528296 , 0.5724
0.14903733 , 0.1304
0.62211573 , 0.9421
0.14168581 , 0.1151
0.6178089 , 0.6932
0.98602825 , 0.9325
0.34312674 , 0.5359
0.13026914 , 0.1117
8.033104 , 7.843
0.3735717 , 0.3666
0.28875217 , 0.2081
0.15720248 , 0.13
0.15169358 , 0.1481
0.13450986 , 0.1128
0.3294014 , 0.3508
0.14488542 , 0.1751
0.45955443 , 0.6246
0.44660187 , 0.4268
0.13725549 , 0.2485
0.13630697 , 0.18
0.2718554 , 0.208
0.3876567 , 0.3174
0.14758953 , 0.1784
0.45214504 , 0.4815
0.13746616 , 0.1174
0.55971855 , 0.7315
0.21444455 , 0.1576
0.15325153 , 0.1599
0.21758285 , 0.2007
0.19490841 , 0.1554
0.15660805 , 0.1196
0.4656968 , 0.7004
0.4966145 , 0.9233
0.16143203 , 0.1189
0.16030657 , 0.1265
0.5599073 , 0.5657
1.0389794 , 0.8783
0.7282828 , 0.8725
0.056937575 , 0.044424
0.57839334 , 0.5519
0.31400758 , 0.2601
0.15343171 , 0.1151
0.27117774 , 0.2681
0.21447185 , 0.1483
0.13943365 , 0.1192
0.16599128 , 0.2345
0.7152098 , 0.6212
0.5808876 , 0.5374
0.7919953 , 0.6353
0.4698969 , 0.6399
0.16045454 , 0.3125
0.37776363 , 0.545
0.53956157 , 0.5227
8.163475 , 7.8099
0.54724497 , 0.4944
0.62757146 , 0.6774
0.9147328 , 0.732
0.14100239 , 0.1048
0.7538747 , 0.8268
0.39135492 , 0.3138
0.1338228 , 0.1398
0.14807355 , 0.1182
0.5960799 , 0.683
0.6267592 , 0.5521
0.40466958 , 0.3158
0.45791885 , 0.4246
0.9106262 , 0.8639
0.242105 , 0.2787
0.5333634 , 0.4446
0.54949725 , 0.4563
8.44595 , 8.4855
0.51394343 , 0.4116
0.1859878 , 0.123
0.447387 , 0.4213
0.36753047 , 0.3672
0.5278878 , 0.6519
1.0909307 , 1.1042
0.15558103 , 0.1038
0.31529883 , 0.3049
8.550329 , 8.3752
0.39129043 , 0.3264
0.45222706 , 0.6895
0.43348962 , 0.5295
0.35451767 , 0.3442
0.39591554 , 0.3744
1.3038872 , 1.4393
0.17378712 , 0.1597
0.61965793 , 0.8299
0.15601656 , 0.1983
0.4287699 , 0.4723
0.079452336 , 0.1103
1.4105561 , 1.5342
0.75714105 , 0.8281
0.40007418 , 0.4493
0.33030754 , 0.2979
0.5229448 , 0.3554
0.24517038 , 0.2359
1.201052 , 0.9548
0.48499152 , 0.4657
0.3706122 , 0.4057
0.43882513 , 0.3912
0.1969325 , 0.2187
0.13727301 , 0.124
0.14235479 , 0.1931
0.4239514 , 0.339
0.5028105 , 0.6458
0.23814136 , 0.2259
0.1725555 , 0.3798
0.15700737 , 0.1748
0.14120534 , 0.1053
0.13645336 , 0.1133
0.16806498 , 0.1894
0.45703107 , 0.4857
0.010186791 , 0.003465
1.1628401 , 1.0708
0.01633054 , 0.004524
1.4219029 , 1.9222
0.70596886 , 0.9304
0.30164596 , 0.2436
1.1990441 , 1.0126
0.042872906 , 0.04715
0.68272245 , 0.5758
0.35708687 , 0.2282
0.42094624 , 0.7287
8.708927 , 9.0239
0.48771036 , 0.3612
0.15544075 , 0.1966
0.3943052 , 0.379
0.9189446 , 0.7465
0.19861671 , 0.1923
0.44138277 , 0.5945
0.3120256 , 0.3192
0.4201641 , 0.6293
0.9066429 , 0.7434
1.2006872 , 1.1784
0.15718979 , 0.1075
0.18867567 , 0.1585
0.34380332 , 0.4908
1.3024851 , 1.2197
0.28870833 , 0.2171
RMSE:  0.11478715186263959  MAPE: 0.20497077068813646
5: ground truth total-  346  predicted total -  346
100: ground truth total-  8  predicted total -  8
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
0.06320888 , 0.152863
0.04089284 , 0.0125
0.051503062 , 0.033572
0.052428126 , 0.13701
0.050824463 , 0.086663
0.0546515 , 0.099688
0.045544147 , 0.016675
0.05349517 , 0.033479
0.061440647 , 0.131455
0.0567894 , 0.088819
0.0488382 , 0.045182
0.022874057 , 0.016943
0.05000013 , 0.066612
0.05795127 , 0.131173
0.053489447 , 0.065222
0.055304468 , 0.039811
0.061222613 , 0.078834
0.051986456 , 0.117401
0.022355676 , 0.01404
0.0209921 , 0.010996
0.057362795 , 0.055779
0.054803073 , 0.046051
0.05934906 , 0.113639
0.046851814 , 0.023433
0.05146396 , 0.099852
0.04884404 , 0.024
0.021742582 , 0.013558
0.05140376 , 0.027936
0.059454262 , 0.066418
0.05547583 , 0.131273
RMSE:  0.04105330570844398  MAPE: 0.5842655004410855
