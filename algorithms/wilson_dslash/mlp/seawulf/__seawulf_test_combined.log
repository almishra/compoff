['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
2748
19
<class 'numpy.dtype'> float64
2748
train batches:  124  validate samples: 219  test samples: 549
Epoch [0/150], Batch loss: 30.067
Epoch: 0 RMSE:  24.323056469957237  MAPE: 2.953112122688761  L2+L1 loss: 23.365
Epoch [1/150], Batch loss: 22.862
Epoch: 1 RMSE:  18.18714413696137  MAPE: 2.148219882409501  L2+L1 loss: 13.131
Epoch [2/150], Batch loss: 22.387
Epoch: 2 RMSE:  18.051843740038745  MAPE: 1.9474418511920097  L2+L1 loss: 12.701
Epoch [3/150], Batch loss: 22.804
Epoch: 3 RMSE:  17.458687881467828  MAPE: 0.5079929336495476  L2+L1 loss: 10.164
Epoch [4/150], Batch loss: 23.819
Epoch: 4 RMSE:  15.25739444147107  MAPE: 0.5277495924689796  L2+L1 loss: 7.66
Epoch [5/150], Batch loss: 18.728
Epoch: 5 RMSE:  13.945232104126305  MAPE: 0.6804767787166605  L2+L1 loss: 7.294
Epoch [6/150], Batch loss: 17.653
Epoch: 6 RMSE:  13.609076166033562  MAPE: 0.7991173802891849  L2+L1 loss: 7.705
Epoch [7/150], Batch loss: 23.538
Epoch: 7 RMSE:  12.70236114028406  MAPE: 1.4738572579996936  L2+L1 loss: 8.984
Epoch [8/150], Batch loss: 21.623
Epoch: 8 RMSE:  18.102725422715785  MAPE: 1.7237660726035144  L2+L1 loss: 12.367
Epoch [9/150], Batch loss: 22.599
Epoch: 9 RMSE:  18.045511575700104  MAPE: 1.9154324511084047  L2+L1 loss: 12.643
Epoch [10/150], Batch loss: 22.195
Epoch: 10 RMSE:  18.082449330484145  MAPE: 2.0203828917778273  L2+L1 loss: 12.839
Epoch [11/150], Batch loss: 22.805
Epoch: 11 RMSE:  18.067677947274326  MAPE: 1.9915142006013706  L2+L1 loss: 12.783
Epoch [12/150], Batch loss: 22.671
Epoch: 12 RMSE:  18.07776346973271  MAPE: 2.0119337904284254  L2+L1 loss: 12.823
Epoch [13/150], Batch loss: 25.815
Epoch: 13 RMSE:  18.06719326122658  MAPE: 1.9904339561843059  L2+L1 loss: 12.781
Epoch [14/150], Batch loss: 50.457
Epoch: 14 RMSE:  18.095751322018316  MAPE: 2.041985081432373  L2+L1 loss: 12.881
Epoch [15/150], Batch loss: 22.198
Epoch: 15 RMSE:  18.09582748287932  MAPE: 2.0421004300831767  L2+L1 loss: 12.881
Epoch [16/150], Batch loss: 22.827
Epoch: 16 RMSE:  18.111505342671826  MAPE: 2.064384151153761  L2+L1 loss: 12.927
Epoch [17/150], Batch loss: 26.315
Epoch: 17 RMSE:  18.058132213692375  MAPE: 1.804407901437178  L2+L1 loss: 12.462
Epoch [18/150], Batch loss: 22.347
Epoch: 18 RMSE:  17.979794579188827  MAPE: 1.9380666644643454  L2+L1 loss: 12.665
Epoch [19/150], Batch loss: 22.53
Epoch: 19 RMSE:  18.051903019645454  MAPE: 1.9476638559223376  L2+L1 loss: 12.701
Epoch [20/150], Batch loss: 22.655
Epoch: 20 RMSE:  18.05921471935061  MAPE: 1.970731490404031  L2+L1 loss: 12.741
Epoch [21/150], Batch loss: 37.1
Epoch: 21 RMSE:  18.044433547199244  MAPE: 1.9054080170266832  L2+L1 loss: 12.624
Epoch [22/150], Batch loss: 22.184
Epoch: 22 RMSE:  18.055564742052777  MAPE: 1.9601101991694505  L2+L1 loss: 12.723
Epoch [23/150], Batch loss: 22.81
Epoch: 23 RMSE:  18.06744724117225  MAPE: 1.9910013793536414  L2+L1 loss: 12.782
Epoch [24/150], Batch loss: 22.713
Epoch: 24 RMSE:  18.075743515930043  MAPE: 2.0081140584005155  L2+L1 loss: 12.815
Epoch [25/150], Batch loss: 22.669
Epoch: 25 RMSE:  18.098959657265993  MAPE: 2.046789105998556  L2+L1 loss: 12.891
Epoch [26/150], Batch loss: 38.766
Epoch: 26 RMSE:  18.10052642944066  MAPE: 2.0490866215857673  L2+L1 loss: 12.896
Epoch [27/150], Batch loss: 22.551
Epoch: 27 RMSE:  18.092918323387153  MAPE: 2.03763319174568  L2+L1 loss: 12.872
Epoch [28/150], Batch loss: 23.409
Epoch: 28 RMSE:  17.213478152888893  MAPE: 1.5124504569738808  L2+L1 loss: 11.604
Epoch [29/150], Batch loss: 21.822
Epoch: 29 RMSE:  18.483716809326268  MAPE: 2.3558514150808563  L2+L1 loss: 13.716
Epoch [30/150], Batch loss: 23.394
Epoch: 30 RMSE:  18.419694958230075  MAPE: 2.3186589287618604  L2+L1 loss: 13.603
Epoch [31/150], Batch loss: 22.931
Epoch: 31 RMSE:  18.35434059410728  MAPE: 2.277656279534326  L2+L1 loss: 13.486
Epoch [32/150], Batch loss: 22.848
Epoch: 32 RMSE:  18.304745554898815  MAPE: 2.2437105638567436  L2+L1 loss: 13.389
Epoch [33/150], Batch loss: 23.399
Epoch: 33 RMSE:  18.26371120284033  MAPE: 2.2132423663377234  L2+L1 loss: 13.303
Epoch [34/150], Batch loss: 22.965
Epoch: 34 RMSE:  18.226147111889876  MAPE: 2.182978324184581  L2+L1 loss: 13.223
Epoch [35/150], Batch loss: 22.803
Epoch: 35 RMSE:  18.19864300065858  MAPE: 2.1588875981257054  L2+L1 loss: 13.159
Epoch [36/150], Batch loss: 22.32
Epoch: 36 RMSE:  15.443487695661045  MAPE: 0.4832908577670833  L2+L1 loss: 8.062
Epoch [37/150], Batch loss: 19.171
Epoch: 37 RMSE:  14.960110111469312  MAPE: 0.38030486888142595  L2+L1 loss: 6.906
Epoch [38/150], Batch loss: 18.254
Epoch: 38 RMSE:  14.66843757257194  MAPE: 0.33514667111846524  L2+L1 loss: 6.509
Epoch [39/150], Batch loss: 17.894
Epoch: 39 RMSE:  14.392708565393551  MAPE: 0.4235139222617866  L2+L1 loss: 6.276
Epoch [40/150], Batch loss: 17.797
Epoch: 40 RMSE:  14.04939281220726  MAPE: 0.1823441432314279  L2+L1 loss: 5.331
Epoch [41/150], Batch loss: 12.433
Epoch: 41 RMSE:  3.2902512914442146  MAPE: 0.11671073691516268  L2+L1 loss: 2.317
Epoch [42/150], Batch loss: 5.158
Epoch: 42 RMSE:  3.0349835633065623  MAPE: 0.15565556017396054  L2+L1 loss: 2.157
Epoch [43/150], Batch loss: 3.826
Epoch: 43 RMSE:  5.288748905580125  MAPE: 0.14395565286063092  L2+L1 loss: 2.813
Epoch [44/150], Batch loss: 3.7
Epoch: 44 RMSE:  2.2489528437672455  MAPE: 0.11551196492434929  L2+L1 loss: 1.795
Epoch [45/150], Batch loss: 3.787
Epoch: 45 RMSE:  2.789461398429042  MAPE: 0.11547752368667456  L2+L1 loss: 1.972
Epoch [46/150], Batch loss: 2.938
Epoch: 46 RMSE:  2.998248370572083  MAPE: 0.150446342871199  L2+L1 loss: 2.217
Epoch [47/150], Batch loss: 3.634
Epoch: 47 RMSE:  6.975356613160473  MAPE: 0.26790419745088456  L2+L1 loss: 2.981
Epoch [48/150], Batch loss: 3.166
Epoch: 48 RMSE:  2.5014810435780706  MAPE: 0.13261268930195133  L2+L1 loss: 1.829
Epoch [49/150], Batch loss: 3.219
Epoch: 49 RMSE:  2.7384374833785103  MAPE: 0.08565825693064888  L2+L1 loss: 1.71
Epoch [50/150], Batch loss: 2.859
Epoch: 50 RMSE:  1.9529328458443824  MAPE: 0.0824351542927613  L2+L1 loss: 1.616
Epoch [51/150], Batch loss: 2.375
Epoch: 51 RMSE:  2.9952934052150124  MAPE: 0.11584341844968675  L2+L1 loss: 1.801
Epoch [52/150], Batch loss: 4.953
Epoch: 52 RMSE:  4.296914573899049  MAPE: 0.09545528947706257  L2+L1 loss: 2.106
Epoch [53/150], Batch loss: 3.422
Epoch: 53 RMSE:  3.0887485748240917  MAPE: 0.160231855683758  L2+L1 loss: 2.061
Epoch [54/150], Batch loss: 2.415
Epoch: 54 RMSE:  2.176031845639845  MAPE: 0.0990930683347118  L2+L1 loss: 1.604
Epoch [55/150], Batch loss: 4.628
Epoch: 55 RMSE:  2.649011054585274  MAPE: 0.2079946336269698  L2+L1 loss: 2.031
Epoch [56/150], Batch loss: 2.182
Epoch: 56 RMSE:  2.818341990005478  MAPE: 0.10645748127414878  L2+L1 loss: 1.851
Epoch [57/150], Batch loss: 2.635
Epoch: 57 RMSE:  1.6694218323959367  MAPE: 0.11204990730605217  L2+L1 loss: 1.473
Epoch [58/150], Batch loss: 2.031
Epoch: 58 RMSE:  1.7687931736225404  MAPE: 0.10451963890724093  L2+L1 loss: 1.599
Epoch [59/150], Batch loss: 2.964
Epoch: 59 RMSE:  2.5835423449435915  MAPE: 0.08895149549539402  L2+L1 loss: 1.8
Epoch [60/150], Batch loss: 1.921
Epoch: 60 RMSE:  1.94363048630233  MAPE: 0.07438346902078473  L2+L1 loss: 1.593
Epoch [61/150], Batch loss: 1.651
Epoch: 61 RMSE:  1.8309467663131285  MAPE: 0.09530875194462783  L2+L1 loss: 1.568
Epoch [62/150], Batch loss: 1.612
Epoch: 62 RMSE:  1.8019848178134306  MAPE: 0.07011505597005817  L2+L1 loss: 1.532
Epoch [63/150], Batch loss: 1.613
Epoch: 63 RMSE:  1.7275891116092774  MAPE: 0.07203112975854749  L2+L1 loss: 1.512
Epoch [64/150], Batch loss: 1.564
Epoch: 64 RMSE:  1.7188127067000785  MAPE: 0.06876938589301104  L2+L1 loss: 1.489
Epoch [65/150], Batch loss: 1.594
Epoch: 65 RMSE:  1.658933095009114  MAPE: 0.07601811509117606  L2+L1 loss: 1.477
Epoch [66/150], Batch loss: 1.557
Epoch: 66 RMSE:  1.6719429529151308  MAPE: 0.06813188704684008  L2+L1 loss: 1.472
Epoch [67/150], Batch loss: 1.536
Epoch: 67 RMSE:  1.696177203029135  MAPE: 0.06715477876591788  L2+L1 loss: 1.509
Epoch [68/150], Batch loss: 1.503
Epoch: 68 RMSE:  1.6362067174718646  MAPE: 0.06664359159368452  L2+L1 loss: 1.49
Epoch [69/150], Batch loss: 1.546
Epoch: 69 RMSE:  1.6311005620629178  MAPE: 0.07511353389044216  L2+L1 loss: 1.472
Epoch [70/150], Batch loss: 1.505
Epoch: 70 RMSE:  1.666518820618652  MAPE: 0.06518280428515721  L2+L1 loss: 1.471
Epoch [71/150], Batch loss: 1.491
Epoch: 71 RMSE:  1.7079036111412238  MAPE: 0.0635553116663382  L2+L1 loss: 1.443
Epoch [72/150], Batch loss: 1.495
Epoch: 72 RMSE:  1.5952145169306686  MAPE: 0.07965971474935092  L2+L1 loss: 1.477
Epoch [73/150], Batch loss: 1.455
Epoch: 73 RMSE:  1.5797454341961266  MAPE: 0.06192197091754446  L2+L1 loss: 1.447
Epoch [74/150], Batch loss: 1.446
Epoch: 74 RMSE:  1.5882536694717648  MAPE: 0.05958257451922957  L2+L1 loss: 1.392
Epoch [75/150], Batch loss: 1.447
Epoch: 75 RMSE:  1.5761409091444314  MAPE: 0.07276894499178571  L2+L1 loss: 1.431
Epoch [76/150], Batch loss: 1.47
Epoch: 76 RMSE:  1.7833054145796368  MAPE: 0.06447608936123209  L2+L1 loss: 1.522
Epoch [77/150], Batch loss: 1.465
Epoch: 77 RMSE:  1.5965012124429176  MAPE: 0.07868850715433906  L2+L1 loss: 1.447
Epoch [78/150], Batch loss: 1.472
Epoch: 78 RMSE:  1.5487588381251227  MAPE: 0.07105969638977425  L2+L1 loss: 1.408
Epoch [79/150], Batch loss: 1.448
Epoch: 79 RMSE:  1.5501081204876912  MAPE: 0.06915217431366699  L2+L1 loss: 1.42
Epoch [80/150], Batch loss: 1.491
Epoch: 80 RMSE:  1.6182977321074155  MAPE: 0.06685961600405069  L2+L1 loss: 1.439
Epoch [81/150], Batch loss: 1.435
Epoch: 81 RMSE:  1.6181817704298915  MAPE: 0.08010833474728592  L2+L1 loss: 1.481
Epoch [82/150], Batch loss: 1.442
Epoch: 82 RMSE:  1.6648694722479207  MAPE: 0.0589543723111161  L2+L1 loss: 1.43
Epoch [83/150], Batch loss: 1.542
Epoch: 83 RMSE:  1.575706890983829  MAPE: 0.06501598958412344  L2+L1 loss: 1.429
Epoch [84/150], Batch loss: 1.446
Epoch: 84 RMSE:  1.5711538774013931  MAPE: 0.06400639863193508  L2+L1 loss: 1.36
Epoch [85/150], Batch loss: 1.417
Epoch: 85 RMSE:  1.5923894398429719  MAPE: 0.05784521307757701  L2+L1 loss: 1.419
Epoch [86/150], Batch loss: 1.397
Epoch: 86 RMSE:  1.543045743087759  MAPE: 0.059333794619909674  L2+L1 loss: 1.419
Epoch [87/150], Batch loss: 1.499
Epoch: 87 RMSE:  1.56928450098908  MAPE: 0.07986230437309501  L2+L1 loss: 1.445
Epoch [88/150], Batch loss: 1.42
Epoch: 88 RMSE:  1.5640756656884556  MAPE: 0.05731687400112022  L2+L1 loss: 1.394
Epoch [89/150], Batch loss: 1.447
Epoch: 89 RMSE:  1.557991131376968  MAPE: 0.06299988971046591  L2+L1 loss: 1.402
Epoch [90/150], Batch loss: 1.384
Epoch: 90 RMSE:  1.5403315725008135  MAPE: 0.058168411690184114  L2+L1 loss: 1.389
Epoch [91/150], Batch loss: 1.385
Epoch: 91 RMSE:  1.5494526656033718  MAPE: 0.05799171965860569  L2+L1 loss: 1.396
Epoch [92/150], Batch loss: 1.383
Epoch: 92 RMSE:  1.547299710333035  MAPE: 0.057481009179075525  L2+L1 loss: 1.393
Epoch [93/150], Batch loss: 1.374
Epoch: 93 RMSE:  1.5456079878056663  MAPE: 0.05789584861670184  L2+L1 loss: 1.394
Epoch [94/150], Batch loss: 1.384
Epoch: 94 RMSE:  1.5469761083904408  MAPE: 0.05792111821134242  L2+L1 loss: 1.398
Epoch [95/150], Batch loss: 1.376
Epoch: 95 RMSE:  1.5512592275779524  MAPE: 0.0596212025560581  L2+L1 loss: 1.405
Epoch [96/150], Batch loss: 1.374
Epoch: 96 RMSE:  1.54515556170215  MAPE: 0.05916530616152148  L2+L1 loss: 1.397
Epoch [97/150], Batch loss: 1.36
Epoch: 97 RMSE:  1.552288245436728  MAPE: 0.05753953448865752  L2+L1 loss: 1.397
Epoch [98/150], Batch loss: 1.368
Epoch: 98 RMSE:  1.541727542214186  MAPE: 0.05935418785033182  L2+L1 loss: 1.384
Epoch [99/150], Batch loss: 1.389
Epoch: 99 RMSE:  1.5413487465289657  MAPE: 0.05844997280417948  L2+L1 loss: 1.39
Epoch [100/150], Batch loss: 1.368
Epoch: 100 RMSE:  1.5415019082661752  MAPE: 0.05687888028821735  L2+L1 loss: 1.384
Epoch [101/150], Batch loss: 1.368
Epoch: 101 RMSE:  1.537894617490455  MAPE: 0.0589076044146334  L2+L1 loss: 1.39
Epoch [102/150], Batch loss: 1.36
Epoch: 102 RMSE:  1.5388162068483764  MAPE: 0.059100119540515104  L2+L1 loss: 1.394
Epoch [103/150], Batch loss: 1.37
Epoch: 103 RMSE:  1.5390174254372433  MAPE: 0.05748477184117891  L2+L1 loss: 1.387
Epoch [104/150], Batch loss: 1.362
Epoch: 104 RMSE:  1.5393799974033688  MAPE: 0.058934283166908345  L2+L1 loss: 1.392
Epoch [105/150], Batch loss: 1.372
Epoch: 105 RMSE:  1.5446810234614359  MAPE: 0.056620471871274725  L2+L1 loss: 1.388
Epoch [106/150], Batch loss: 1.378
Epoch: 106 RMSE:  1.541872075713626  MAPE: 0.057108800295886025  L2+L1 loss: 1.389
Epoch [107/150], Batch loss: 1.351
Epoch: 107 RMSE:  1.5523184920929545  MAPE: 0.05745472076147284  L2+L1 loss: 1.395
Epoch [108/150], Batch loss: 1.369
Epoch: 108 RMSE:  1.53694943719159  MAPE: 0.05743623353348185  L2+L1 loss: 1.384
Epoch [109/150], Batch loss: 1.362
Epoch: 109 RMSE:  1.536128192824243  MAPE: 0.0567084081228679  L2+L1 loss: 1.383
Epoch [110/150], Batch loss: 1.356
Epoch: 110 RMSE:  1.5360069965251257  MAPE: 0.05837290541016716  L2+L1 loss: 1.384
Epoch [111/150], Batch loss: 1.367
Epoch: 111 RMSE:  1.5399266765146153  MAPE: 0.057069227430364675  L2+L1 loss: 1.39
Epoch [112/150], Batch loss: 1.366
Epoch: 112 RMSE:  1.5349861990613443  MAPE: 0.057123909696974604  L2+L1 loss: 1.382
Epoch [113/150], Batch loss: 1.356
Epoch: 113 RMSE:  1.5330790355482615  MAPE: 0.06009664155751001  L2+L1 loss: 1.386
Epoch [114/150], Batch loss: 1.379
Epoch: 114 RMSE:  1.540477052692093  MAPE: 0.05636928407737965  L2+L1 loss: 1.379
Epoch [115/150], Batch loss: 1.382
Epoch: 115 RMSE:  1.547912277309858  MAPE: 0.05612055231170206  L2+L1 loss: 1.384
Epoch [116/150], Batch loss: 1.377
Epoch: 116 RMSE:  1.5328830819529689  MAPE: 0.05749974814399723  L2+L1 loss: 1.385
Epoch [117/150], Batch loss: 1.356
Epoch: 117 RMSE:  1.5322064871627548  MAPE: 0.05638061128927993  L2+L1 loss: 1.378
Epoch [118/150], Batch loss: 1.367
Epoch: 118 RMSE:  1.5374922561251276  MAPE: 0.0605346421230875  L2+L1 loss: 1.395
Epoch [119/150], Batch loss: 1.368
Epoch: 119 RMSE:  1.5460679011213532  MAPE: 0.05700871399664119  L2+L1 loss: 1.391
Epoch [120/150], Batch loss: 1.354
Epoch: 120 RMSE:  1.5411431758321188  MAPE: 0.056910976290895775  L2+L1 loss: 1.389
Epoch [121/150], Batch loss: 1.37
Epoch: 121 RMSE:  1.538710736766484  MAPE: 0.05667086585222531  L2+L1 loss: 1.386
Epoch [122/150], Batch loss: 1.348
Epoch: 122 RMSE:  1.5373085935376543  MAPE: 0.05638604881752122  L2+L1 loss: 1.383
Epoch [123/150], Batch loss: 1.36
Epoch: 123 RMSE:  1.5362783293692643  MAPE: 0.056373160079976045  L2+L1 loss: 1.382
Epoch [124/150], Batch loss: 1.353
Epoch: 124 RMSE:  1.5363735836114945  MAPE: 0.05672748525152218  L2+L1 loss: 1.384
Epoch [125/150], Batch loss: 1.345
Epoch: 125 RMSE:  1.536198794590832  MAPE: 0.056551762606364776  L2+L1 loss: 1.383
Epoch [126/150], Batch loss: 1.366
Epoch: 126 RMSE:  1.5361570279650265  MAPE: 0.056711677601561254  L2+L1 loss: 1.384
Epoch [127/150], Batch loss: 1.353
Epoch: 127 RMSE:  1.5357242568029623  MAPE: 0.05720434967357918  L2+L1 loss: 1.386
Epoch [128/150], Batch loss: 1.363
Epoch: 128 RMSE:  1.5359708134991714  MAPE: 0.05681031355968421  L2+L1 loss: 1.385
Epoch [129/150], Batch loss: 1.352
Epoch: 129 RMSE:  1.5357878206339246  MAPE: 0.056808883097392314  L2+L1 loss: 1.384
Epoch [130/150], Batch loss: 1.349
Epoch: 130 RMSE:  1.5358283385382066  MAPE: 0.05666764541858244  L2+L1 loss: 1.384
Epoch [131/150], Batch loss: 1.357
Epoch: 131 RMSE:  1.5358182434648653  MAPE: 0.05684377389658137  L2+L1 loss: 1.385
Epoch [132/150], Batch loss: 1.344
Epoch: 132 RMSE:  1.5353008523030796  MAPE: 0.05751640232439325  L2+L1 loss: 1.387
Epoch [133/150], Batch loss: 1.35
Epoch: 133 RMSE:  1.5352781005646534  MAPE: 0.056602115009114766  L2+L1 loss: 1.383
Epoch [134/150], Batch loss: 1.351
Epoch: 134 RMSE:  1.5353171020967  MAPE: 0.05718131099013878  L2+L1 loss: 1.386
Epoch [135/150], Batch loss: 1.36
Epoch: 135 RMSE:  1.5350461285671946  MAPE: 0.05696541727198809  L2+L1 loss: 1.384
Epoch [136/150], Batch loss: 1.357
Epoch: 136 RMSE:  1.5357861744258678  MAPE: 0.05689682021313122  L2+L1 loss: 1.385
Epoch [137/150], Batch loss: 1.351
Epoch: 137 RMSE:  1.5353583609336505  MAPE: 0.05658841331068205  L2+L1 loss: 1.383
Epoch [138/150], Batch loss: 1.351
Epoch: 138 RMSE:  1.5352407401174197  MAPE: 0.05663124457823923  L2+L1 loss: 1.383
Epoch [139/150], Batch loss: 1.353
Epoch: 139 RMSE:  1.535359579190927  MAPE: 0.05645569692593576  L2+L1 loss: 1.382
Epoch [140/150], Batch loss: 1.362
Epoch: 140 RMSE:  1.535460185815856  MAPE: 0.056373758853158554  L2+L1 loss: 1.382
Epoch [141/150], Batch loss: 1.352
Epoch: 141 RMSE:  1.5358229163594737  MAPE: 0.05655329320789588  L2+L1 loss: 1.383
Epoch [142/150], Batch loss: 1.344
Epoch: 142 RMSE:  1.5357034267143408  MAPE: 0.05653739443745042  L2+L1 loss: 1.381
Epoch [143/150], Batch loss: 1.349
Epoch: 143 RMSE:  1.5353781432809783  MAPE: 0.05718560818131902  L2+L1 loss: 1.385
Epoch [144/150], Batch loss: 1.339
Epoch: 144 RMSE:  1.5356601827058938  MAPE: 0.056809404992825795  L2+L1 loss: 1.385
Epoch [145/150], Batch loss: 1.344
Epoch: 145 RMSE:  1.5353437154679315  MAPE: 0.05695197049750176  L2+L1 loss: 1.385
Epoch [146/150], Batch loss: 1.362
Epoch: 146 RMSE:  1.5351877878561824  MAPE: 0.056846632475554756  L2+L1 loss: 1.385
Epoch [147/150], Batch loss: 1.353
Epoch: 147 RMSE:  1.53589007857043  MAPE: 0.05649360989335571  L2+L1 loss: 1.383
Epoch [148/150], Batch loss: 1.346
Epoch: 148 RMSE:  1.535674637577872  MAPE: 0.05691493558872699  L2+L1 loss: 1.385
Epoch [149/150], Batch loss: 1.347
Epoch: 149 RMSE:  1.5351184839257346  MAPE: 0.05693794195853645  L2+L1 loss: 1.385


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
5.704405 , 5.4837
5.785385 , 5.6265
5.502861 , 6.0533
8.132761 , 7.44
22.0854 , 21.1541
10.241242 , 9.6106
6.8442955 , 6.5451
7.9592648 , 7.5354
20.950552 , 20.3385
20.3164 , 19.384
13.500127 , 14.6377
23.514965 , 22.8133
8.467081 , 8.0321
5.3340626 , 5.6792
26.05582 , 26.054
7.992182 , 7.4997
1.9071674 , 1.7904
8.79958 , 9.6035
12.323845 , 13.9861
14.031778 , 15.7178
9.008266 , 9.7423
7.302784 , 6.9349
121.704315 , 116.1073
21.475164 , 20.3878
58.35191 , 58.3619
42.676872 , 42.2289
7.0460224 , 7.521
7.967455 , 7.4782
11.234089 , 11.5014
5.567066 , 5.2675
27.416933 , 30.789
18.73254 , 17.5373
7.838686 , 7.2378
17.10185 , 16.6122
5.0324116 , 4.7991
21.393076 , 22.3974
8.155268 , 7.9123
243.74582 , 247.054
27.050182 , 26.7897
7.96682 , 8.521
7.5519047 , 7.1291
1.1345711 , 1.1435
47.28484 , 42.3348
7.8826103 , 7.4828
22.38524 , 20.9077
8.171148 , 8.6685
4.0274353 , 3.7808
7.656933 , 7.23
13.137551 , 14.5378
50.52662 , 51.1091
7.6939373 , 8.3434
8.676208 , 8.2241
42.719093 , 38.9111
11.217966 , 11.9081
11.2431965 , 11.135
25.314825 , 24.3423
5.3961906 , 5.0955
23.804197 , 22.8759
7.9974957 , 7.9215
29.00313 , 27.3309
16.359627 , 16.4065
8.418229 , 9.202
20.227303 , 22.6874
29.851585 , 28.891
6.2430477 , 5.9929
20.89565 , 21.3367
8.722392 , 8.2077
18.551712 , 17.8934
7.1520653 , 7.7369
9.432653 , 10.0516
11.241503 , 11.809
8.044432 , 7.7271
23.472202 , 22.3579
0.89834595 , 1.0054
25.14817 , 24.3646
11.252618 , 10.588
22.541239 , 20.9278
5.4717064 , 5.8687
32.051804 , 32.6996
38.37394 , 34.7305
11.250087 , 11.2706
55.02806 , 55.2325
4.948658 , 4.6879
1.141859 , 1.2218
2.151743 , 2.2243
209.60269 , 212.6642
36.303547 , 35.5275
23.475334 , 25.8022
6.0394154 , 6.5692
25.206783 , 24.3901
11.251477 , 11.6114
1.4552021 , 1.4283
39.224724 , 43.7042
8.005745 , 7.5669
49.019775 , 44.057
11.246656 , 11.4793
1.3002987 , 1.4166
25.03876 , 22.2277
6.3418064 , 6.8251
38.324326 , 42.9929
19.366861 , 19.7861
13.565318 , 13.2738
11.238418 , 9.7771
22.107164 , 21.6738
1.1659813 , 1.2592
10.306542 , 9.8911
5.124338 , 5.3709
5.7971783 , 5.5232
4.0024967 , 3.7711
24.521389 , 24.0553
10.133509 , 10.7467
8.393325 , 8.1068
23.317255 , 25.1197
9.62339 , 9.2197
11.252809 , 10.9772
11.222937 , 10.6043
21.531507 , 22.0165
29.149748 , 29.1971
17.744844 , 16.6514
22.780172 , 21.9444
28.985895 , 31.313
2.2608814 , 2.2606
29.248749 , 31.4602
1.5577278 , 1.4646
21.70971 , 19.9835
7.657772 , 6.9527
20.220129 , 19.084
1.9090252 , 1.8664
3.15493 , 2.956
8.311762 , 8.0266
218.4519 , 214.9769
5.0648117 , 5.4448
7.144285 , 7.6224
11.259639 , 11.8427
11.22967 , 11.5908
4.149042 , 3.948
11.254936 , 11.6089
9.171 , 9.9667
8.479567 , 9.0133
28.302383 , 25.8961
49.453285 , 47.664
5.903515 , 6.3458
20.245338 , 19.746
9.547362 , 8.9744
100.01507 , 97.8793
23.34375 , 24.4631
2.0565987 , 2.0083
16.811447 , 16.9392
45.14144 , 49.0151
6.7161922 , 6.6692
2.1785603 , 2.2785
24.33604 , 24.0753
15.749046 , 15.1203
11.255125 , 12.4074
40.823914 , 43.9626
29.3927 , 27.9367
32.382656 , 30.8091
9.261527 , 10.1226
25.690048 , 23.9531
1.9433479 , 1.8274
23.661316 , 27.1083
23.934046 , 22.9978
17.57837 , 19.885
7.539978 , 7.0685
5.8980675 , 5.7696
7.4719067 , 7.0501
1.4624825 , 1.4662
1.834156 , 1.7386
27.727976 , 30.7687
1.3219299 , 1.2538
12.679103 , 11.0695
49.286507 , 50.1607
268.12988 , 264.3729
33.7781 , 34.2801
6.119211 , 6.6063
21.30663 , 23.467
23.828754 , 25.5776
22.658104 , 25.0752
118.92621 , 119.5156
27.476204 , 30.9978
13.475124 , 15.0088
12.013292 , 8.430584
9.440536 , 9.0247
7.831629 , 7.4159
5.7021866 , 5.4048
7.191313 , 6.8512
8.605629 , 8.1455
7.6561165 , 7.316
1.1474743 , 1.1665
31.37807 , 30.718
52.23558 , 58.1978
1.5407867 , 1.5704
9.126222 , 9.9326
7.1957436 , 6.7949
21.774132 , 20.4429
11.244523 , 10.185
35.679367 , 37.4119
5.28883 , 5.6902
2.0801868 , 2.0415
17.723448 , 18.2391
11.75885 , 10.4981
51.305305 , 56.8843
19.336548 , 18.3269
6.4354267 , 6.1347
19.402859 , 19.0191
5.9459305 , 5.4963
96.46959 , 94.0797
25.531145 , 24.4843
21.912424 , 21.417
6.135067 , 5.9069
49.214157 , 45.3726
25.036743 , 23.1168
12.33615 , 13.8028
5.36491 , 5.0903
2.1207218 , 2.056
30.361597 , 33.4864
25.11532 , 23.7133
4.503273 , 4.7698
29.47525 , 33.5429
19.734976 , 21.8898
27.732494 , 23.6683
35.15107 , 33.2161
11.240969 , 10.0843
5.8155003 , 5.7518
22.967201 , 22.139
26.674267 , 25.6333
38.407204 , 36.8031
14.86181 , 14.7895
6.6496506 , 6.3206
5.3517323 , 5.0972
15.979302 , 15.0997
1.0661316 , 1.0808
6.062742 , 6.0173
32.723476 , 30.8261
128.44543 , 126.0529
25.323015 , 29.2493
22.379148 , 20.9216
7.1731415 , 6.7498
14.482498 , 13.6912
21.089766 , 19.9483
11.245349 , 12.4269
33.73869 , 34.2522
21.17608 , 19.4974
25.036903 , 23.2867
8.894987 , 8.4826
1.5698891 , 1.5022
19.09448 , 17.8306
9.5954685 , 10.3607
5.4497337 , 5.2107
23.009369 , 21.9917
2.0945587 , 2.2052
6.754696 , 6.5478
11.619704 , 13.449
8.209705 , 8.7503
1.8792572 , 1.9292
7.1968155 , 7.821
29.559694 , 28.2643
5.544468 , 5.4784
29.276428 , 27.8889
11.217742 , 11.0937
2.2316036 , 2.2053
5.193077 , 4.8909
20.075197 , 20.7085
11.221596 , 11.4376
14.64215 , 14.3597
7.2096405 , 7.029
2.2720623 , 4.100066
4.116764 , 4.4014
1.187851 , 1.174
11.228588 , 11.2161
7.9964695 , 7.5516
21.328661 , 19.5546
45.933205 , 45.2417
9.617142 , 9.0908
5.243656 , 6.083
25.46545 , 25.9578
17.643202 , 17.8104
7.1081333 , 6.7397
6.9506702 , 7.6237
5.30389 , 5.0378
7.6766033 , 7.2698
4.873596 , 5.2028
12.911593 , 14.7004
14.331391 , 14.9641
6.5959206 , 6.5287
5.0629997 , 4.8142
41.536987 , 43.6452
6.4785595 , 7.1703
28.747364 , 29.8592
9.588039 , 10.1428
22.286606 , 20.6451
1.6112862 , 1.5052
1.2255306 , 1.1983
28.941618 , 27.9208
52.321297 , 50.4055
11.252434 , 10.9052
11.246309 , 10.287
7.079569 , 6.7153
23.47311 , 22.7747
31.108627 , 29.6946
8.183186 , 8.7346
126.28624 , 122.6004
5.6482944 , 6.0462
8.741816 , 8.059
5.748804 , 5.4356
9.684406 , 10.5637
6.336813 , 7.0086
29.210693 , 32.271
26.043797 , 25.8244
6.131216 , 6.7903
7.3860855 , 7.0735
11.881236 , 10.8373
7.770294 , 7.3442
118.91024 , 123.8062
1.6345234 , 1.5979
8.357688 , 7.9409
22.62045 , 21.1573
8.583392 , 9.2319
6.3328915 , 6.0821
4.773527 , 4.4974
15.0935135 , 14.6195
24.247673 , 23.4205
19.252306 , 17.741
32.8205 , 37.1405
4.530113 , 4.0493
24.192204 , 26.9055
0.31282997 , 0.284881
5.82848 , 6.4981
24.000673 , 23.0868
19.485056 , 18.1237
28.401642 , 27.1628
45.757076 , 42.1239
28.108154 , 30.2656
24.36664 , 23.3737
1.9308491 , 1.8308
35.610023 , 37.3114
23.335629 , 22.1272
25.269434 , 23.0704
21.813705 , 20.841
42.90227 , 40.4706
1.708765 , 1.865
21.73458 , 23.4441
36.046024 , 35.1074
20.247486 , 20.1294
21.521286 , 23.1548
6.9250164 , 6.5559
4.7428684 , 4.6653
5.739069 , 6.2987
8.433168 , 7.939
7.122282 , 7.5217
5.2654476 , 5.0687
7.753784 , 8.346
35.86577 , 37.5349
11.248978 , 13.2597
6.339157 , 7.0737
25.044731 , 24.1825
21.154358 , 23.6506
3.6930218 , 3.4875
1.1656914 , 1.1686
1.7572708 , 1.872011
19.559706 , 19.6415
8.914134 , 9.7072
47.26422 , 50.8715
7.851143 , 8.3835
22.19328 , 21.2833
103.46894 , 102.033
4.716408 , 5.0106
5.8190136 , 6.2998
46.20049 , 44.1975
17.659887 , 17.8079
4.1347694 , 3.9459
20.487236 , 19.4908
68.55965 , 65.3663
28.293127 , 27.0771
25.430075 , 25.5547
7.1135864 , 7.6304
47.38629 , 50.2946
1.6066227 , 1.5534
5.33239 , 5.6691
1.04315 , 1.1181
11.248716 , 12.1076
0.9717827 , 1.1279
7.335003 , 7.829
7.3593254 , 7.0031
5.494917 , 5.1989
2.3406181 , 2.3937
1.7938519 , 1.8609
25.445087 , 25.1689
5.04747 , 5.3514
25.48958 , 24.2649
6.37076 , 6.8697
2.4780655 , 2.3518
8.047886 , 8.1119
21.768742 , 23.4217
7.206274 , 7.6431
29.282295 , 28.1879
10.881038 , 10.022
21.989368 , 24.5565
9.592203 , 10.1878
33.73636 , 35.0278
33.401367 , 34.6839
2.10606 , 2.135
23.137419 , 21.7922
19.541142 , 18.2954
19.80991 , 19.63
142.40569 , 145.9228
7.6049385 , 8.0118
4.3195057 , 4.2576
49.151577 , 49.4628
38.791885 , 34.766
20.182516 , 20.871
6.1027393 , 6.5788
28.322205 , 27.075
48.27366 , 48.7818
7.4297886 , 7.0048
27.85992 , 26.5831
1.92239 , 1.89644
9.729385 , 9.556
16.49027 , 16.6821
4.7807198 , 4.5162
8.406101 , 8.0405
28.148935 , 26.5112
6.6780396 , 7.2791
43.9358 , 44.1991
20.882378 , 20.0144
6.9978657 , 6.6813
8.337407 , 7.8203
5.131338 , 4.8788
9.016962 , 8.5755
7.1250153 , 6.7185
293.22314 , 304.7569
1.9161758 , 2.076
11.243284 , 11.5324
6.2823334 , 6.755
21.284557 , 21.8143
55.83072 , 58.76
11.250275 , 11.8825
7.0302258 , 7.6784
11.230974 , 11.4784
7.2911263 , 6.6034
35.47121 , 34.1683
7.246275 , 6.8435
8.318118 , 7.8174
15.9065895 , 15.1923
6.080105 , 6.6111
12.20157 , 12.1473
9.141293 , 9.8295
12.637803 , 12.1518
44.206314 , 43.125
19.54087 , 21.4269
1.5797863 , 1.5037
7.745222 , 7.429
46.733974 , 44.2047
24.086561 , 23.974
13.49637 , 12.8887
20.402374 , 21.2438
13.370559 , 13.3502
48.82579 , 47.0751
1.6680679 , 1.5666
11.936586 , 11.703
14.399172 , 13.9406
22.942375 , 22.8771
6.206869 , 6.7048
20.372982 , 19.9114
20.636848 , 21.4286
4.1481495 , 3.9545
9.0129595 , 8.7821
22.650217 , 22.3798
18.663445 , 20.8535
8.634207 , 8.252
14.72622 , 15.1453
7.293844 , 6.8678
151.85678 , 151.7387
22.962278 , 21.7034
30.487143 , 29.2077
9.560258 , 10.1843
56.805836 , 55.3157
5.3787117 , 5.0553
8.530523 , 8.282
1.3465347 , 1.3442
7.9031754 , 8.3933
25.036804 , 23.6844
36.90343 , 39.7144
5.3207893 , 5.0649
3.9198418 , 3.7402
32.385666 , 34.9744
15.733421 , 16.8258
12.503825 , 12.3963
8.693567 , 8.3609
25.25339 , 24.8778
39.633327 , 41.1687
5.914137 , 6.6057
15.546637 , 16.0763
1.8667717 , 1.8363
1.4481564 , 1.4588
1.3818264 , 1.3292
7.255726 , 6.8729
32.436813 , 32.0022
41.59324 , 43.7787
22.874184 , 21.9889
20.19252 , 19.5255
5.294262 , 4.9704
8.530805 , 8.0612
59.38295 , 59.3762
13.665171 , 12.4769
15.614855 , 15.1058
21.194004 , 21.1151
5.748724 , 5.4849
64.33058 , 65.4885
11.253425 , 11.8254
22.885683 , 21.763
5.5603714 , 5.2841
11.221987 , 12.3996
22.442438 , 21.722
3.2580261 , 3.1975
31.30043 , 31.8305
5.710354 , 5.5928
27.977999 , 30.2767
7.8030167 , 7.3249
27.791847 , 26.8043
6.9206276 , 7.3513
9.254048 , 9.9319
6.98872 , 6.6567
24.152033 , 23.9524
7.553938 , 7.2323
10.283602 , 11.4524
28.719236 , 31.8471
54.407093 , 58.7516
4.315672 , 4.5921
34.100594 , 30.5698
19.928463 , 19.7932
5.469845 , 5.1562
32.622643 , 34.9618
11.223021 , 10.3814
2.692873 , 2.7005
26.693865 , 30.2534
14.211072 , 12.5132
8.887968 , 9.4493
5.2733116 , 5.0279
1.4325027 , 1.3974
19.712616 , 18.8778
64.003426 , 59.7001
10.72275 , 11.1713
18.59388 , 16.8796
16.931967 , 19.5059
40.004295 , 39.3289
16.31941 , 14.2076
8.229109 , 8.4968
25.193626 , 25.0477
RMSE:  1.4954699763617458  MAPE: 0.055233145737413884
5: ground truth total-  76  predicted total -  76
100: ground truth total-  460  predicted total -  460
 more 100: ground truth total -  13  predicted total -  13



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
0.30319977 , 0.294518
0.33634758 , 0.324536
8.826222 , 5.395045
11.11219 , 7.451263
10.064713 , 6.363127
4.2436924 , 3.64278
7.2552223 , 4.320264
6.578821 , 5.413956
0.48279 , 0.326285
0.6527443 , 0.198183
6.728817 , 5.506002
3.5951843 , 2.286682
8.469313 , 7.207104
0.32374573 , 0.299314
0.35124397 , 0.306801
9.7176695 , 8.803327
4.7538204 , 4.358702
8.16547 , 7.129483
4.4987583 , 3.6542
4.647955 , 3.662855
10.0148115 , 9.022602
0.39299393 , 0.328157
6.3223896 , 5.383395
0.36948204 , 0.322715
12.006287 , 12.298204
10.956159 , 10.556656
0.42613602 , 0.326436
5.55311 , 3.306538
4.8359356 , 4.499164
RMSE:  1.5003513764629206  MAPE: 0.32385053084753046
