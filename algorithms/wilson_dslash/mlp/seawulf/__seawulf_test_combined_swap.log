['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
2707
19
<class 'numpy.dtype'> float64
2707
train batches:  122  validate samples: 216  test samples: 541
Epoch [0/150], Batch loss: 72.177
Epoch: 0 RMSE:  32.19455746013357  MAPE: 0.959580238295863  L2+L1 loss: 13.212
Epoch [1/150], Batch loss: 58.353
Epoch: 1 RMSE:  31.401888048633165  MAPE: 1.4678202411080887  L2+L1 loss: 13.741
Epoch [2/150], Batch loss: 57.937
Epoch: 2 RMSE:  31.264421005120617  MAPE: 1.6083197003890624  L2+L1 loss: 13.958
Epoch [3/150], Batch loss: 57.557
Epoch: 3 RMSE:  31.177509544309917  MAPE: 1.719069353882445  L2+L1 loss: 14.132
Epoch [4/150], Batch loss: 57.952
Epoch: 4 RMSE:  31.20840519851186  MAPE: 1.6766937516099854  L2+L1 loss: 14.062
Epoch [5/150], Batch loss: 57.562
Epoch: 5 RMSE:  31.2074326066457  MAPE: 1.6779653991533203  L2+L1 loss: 14.064
Epoch [6/150], Batch loss: 56.784
Epoch: 6 RMSE:  31.200455871616875  MAPE: 1.68718603944515  L2+L1 loss: 14.078
Epoch [7/150], Batch loss: 63.381
Epoch: 7 RMSE:  31.42765869884831  MAPE: 1.4444482352170083  L2+L1 loss: 13.705
Epoch [8/150], Batch loss: 58.227
Epoch: 8 RMSE:  31.278646191264393  MAPE: 1.5922450856744172  L2+L1 loss: 13.933
Epoch [9/150], Batch loss: 58.262
Epoch: 9 RMSE:  31.21082032712839  MAPE: 1.6735496487321082  L2+L1 loss: 14.057
Epoch [10/150], Batch loss: 57.465
Epoch: 10 RMSE:  31.228383570531303  MAPE: 1.6512401786836526  L2+L1 loss: 14.022
Epoch [11/150], Batch loss: 57.032
Epoch: 11 RMSE:  31.174363741891398  MAPE: 1.7236341580332246  L2+L1 loss: 14.14
Epoch [12/150], Batch loss: 75.04
Epoch: 12 RMSE:  31.12244267465036  MAPE: 1.2392419582636152  L2+L1 loss: 12.579
Epoch [13/150], Batch loss: 53.434
Epoch: 13 RMSE:  30.37470029238262  MAPE: 1.0084301790801675  L2+L1 loss: 10.719
Epoch [14/150], Batch loss: 59.349
Epoch: 14 RMSE:  31.087656728823408  MAPE: 1.8763584343279303  L2+L1 loss: 14.428
Epoch [15/150], Batch loss: 58.81
Epoch: 15 RMSE:  31.18360592653627  MAPE: 1.7103500487626238  L2+L1 loss: 14.116
Epoch [16/150], Batch loss: 57.002
Epoch: 16 RMSE:  31.232856993761025  MAPE: 1.6457047473062394  L2+L1 loss: 14.014
Epoch [17/150], Batch loss: 58.376
Epoch: 17 RMSE:  31.202972200085217  MAPE: 1.683838534057696  L2+L1 loss: 14.073
Epoch [18/150], Batch loss: 45.07
Epoch: 18 RMSE:  8.078966012137506  MAPE: 0.37633085667944155  L2+L1 loss: 3.717
Epoch [19/150], Batch loss: 55.65
Epoch: 19 RMSE:  31.24687789954362  MAPE: 1.6287812306033684  L2+L1 loss: 13.988
Epoch [20/150], Batch loss: 55.516
Epoch: 20 RMSE:  28.253118569211676  MAPE: 1.1303203706014264  L2+L1 loss: 9.354
Epoch [21/150], Batch loss: 55.187
Epoch: 21 RMSE:  29.195736689006747  MAPE: 2.5844203696362653  L2+L1 loss: 14.2
Epoch [22/150], Batch loss: 49.041
Epoch: 22 RMSE:  25.807712844704977  MAPE: 1.229767545044048  L2+L1 loss: 10.914
Epoch [23/150], Batch loss: 54.329
Epoch: 23 RMSE:  50.63318719702837  MAPE: 2.704477788922922  L2+L1 loss: 16.709
Epoch [24/150], Batch loss: 70.243
Epoch: 24 RMSE:  31.55317679462393  MAPE: 1.3409483006589544  L2+L1 loss: 13.558
Epoch [25/150], Batch loss: 57.858
Epoch: 25 RMSE:  31.276258390829266  MAPE: 1.5949128492251148  L2+L1 loss: 13.937
Epoch [26/150], Batch loss: 57.692
Epoch: 26 RMSE:  31.22785155887888  MAPE: 1.6519022990414358  L2+L1 loss: 14.024
Epoch [27/150], Batch loss: 58.018
Epoch: 27 RMSE:  31.22134236655878  MAPE: 1.6600707107044825  L2+L1 loss: 14.036
Epoch [28/150], Batch loss: 57.045
Epoch: 28 RMSE:  31.216466178544618  MAPE: 1.6662737985630718  L2+L1 loss: 14.046
Epoch [29/150], Batch loss: 57.379
Epoch: 29 RMSE:  31.17751158596994  MAPE: 1.719066405586721  L2+L1 loss: 14.132
Epoch [30/150], Batch loss: 58.546
Epoch: 30 RMSE:  31.184841790524615  MAPE: 1.708609086631789  L2+L1 loss: 14.114
Epoch [31/150], Batch loss: 58.87
Epoch: 31 RMSE:  31.19156986478175  MAPE: 1.699239555144773  L2+L1 loss: 14.097
Epoch [32/150], Batch loss: 57.742
Epoch: 32 RMSE:  31.192805221548458  MAPE: 1.6975429549706451  L2+L1 loss: 14.095
Epoch [33/150], Batch loss: 57.976
Epoch: 33 RMSE:  31.197857385496608  MAPE: 1.690679509715862  L2+L1 loss: 14.084
Epoch [34/150], Batch loss: 57.246
Epoch: 34 RMSE:  31.197471960822384  MAPE: 1.6911997753322126  L2+L1 loss: 14.085
Epoch [35/150], Batch loss: 57.482
Epoch: 35 RMSE:  31.20088046606189  MAPE: 1.6866175160985468  L2+L1 loss: 14.077
Epoch [36/150], Batch loss: 57.211
Epoch: 36 RMSE:  31.203346182864657  MAPE: 1.6833434662232918  L2+L1 loss: 14.072
Epoch [37/150], Batch loss: 58.172
Epoch: 37 RMSE:  31.20628173235326  MAPE: 1.6794742695840374  L2+L1 loss: 14.066
Epoch [38/150], Batch loss: 57.522
Epoch: 38 RMSE:  31.20550239245604  MAPE: 1.6804985908147974  L2+L1 loss: 14.068
Epoch [39/150], Batch loss: 57.489
Epoch: 39 RMSE:  31.20490482166394  MAPE: 1.6812854123520042  L2+L1 loss: 14.069
Epoch [40/150], Batch loss: 45.263
Epoch: 40 RMSE:  8.608161500039143  MAPE: 1.5360679614510107  L2+L1 loss: 7.554
Epoch [41/150], Batch loss: 26.936
Epoch: 41 RMSE:  7.046498327327396  MAPE: 1.1451488572858624  L2+L1 loss: 5.369
Epoch [42/150], Batch loss: 15.154
Epoch: 42 RMSE:  7.553779497977087  MAPE: 0.593844895683588  L2+L1 loss: 4.93
Epoch [43/150], Batch loss: 10.859
Epoch: 43 RMSE:  4.168038792805954  MAPE: 0.18713386511595623  L2+L1 loss: 2.271
Epoch [44/150], Batch loss: 8.436
Epoch: 44 RMSE:  4.300860377379495  MAPE: 0.24665139182148918  L2+L1 loss: 1.909
Epoch [45/150], Batch loss: 8.242
Epoch: 45 RMSE:  2.8732816258815252  MAPE: 0.1709004896452626  L2+L1 loss: 2.134
Epoch [46/150], Batch loss: 7.499
Epoch: 46 RMSE:  2.1801635119636953  MAPE: 0.1354649235727918  L2+L1 loss: 1.361
Epoch [47/150], Batch loss: 6.8
Epoch: 47 RMSE:  3.6544681507725625  MAPE: 0.11448035345697706  L2+L1 loss: 1.499
Epoch [48/150], Batch loss: 8.008
Epoch: 48 RMSE:  2.2101188967121015  MAPE: 0.1107222891322631  L2+L1 loss: 1.195
Epoch [49/150], Batch loss: 5.698
Epoch: 49 RMSE:  1.6283125316638052  MAPE: 0.115850884872758  L2+L1 loss: 1.103
Epoch [50/150], Batch loss: 8.668
Epoch: 50 RMSE:  4.623435297294901  MAPE: 0.12860870163325114  L2+L1 loss: 1.88
Epoch [51/150], Batch loss: 6.875
Epoch: 51 RMSE:  1.3575932392535164  MAPE: 0.09725160121451983  L2+L1 loss: 0.993
Epoch [52/150], Batch loss: 10.689
Epoch: 52 RMSE:  5.4787299510657625  MAPE: 0.18943322569532187  L2+L1 loss: 2.431
Epoch [53/150], Batch loss: 7.544
Epoch: 53 RMSE:  1.6772676800587145  MAPE: 0.11779005461304226  L2+L1 loss: 1.076
Epoch [54/150], Batch loss: 9.172
Epoch: 54 RMSE:  8.163508184798413  MAPE: 0.7848557289743486  L2+L1 loss: 3.24
Epoch [55/150], Batch loss: 8.794
Epoch: 55 RMSE:  4.326546061292013  MAPE: 0.07159621554278604  L2+L1 loss: 1.617
Epoch [56/150], Batch loss: 4.405
Epoch: 56 RMSE:  1.6963731164723106  MAPE: 0.09698858200352055  L2+L1 loss: 1.239
Epoch [57/150], Batch loss: 8.521
Epoch: 57 RMSE:  1.7369266137952009  MAPE: 0.08581973955412406  L2+L1 loss: 1.094
Epoch [58/150], Batch loss: 4.618
Epoch: 58 RMSE:  2.754889632705548  MAPE: 0.1011012534079201  L2+L1 loss: 1.535
Epoch [59/150], Batch loss: 5.061
Epoch: 59 RMSE:  1.4761520820181635  MAPE: 0.11384390227718358  L2+L1 loss: 1.085
Epoch [60/150], Batch loss: 3.072
Epoch: 60 RMSE:  1.6791474220630178  MAPE: 0.055556774582730326  L2+L1 loss: 0.857
Epoch [61/150], Batch loss: 3.131
Epoch: 61 RMSE:  2.310011895759215  MAPE: 0.059310860446794116  L2+L1 loss: 1.02
Epoch [62/150], Batch loss: 2.925
Epoch: 62 RMSE:  2.4589043513343674  MAPE: 0.05729357356619351  L2+L1 loss: 1.005
Epoch [63/150], Batch loss: 2.811
Epoch: 63 RMSE:  1.5260223960266879  MAPE: 0.05879911927687557  L2+L1 loss: 0.783
Epoch [64/150], Batch loss: 2.693
Epoch: 64 RMSE:  1.4307363886690325  MAPE: 0.06780733757232722  L2+L1 loss: 0.83
Epoch [65/150], Batch loss: 2.8
Epoch: 65 RMSE:  1.8232928521538798  MAPE: 0.054406083818413276  L2+L1 loss: 0.851
Epoch [66/150], Batch loss: 2.607
Epoch: 66 RMSE:  1.013288174959526  MAPE: 0.05982621597805489  L2+L1 loss: 0.72
Epoch [67/150], Batch loss: 2.61
Epoch: 67 RMSE:  1.6911479452460112  MAPE: 0.05743895674213705  L2+L1 loss: 0.81
Epoch [68/150], Batch loss: 2.664
Epoch: 68 RMSE:  1.5974183805968378  MAPE: 0.05091997261115347  L2+L1 loss: 0.768
Epoch [69/150], Batch loss: 2.352
Epoch: 69 RMSE:  1.4899901237397581  MAPE: 0.04662614888200825  L2+L1 loss: 0.773
Epoch [70/150], Batch loss: 2.556
Epoch: 70 RMSE:  1.4999528123219448  MAPE: 0.05751977372555745  L2+L1 loss: 0.793
Epoch [71/150], Batch loss: 2.465
Epoch: 71 RMSE:  1.0328827273163703  MAPE: 0.054019979920864415  L2+L1 loss: 0.688
Epoch [72/150], Batch loss: 2.539
Epoch: 72 RMSE:  1.140934598924754  MAPE: 0.0514714259414875  L2+L1 loss: 0.706
Epoch [73/150], Batch loss: 2.533
Epoch: 73 RMSE:  2.085866436759897  MAPE: 0.05319375474877482  L2+L1 loss: 0.884
Epoch [74/150], Batch loss: 2.428
Epoch: 74 RMSE:  1.9101646870566624  MAPE: 0.057117977259278106  L2+L1 loss: 0.854
Epoch [75/150], Batch loss: 2.407
Epoch: 75 RMSE:  1.0816445084127264  MAPE: 0.049115030142105456  L2+L1 loss: 0.66
Epoch [76/150], Batch loss: 2.276
Epoch: 76 RMSE:  1.5571782810628263  MAPE: 0.05460624578677935  L2+L1 loss: 0.758
Epoch [77/150], Batch loss: 2.252
Epoch: 77 RMSE:  1.202029174982508  MAPE: 0.04572808969671717  L2+L1 loss: 0.654
Epoch [78/150], Batch loss: 2.547
Epoch: 78 RMSE:  1.7191467792752955  MAPE: 0.041683436095336035  L2+L1 loss: 0.75
Epoch [79/150], Batch loss: 2.301
Epoch: 79 RMSE:  1.2397562530473023  MAPE: 0.0371368895641492  L2+L1 loss: 0.661
Epoch [80/150], Batch loss: 2.273
Epoch: 80 RMSE:  1.525106880974137  MAPE: 0.03625196732676477  L2+L1 loss: 0.727
Epoch [81/150], Batch loss: 2.377
Epoch: 81 RMSE:  1.2176004573481107  MAPE: 0.047855203670590374  L2+L1 loss: 0.688
Epoch [82/150], Batch loss: 2.199
Epoch: 82 RMSE:  1.3980089811061185  MAPE: 0.05096279281290084  L2+L1 loss: 0.709
Epoch [83/150], Batch loss: 2.152
Epoch: 83 RMSE:  1.6525673198692932  MAPE: 0.03628332522369454  L2+L1 loss: 0.713
Epoch [84/150], Batch loss: 2.086
Epoch: 84 RMSE:  1.1552051158962882  MAPE: 0.04654018447772495  L2+L1 loss: 0.646
Epoch [85/150], Batch loss: 2.064
Epoch: 85 RMSE:  0.9681223948145937  MAPE: 0.0525321919434552  L2+L1 loss: 0.633
Epoch [86/150], Batch loss: 2.155
Epoch: 86 RMSE:  1.056424597384796  MAPE: 0.05858113922913645  L2+L1 loss: 0.732
Epoch [87/150], Batch loss: 2.079
Epoch: 87 RMSE:  0.9906789534211822  MAPE: 0.0381000803276696  L2+L1 loss: 0.64
Epoch [88/150], Batch loss: 1.985
Epoch: 88 RMSE:  1.3553631082061786  MAPE: 0.045735079744217394  L2+L1 loss: 0.68
Epoch [89/150], Batch loss: 1.99
Epoch: 89 RMSE:  1.3754920019546382  MAPE: 0.04403027446847712  L2+L1 loss: 0.691
Epoch [90/150], Batch loss: 1.818
Epoch: 90 RMSE:  1.459056087819803  MAPE: 0.0390710963844897  L2+L1 loss: 0.653
Epoch [91/150], Batch loss: 1.815
Epoch: 91 RMSE:  1.3352887685671553  MAPE: 0.039015170591453936  L2+L1 loss: 0.626
Epoch [92/150], Batch loss: 1.747
Epoch: 92 RMSE:  1.2801686856798054  MAPE: 0.03805341236732325  L2+L1 loss: 0.604
Epoch [93/150], Batch loss: 1.763
Epoch: 93 RMSE:  1.191459047421689  MAPE: 0.03750429302908903  L2+L1 loss: 0.596
Epoch [94/150], Batch loss: 1.776
Epoch: 94 RMSE:  1.323793690142651  MAPE: 0.037972719906334176  L2+L1 loss: 0.619
Epoch [95/150], Batch loss: 1.769
Epoch: 95 RMSE:  1.3949266920653747  MAPE: 0.03689982878171617  L2+L1 loss: 0.629
Epoch [96/150], Batch loss: 1.765
Epoch: 96 RMSE:  1.3248969766747272  MAPE: 0.036177124097053905  L2+L1 loss: 0.61
Epoch [97/150], Batch loss: 1.727
Epoch: 97 RMSE:  1.25159356699383  MAPE: 0.03823930820158849  L2+L1 loss: 0.601
Epoch [98/150], Batch loss: 1.772
Epoch: 98 RMSE:  1.2720582103529154  MAPE: 0.037116871874965617  L2+L1 loss: 0.598
Epoch [99/150], Batch loss: 1.766
Epoch: 99 RMSE:  1.224191374206046  MAPE: 0.03826533412315088  L2+L1 loss: 0.596
Epoch [100/150], Batch loss: 1.746
Epoch: 100 RMSE:  1.3131057497974112  MAPE: 0.03677657263289285  L2+L1 loss: 0.606
Epoch [101/150], Batch loss: 1.733
Epoch: 101 RMSE:  1.4537022663729842  MAPE: 0.03701015153913831  L2+L1 loss: 0.64
Epoch [102/150], Batch loss: 1.744
Epoch: 102 RMSE:  1.3429918612711103  MAPE: 0.0364017487070351  L2+L1 loss: 0.611
Epoch [103/150], Batch loss: 1.733
Epoch: 103 RMSE:  1.26120704833718  MAPE: 0.03638508150048029  L2+L1 loss: 0.593
Epoch [104/150], Batch loss: 1.72
Epoch: 104 RMSE:  1.3155120028697112  MAPE: 0.03612398166282909  L2+L1 loss: 0.606
Epoch [105/150], Batch loss: 1.749
Epoch: 105 RMSE:  1.2113807746960783  MAPE: 0.036843168317240826  L2+L1 loss: 0.591
Epoch [106/150], Batch loss: 1.725
Epoch: 106 RMSE:  1.234046216301277  MAPE: 0.03593926507515722  L2+L1 loss: 0.586
Epoch [107/150], Batch loss: 1.746
Epoch: 107 RMSE:  1.309716106093322  MAPE: 0.037124040670359945  L2+L1 loss: 0.614
Epoch [108/150], Batch loss: 1.748
Epoch: 108 RMSE:  1.291190976408675  MAPE: 0.03626252351757521  L2+L1 loss: 0.599
Epoch [109/150], Batch loss: 1.65
Epoch: 109 RMSE:  1.3648058901023374  MAPE: 0.03566629670200315  L2+L1 loss: 0.615
Epoch [110/150], Batch loss: 1.749
Epoch: 110 RMSE:  1.4217674209457039  MAPE: 0.03696637668073607  L2+L1 loss: 0.637
Epoch [111/150], Batch loss: 1.71
Epoch: 111 RMSE:  1.4757549096600802  MAPE: 0.037133420084929356  L2+L1 loss: 0.651
Epoch [112/150], Batch loss: 1.744
Epoch: 112 RMSE:  1.3460391891979167  MAPE: 0.036581094116014276  L2+L1 loss: 0.612
Epoch [113/150], Batch loss: 1.77
Epoch: 113 RMSE:  1.2938270518815862  MAPE: 0.0377278760946434  L2+L1 loss: 0.606
Epoch [114/150], Batch loss: 1.694
Epoch: 114 RMSE:  1.2457852311452793  MAPE: 0.03715943706672483  L2+L1 loss: 0.596
Epoch [115/150], Batch loss: 1.687
Epoch: 115 RMSE:  1.3833576480735486  MAPE: 0.03600926842477741  L2+L1 loss: 0.623
Epoch [116/150], Batch loss: 1.713
Epoch: 116 RMSE:  1.2678086290154753  MAPE: 0.036169504465240046  L2+L1 loss: 0.593
Epoch [117/150], Batch loss: 1.71
Epoch: 117 RMSE:  1.393804520610169  MAPE: 0.03624952420734268  L2+L1 loss: 0.623
Epoch [118/150], Batch loss: 1.719
Epoch: 118 RMSE:  1.344693047189839  MAPE: 0.03517052518485176  L2+L1 loss: 0.607
Epoch [119/150], Batch loss: 1.716
Epoch: 119 RMSE:  1.205505970124294  MAPE: 0.03594294802091445  L2+L1 loss: 0.579
Epoch [120/150], Batch loss: 1.722
Epoch: 120 RMSE:  1.2487179379990212  MAPE: 0.03592425795348417  L2+L1 loss: 0.581
Epoch [121/150], Batch loss: 1.676
Epoch: 121 RMSE:  1.2700964424758954  MAPE: 0.03588714265140258  L2+L1 loss: 0.587
Epoch [122/150], Batch loss: 1.699
Epoch: 122 RMSE:  1.2850439484880285  MAPE: 0.03602733327801664  L2+L1 loss: 0.593
Epoch [123/150], Batch loss: 1.647
Epoch: 123 RMSE:  1.3090752822164184  MAPE: 0.03601732178787684  L2+L1 loss: 0.599
Epoch [124/150], Batch loss: 1.683
Epoch: 124 RMSE:  1.3119377998750048  MAPE: 0.036073627236070216  L2+L1 loss: 0.601
Epoch [125/150], Batch loss: 1.682
Epoch: 125 RMSE:  1.3123846617924935  MAPE: 0.03600355890155045  L2+L1 loss: 0.6
Epoch [126/150], Batch loss: 1.661
Epoch: 126 RMSE:  1.3172881199166318  MAPE: 0.03595758689148448  L2+L1 loss: 0.601
Epoch [127/150], Batch loss: 1.667
Epoch: 127 RMSE:  1.3192857920486751  MAPE: 0.03596819739654745  L2+L1 loss: 0.602
Epoch [128/150], Batch loss: 1.717
Epoch: 128 RMSE:  1.3192319109678852  MAPE: 0.03579888488222883  L2+L1 loss: 0.599
Epoch [129/150], Batch loss: 1.697
Epoch: 129 RMSE:  1.3155927990679113  MAPE: 0.03575003193834964  L2+L1 loss: 0.599
Epoch [130/150], Batch loss: 1.689
Epoch: 130 RMSE:  1.3287486098740804  MAPE: 0.03605934462575636  L2+L1 loss: 0.603
Epoch [131/150], Batch loss: 1.702
Epoch: 131 RMSE:  1.3241139302458322  MAPE: 0.03578960752348285  L2+L1 loss: 0.601
Epoch [132/150], Batch loss: 1.664
Epoch: 132 RMSE:  1.3221573786184688  MAPE: 0.035916762937495135  L2+L1 loss: 0.601
Epoch [133/150], Batch loss: 1.685
Epoch: 133 RMSE:  1.3103675168058992  MAPE: 0.03576975960917611  L2+L1 loss: 0.598
Epoch [134/150], Batch loss: 1.697
Epoch: 134 RMSE:  1.323665862969004  MAPE: 0.035786202024295256  L2+L1 loss: 0.601
Epoch [135/150], Batch loss: 1.683
Epoch: 135 RMSE:  1.3239188798050179  MAPE: 0.035773926557860436  L2+L1 loss: 0.602
Epoch [136/150], Batch loss: 1.667
Epoch: 136 RMSE:  1.323548647628585  MAPE: 0.03582682556050271  L2+L1 loss: 0.601
Epoch [137/150], Batch loss: 1.705
Epoch: 137 RMSE:  1.327433861329346  MAPE: 0.03561407294783441  L2+L1 loss: 0.601
Epoch [138/150], Batch loss: 1.657
Epoch: 138 RMSE:  1.3283176136390271  MAPE: 0.03573430677060143  L2+L1 loss: 0.602
Epoch [139/150], Batch loss: 1.666
Epoch: 139 RMSE:  1.3303879213652439  MAPE: 0.03563665962150676  L2+L1 loss: 0.602
Epoch [140/150], Batch loss: 1.703
Epoch: 140 RMSE:  1.32341342425391  MAPE: 0.03567606982179107  L2+L1 loss: 0.601
Epoch [141/150], Batch loss: 1.682
Epoch: 141 RMSE:  1.3210168096967467  MAPE: 0.03564693874373176  L2+L1 loss: 0.6
Epoch [142/150], Batch loss: 1.692
Epoch: 142 RMSE:  1.3226565544396056  MAPE: 0.03573727321494537  L2+L1 loss: 0.601
Epoch [143/150], Batch loss: 1.648
Epoch: 143 RMSE:  1.323165273470243  MAPE: 0.035592131644691105  L2+L1 loss: 0.6
Epoch [144/150], Batch loss: 1.713
Epoch: 144 RMSE:  1.3174548663929064  MAPE: 0.035531327954349046  L2+L1 loss: 0.599
Epoch [145/150], Batch loss: 1.644
Epoch: 145 RMSE:  1.3126093348266608  MAPE: 0.03556548699618741  L2+L1 loss: 0.599
Epoch [146/150], Batch loss: 1.689
Epoch: 146 RMSE:  1.3278986088797629  MAPE: 0.03555088077651364  L2+L1 loss: 0.602
Epoch [147/150], Batch loss: 1.688
Epoch: 147 RMSE:  1.3235398517173547  MAPE: 0.035481804968279004  L2+L1 loss: 0.599
Epoch [148/150], Batch loss: 1.711
Epoch: 148 RMSE:  1.3228542907222403  MAPE: 0.03572580074643722  L2+L1 loss: 0.601
Epoch [149/150], Batch loss: 1.706
Epoch: 149 RMSE:  1.321288645280596  MAPE: 0.03556620628803406  L2+L1 loss: 0.6


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
18.708416 , 17.6499
19.04327 , 19.1884
90.6367 , 87.5936
1.3325615 , 1.4016
7.4270926 , 7.4744
4.297269 , 4.2505
15.191808 , 15.2208
4.9194593 , 4.9778
20.045454 , 20.2647
3.6460238 , 3.6269
3.7898521 , 3.7479
5.8384495 , 5.7415
14.121696 , 13.9818
13.097677 , 13.3039
17.895126 , 17.6546
15.9883995 , 15.9355
5.337208 , 5.138867
13.400036 , 13.4273
21.508488 , 20.7904
20.448391 , 21.1846
33.411472 , 33.2427
17.296976 , 17.2465
8.243199 , 8.5085
33.688198 , 33.858
13.345491 , 13.1204
5.1026783 , 5.0554
4.747362 , 4.7286
3.9441261 , 3.9221
4.5615106 , 4.3401
18.668724 , 19.1346
14.722916 , 14.646
10.999209 , 11.1968
9.68095 , 10.0179
3.787983 , 3.811
3.9598536 , 3.9217
20.593405 , 20.0715
26.990913 , 25.4688
4.118862 , 4.0845
167.84567 , 159.5508
5.268126 , 5.2938
38.767597 , 38.2688
3.9573565 , 3.9887
13.0640135 , 13.2176
13.04203 , 12.9883
14.716948 , 14.6349
14.221498 , 14.8823
32.86274 , 32.7824
181.29987 , 185.9658
3.6345854 , 3.6577
22.074852 , 20.9343
5.3529587 , 5.4143
3.6079588 , 3.6034
4.7527256 , 4.7552
15.882498 , 16.0655
1.1530027 , 1.1839
17.307358 , 17.3496
40.97607 , 40.0313
0.82899666 , 1.028
32.016483 , 32.13
3.5637531 , 3.598
20.965115 , 20.7537
5.705175 , 5.7583
5.2880745 , 5.297
24.95774 , 24.9661
15.242529 , 15.8142
29.72821 , 30.1171
19.110655 , 19.4228
1.2388067 , 1.2435
10.708142 , 10.4676
28.750118 , 29.9405
3.8553286 , 3.9041
19.879055 , 20.2147
4.240943 , 4.2149
5.745068 , 5.9202
17.166197 , 18.042
5.921739 , 5.9257
26.123123 , 26.6889
5.9083343 , 5.818
3.612794 , 3.583
2.153905 , 2.0435
3.5448751 , 3.6413
4.799416 , 4.8095
1.5669403 , 1.5275
5.439596 , 5.444
17.525743 , 17.2243
8.778944 , 8.4457
16.276823 , 15.9831
3.6690607 , 3.5929
10.464985 , 10.669
11.274467 , 11.2253
42.458763 , 42.7743
16.041498 , 15.9349
4.731795 , 4.7541
4.9083366 , 4.8525
5.0075283 , 4.9893
18.95889 , 19.188
18.583105 , 18.7201
7.556171 , 7.7137
1.6344395 , 1.6374
0.9510431 , 1.1364
3.6540718 , 3.593
3.5050263 , 3.4772
3.1951628 , 3.1174
8.595001 , 8.4299
27.909966 , 28.2203
15.966301 , 15.9364
33.189423 , 32.5482
5.7722793 , 5.7481
3.7949662 , 3.7583
7.710618 , 7.9746
3.7949767 , 3.808
19.805061 , 20.1963
1.6451807 , 1.582
3.5033436 , 3.5862
1.7768688 , 1.6627
14.75853 , 15.6243
17.925606 , 17.6415
5.3758826 , 5.3607
5.520511 , 5.6193
16.04133 , 15.9801
26.79295 , 26.9468
4.795169 , 4.7678
8.643918 , 8.7186
5.5781903 , 5.6026
1.3952417 , 1.2285
4.788477 , 4.767
19.770714 , 20.2926
19.28868 , 18.9196
18.518778 , 18.5261
21.64003 , 21.5286
44.201797 , 45.2592
10.681594 , 10.1967
18.286518 , 17.994
9.079739 , 8.876
33.065826 , 33.2383
3.5698 , 3.6294
6.237944 , 6.3913
3.7586918 , 3.7559
13.813382 , 13.5776
5.3695183 , 5.3594
7.988531 , 7.5963
2.1991072 , 2.0283
6.2480526 , 6.2372
1.148879 , 1.1697
4.630652 , 4.5806
17.778309 , 17.7193
20.869427 , 20.4044
28.938473 , 29.3775
5.213419 , 5.2513
8.791423 , 8.6465
3.4361668 , 3.4316
4.2814436 , 4.2436
10.902951 , 11.3429
6.684761 , 6.5059
24.835072 , 26.16
21.837727 , 21.17
13.285619 , 13.2254
7.9061584 , 8.0065
17.479374 , 17.7704
5.153839 , 5.157
17.233173 , 17.2242
5.859699 , 5.933
3.9203215 , 3.9728
16.215525 , 15.7643
13.241778 , 13.3002
4.929806 , 4.8397
24.103615 , 23.6336
25.423887 , 24.4582
5.9936447 , 6.0568
6.31554 , 6.3264
12.092562 , 11.5894
17.936045 , 17.9458
0.97476196 , 1.1276
21.386227 , 21.6313
21.448818 , 21.1964
16.507027 , 16.6909
33.851517 , 33.3104
5.178763 , 5.1416
3.6530294 , 3.6797
1.487606 , 1.5208
5.6193686 , 5.61
42.32704 , 42.3751
6.4204464 , 6.2726
4.9993873 , 4.9962
3.5163183 , 3.5896
4.909771 , 4.9002
4.442996 , 4.4232
9.055199 , 8.8776
17.755884 , 17.7256
4.7187395 , 4.7113
13.277451 , 13.3757
1.6940308 , 1.5273
17.785349 , 17.2053
9.955073 , 9.6095
16.450676 , 16.5798
22.741926 , 23.0231
1.1136837 , 1.1845
5.664898 , 5.6549
6.098811 , 6.1808
20.31417 , 19.5295
17.540207 , 17.2439
6.2280903 , 6.3359
2.2957697 , 2.0631
101.0028 , 97.1956
18.76461 , 18.6978
3.6981416 , 3.7251
13.197586 , 13.4238
37.139214 , 37.8913
10.725549 , 10.6765
3.6741257 , 3.5965
6.7907495 , 6.9825
1.4002752 , 1.2713
14.952077 , 15.6229
33.13817 , 32.6276
5.1761303 , 5.2399
5.5853477 , 5.6015
9.457929 , 9.7778
17.457108 , 17.2838
13.970299 , 13.6881
25.771362 , 26.2809
4.7960644 , 4.7605
9.894187 , 10.0486
4.5776014 , 4.5768
27.979721 , 27.3226
4.2168674 , 4.2342
90.64231 , 88.4225
3.4380722 , 3.5015
22.12799 , 22.635
1.4481392 , 1.5236
1.9162369 , 1.8011
15.260253 , 15.7579
26.470528 , 26.3309
17.12611 , 17.2106
11.167399 , 10.8762
4.4632435 , 4.4846
8.934189 , 8.8887
17.05074 , 17.3025
16.6585 , 16.6395
6.0281043 , 6.0623
5.6729565 , 5.7218
43.409122 , 44.4767
8.75813 , 8.8951
163.91586 , 164.1594
13.047486 , 12.7009
33.758614 , 34.2912
20.628408 , 20.161
4.3804245 , 4.33
16.705914 , 16.6207
4.0728526 , 4.1286
1.1458454 , 1.1377
17.25616 , 17.2386
4.042569 , 4.0272
4.753972 , 4.7371
5.1956244 , 5.2334
4.277317 , 4.217
1.9636402 , 1.7752
4.5583878 , 4.6139
4.596833 , 4.5744
3.5955596 , 3.6243
25.38379 , 24.0468
3.3746777 , 3.3443
1.682785 , 1.7761
6.5115995 , 6.5956
15.800735 , 15.1658
100.35106 , 95.512
20.045025 , 20.0289
31.689339 , 29.4596
5.99664 , 6.0917
5.593494 , 0.213552
3.5697608 , 3.6325
8.371889 , 7.8943
4.5713153 , 4.5361
18.557919 , 17.6282
16.659433 , 16.2007
84.96766 , 85.5391
12.718058 , 12.7113
8.661236 , 8.6262
3.6300507 , 3.6527
4.729414 , 4.7497
17.318645 , 17.3165
1.3094959 , 1.354
4.948476 , 4.9209
3.8012352 , 3.8687
14.421706 , 13.8712
14.069729 , 13.9957
4.876252 , 4.885
8.813231 , 8.7404
18.084974 , 17.8319
5.0825367 , 4.9963
12.928138 , 12.8593
9.100959 , 9.5748
26.139637 , 25.8561
4.790278 , 4.7899
4.933114 , 4.9359
5.11557 , 5.1604
24.891903 , 24.7126
168.52805 , 172.9229
18.843594 , 18.6739
4.959429 , 4.9616
15.298357 , 15.2916
25.850101 , 25.4363
5.996402 , 5.9856
11.605238 , 11.7901
24.046015 , 23.8647
31.598167 , 31.7844
4.243433 , 4.2439
10.206619 , 9.812
6.2154436 , 6.3611
3.888389 , 3.9029
32.752533 , 33.064
5.257949 , 5.3121
8.177241 , 8.4686
18.39438 , 18.6494
6.379743 , 6.3156
5.7088842 , 5.6625
5.6317306 , 5.7033
14.166891 , 13.7303
18.712074 , 18.526
29.899376 , 28.8733
18.856466 , 18.5829
6.9466023 , 6.9271
4.7148924 , 4.6939
5.689051 , 5.6974
14.2470045 , 13.3564
24.148144 , 24.0171
15.233893 , 15.2333
4.0756884 , 4.0866
13.30336 , 12.8924
32.503063 , 33.2903
4.5097136 , 4.5358
6.415414 , 6.1951
6.052132 , 6.087
5.0770435 , 5.0302
15.282965 , 15.2795
4.052334 , 4.0125
5.7701244 , 5.7929
15.991288 , 15.9134
28.217575 , 28.9268
23.978203 , 23.7083
0.911994 , 1.0211
7.7023973 , 10.225809
7.978999 , 7.5755
3.843789 , 3.6602
4.609577 , 4.573
18.606373 , 18.2613
34.45696 , 34.2727
7.84299 , 7.6561
4.729574 , 4.6905
17.94603 , 18.0205
1.3074636 , 1.2774
39.314632 , 38.4588
19.85778 , 20.1887
7.7176914 , 7.7293
7.6393538 , 7.8636
25.137901 , 25.4485
7.565105 , 7.6566
14.100838 , 13.9916
1.7105894 , 1.5385
5.557927 , 5.5006
4.522108 , 4.4465
5.316549 , 5.3804
5.501181 , 5.4636
17.174654 , 17.0911
13.698335 , 13.3795
4.808346 , 4.7813
5.798977 , 5.8241
1.1419716 , 1.2006
4.4596786 , 4.474
6.1871424 , 6.3616
16.582485 , 16.6518
17.989393 , 17.9962
7.975416 , 7.6636
17.317375 , 17.5621
24.850105 , 23.9959
23.851099 , 23.6558
10.132508 , 10.4313
6.1124163 , 6.1757
6.0752563 , 6.19
4.583144 , 4.6309
4.797473 , 4.7485
28.435062 , 28.5731
19.14645 , 19.1709
28.496552 , 28.3912
14.704953 , 14.7919
4.7245674 , 4.7079
5.263893 , 5.225
14.061514 , 13.8059
3.8961468 , 3.9367
16.302792 , 15.8804
3.7667732 , 3.7441
10.829364 , 11.2221
9.527159 , 9.6247
4.1349363 , 4.1785
3.7590084 , 3.7992
22.853138 , 23.5039
8.717884 , 8.7347
1.1910152 , 1.1979
3.1143064 , 3.1444
1.1956863 , 1.0638
5.8128614 , 5.7618
19.285015 , 19.5123
10.991149 , 10.7677
15.983143 , 16.05
3.5304193 , 3.4906
20.754078 , 20.8219
7.4396005 , 7.4981
3.2824144 , 3.273
13.2027235 , 12.9428
24.592241 , 24.3987
11.177117 , 11.1457
31.534054 , 32.2372
8.961048 , 8.6712
5.013707 , 4.998
9.47661 , 9.7739
3.424531 , 3.4835
25.595684 , 26.9335
3.9972568 , 4.0137
78.61354 , 80.0158
21.868315 , 20.7789
4.4724193 , 4.3113
18.840654 , 18.7641
9.362299 , 9.4717
4.380332 , 4.3732
5.851054 , 5.8187
5.156636 , 5.2315
4.696744 , 4.7736
5.9771066 , 5.7862
26.404167 , 26.7105
1.595211 , 1.5459
9.901897 , 9.603
22.191784 , 22.3229
5.85142 , 5.8595
8.694289 , 8.3926
3.5985107 , 3.6034
16.626999 , 16.4681
111.16362 , 107.097
9.625634 , 9.4097
39.520187 , 39.264
10.88324 , 10.8662
20.028988 , 20.2726
3.7644773 , 3.743
12.351666 , 12.0302
18.453104 , 18.3492
14.005613 , 14.031
0.85591507 , 1.0154
14.622056 , 14.5755
19.862448 , 20.0297
19.532696 , 19.5199
32.83654 , 32.9473
4.795452 , 4.8002
11.013403 , 11.2857
13.636448 , 12.8327
18.573532 , 18.6802
18.170006 , 18.0512
14.68695 , 14.3916
5.3523736 , 5.3972
6.151638 , 6.0822
11.582176 , 11.9747
1.1936998 , 1.3874
15.018013 , 15.1034
1.1222563 , 1.1709
9.287558 , 9.3963
1.2612057 , 1.1302
19.46434 , 18.9393
4.7740374 , 4.7746
4.467787 , 4.4135
8.208986 , 8.34
4.711275 , 4.7085
19.229387 , 19.1111
7.107217 , 7.0499
3.4765272 , 3.4335
15.940666 , 15.7953
1.239006 , 1.2836
13.03295 , 13.4325
25.135187 , 26.0689
4.2470512 , 4.254
3.7828555 , 3.8118
16.003641 , 16.0631
5.6896195 , 5.6259
16.625278 , 16.6702
1.2893219 , 1.1345
3.546916 , 3.6303
30.08086 , 29.5302
5.7308664 , 5.9195
3.5192323 , 3.4807
4.1021857 , 4.0888
3.8965235 , 3.8787
3.569448 , 3.6582
15.617262 , 15.8465
5.9958773 , 5.9674
14.714744 , 14.6215
1.9928055 , 1.8036
4.760171 , 4.7339
16.905163 , 17.5576
22.803654 , 23.755
8.298374 , 8.0829
10.96686 , 11.0077
1.3444872 , 1.1713
10.03643 , 9.7861
1.6905642 , 1.8035
27.313343 , 27.7314
4.7640104 , 4.7545
18.693878 , 18.7668
5.2073164 , 5.2399
4.276114 , 4.2414
27.56538 , 26.6323
4.634158 , 4.6416
16.61089 , 16.6838
14.6322975 , 14.6396
11.124021 , 11.2852
111.987465 , 113.6216
8.854393 , 8.5783
3.5209646 , 3.5061
106.171585 , 106.8794
3.2822475 , 3.2825
23.170982 , 23.1038
5.713372 , 5.7507
44.72119 , 44.0536
3.4582233 , 3.4282
15.257618 , 15.2287
3.3687744 , 3.33
7.7518835 , 7.9819
29.685879 , 29.6681
5.522038 , 5.5022
28.353138 , 28.2001
15.222962 , 15.2223
0.87332535 , 1.0172
0.7134304 , 1.762839
25.449234 , 25.0141
3.475893 , 3.5737
6.152942 , 6.0886
3.2359762 , 3.1179
17.067394 , 17.6003
5.6472206 , 5.6015
4.0868564 , 4.0616
37.679405 , 38.4093
4.645829 , 4.6472
5.794713 , 5.8706
111.22437 , 106.0376
20.853342 , 20.7205
15.980248 , 15.9585
RMSE:  0.7498275441761154  MAPE: 0.06778207397944296
5: ground truth total-  170  predicted total -  169
100: ground truth total-  363  predicted total -  363
 more 100: ground truth total -  8  predicted total -  8



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
3.2706223 , 0.216829
5.3994255 , 3.454315
4.452136 , 0.216082
1.7831354 , 0.217022
5.538695 , 5.183322
3.9824967 , 1.7481
5.6375065 , 6.911863
3.3841362 , 3.466406
4.22055 , 3.462093
1.5190372 , 1.739144
3.146081 , 1.749487
4.3598046 , 5.192379
1.9672155 , 1.736673
2.4973078 , 1.734035
2.4323292 , 0.217044
0.804863 , 0.217654
5.7141504 , 8.642148
2.735362 , 3.46981
1.2530432 , 0.217428
RMSE:  1.7710541600182304  MAPE: 3.3219649104088433
