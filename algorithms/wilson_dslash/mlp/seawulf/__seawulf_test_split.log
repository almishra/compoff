['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1384
19
<class 'numpy.dtype'> float64
1384
train batches:  42  validate samples: 110  test samples: 276
Epoch [0/150], Batch loss: 11.131
Epoch: 0 RMSE:  21.382066122994345  MAPE: 5.653057129119934  L2+L1 loss: 6.708
Epoch [1/150], Batch loss: 8.103
Epoch: 1 RMSE:  4.6132124801357195  MAPE: 0.6011497687410523  L2+L1 loss: 3.045
Epoch [2/150], Batch loss: 3.432
Epoch: 2 RMSE:  7.296914429158739  MAPE: 0.35500076076355663  L2+L1 loss: 2.653
Epoch [3/150], Batch loss: 3.581
Epoch: 3 RMSE:  4.691814998208401  MAPE: 0.297680914855563  L2+L1 loss: 1.93
Epoch [4/150], Batch loss: 2.824
Epoch: 4 RMSE:  2.337604100542129  MAPE: 0.2522997726846632  L2+L1 loss: 1.28
Epoch [5/150], Batch loss: 1.819
Epoch: 5 RMSE:  2.4378483634626416  MAPE: 0.236184887839055  L2+L1 loss: 1.267
Epoch [6/150], Batch loss: 4.801
Epoch: 6 RMSE:  3.754327465069574  MAPE: 0.35866550631356303  L2+L1 loss: 1.609
Epoch [7/150], Batch loss: 2.992
Epoch: 7 RMSE:  3.1717915814939723  MAPE: 0.41770096173220755  L2+L1 loss: 1.741
Epoch [8/150], Batch loss: 1.9
Epoch: 8 RMSE:  1.3722628811692548  MAPE: 0.30513085484361674  L2+L1 loss: 1.099
Epoch [9/150], Batch loss: 6.963
Epoch: 9 RMSE:  37.57114797391704  MAPE: 10.248980866972175  L2+L1 loss: 9.093
Epoch [10/150], Batch loss: 8.311
Epoch: 10 RMSE:  4.704931341338501  MAPE: 0.9206707589982365  L2+L1 loss: 3.289
Epoch [11/150], Batch loss: 3.267
Epoch: 11 RMSE:  2.4690361584871177  MAPE: 0.38847100154101694  L2+L1 loss: 1.441
Epoch [12/150], Batch loss: 2.54
Epoch: 12 RMSE:  1.7687612496278007  MAPE: 0.2543560047509611  L2+L1 loss: 1.199
Epoch [13/150], Batch loss: 2.369
Epoch: 13 RMSE:  2.7680385969401264  MAPE: 0.24546731576686773  L2+L1 loss: 1.39
Epoch [14/150], Batch loss: 2.938
Epoch: 14 RMSE:  45.18878073437593  MAPE: 11.729090508153748  L2+L1 loss: 10.094
Epoch [15/150], Batch loss: 5.031
Epoch: 15 RMSE:  2.02293035958979  MAPE: 0.32463611523777786  L2+L1 loss: 1.402
Epoch [16/150], Batch loss: 4.427
Epoch: 16 RMSE:  6.246801240574864  MAPE: 0.4428957503869509  L2+L1 loss: 2.344
Epoch [17/150], Batch loss: 2.734
Epoch: 17 RMSE:  3.1620721241482435  MAPE: 0.48915041930821535  L2+L1 loss: 1.764
Epoch [18/150], Batch loss: 2.35
Epoch: 18 RMSE:  2.9562672709239224  MAPE: 0.30152762611378975  L2+L1 loss: 1.485
Epoch [19/150], Batch loss: 2.103
Epoch: 19 RMSE:  1.3653690129087122  MAPE: 0.24052886998994633  L2+L1 loss: 1.006
Epoch [20/150], Batch loss: 1.378
Epoch: 20 RMSE:  0.8251912089905308  MAPE: 0.1948649468954738  L2+L1 loss: 0.711
Epoch [21/150], Batch loss: 1.24
Epoch: 21 RMSE:  0.600158511242545  MAPE: 0.1837242726560262  L2+L1 loss: 0.633
Epoch [22/150], Batch loss: 0.93
Epoch: 22 RMSE:  0.5837743187069168  MAPE: 0.19217128367244757  L2+L1 loss: 0.675
Epoch [23/150], Batch loss: 2.792
Epoch: 23 RMSE:  4.117498613745909  MAPE: 0.2089252176409256  L2+L1 loss: 1.247
Epoch [24/150], Batch loss: 4.501
Epoch: 24 RMSE:  9.167756711919596  MAPE: 2.3055609214406405  L2+L1 loss: 3.103
Epoch [25/150], Batch loss: 3.323
Epoch: 25 RMSE:  0.9521803350502697  MAPE: 0.3149899979235312  L2+L1 loss: 1.067
Epoch [26/150], Batch loss: 1.954
Epoch: 26 RMSE:  1.4279553702730226  MAPE: 0.24924920149537105  L2+L1 loss: 0.986
Epoch [27/150], Batch loss: 1.463
Epoch: 27 RMSE:  3.8467267760926234  MAPE: 0.2698680926305382  L2+L1 loss: 1.584
Epoch [28/150], Batch loss: 1.451
Epoch: 28 RMSE:  0.8183405542560509  MAPE: 0.19691303902325438  L2+L1 loss: 0.756
Epoch [29/150], Batch loss: 2.405
Epoch: 29 RMSE:  5.551599514729182  MAPE: 0.4464432550429473  L2+L1 loss: 2.555
Epoch [30/150], Batch loss: 7.181
Epoch: 30 RMSE:  8.82531316513444  MAPE: 3.4621151697275097  L2+L1 loss: 8.805
Epoch [31/150], Batch loss: 9.757
Epoch: 31 RMSE:  9.367715429438046  MAPE: 1.0095285663015143  L2+L1 loss: 3.749
Epoch [32/150], Batch loss: 7.817
Epoch: 32 RMSE:  8.75072085788811  MAPE: 0.7979391506668642  L2+L1 loss: 3.031
Epoch [33/150], Batch loss: 6.59
Epoch: 33 RMSE:  9.037922289002466  MAPE: 0.6426229485065444  L2+L1 loss: 2.959
Epoch [34/150], Batch loss: 6.853
Epoch: 34 RMSE:  8.747706703649907  MAPE: 0.6083520314927832  L2+L1 loss: 2.657
Epoch [35/150], Batch loss: 6.715
Epoch: 35 RMSE:  8.533364072694013  MAPE: 0.6812539409396764  L2+L1 loss: 2.513
Epoch [36/150], Batch loss: 5.319
Epoch: 36 RMSE:  5.570550868979228  MAPE: 0.7280519927531106  L2+L1 loss: 2.207
Epoch [37/150], Batch loss: 5.676
Epoch: 37 RMSE:  8.314426586507565  MAPE: 0.6978631911173002  L2+L1 loss: 2.424
Epoch [38/150], Batch loss: 6.472
Epoch: 38 RMSE:  7.4143070844224255  MAPE: 0.8415721322074659  L2+L1 loss: 2.909
Epoch [39/150], Batch loss: 11.973
Epoch: 39 RMSE:  11.3831539088925  MAPE: 3.3956646110272533  L2+L1 loss: 9.371
Epoch [40/150], Batch loss: 10.693
Epoch: 40 RMSE:  11.200309946311613  MAPE: 3.264635684681379  L2+L1 loss: 9.049
Epoch [41/150], Batch loss: 10.399
Epoch: 41 RMSE:  10.9459705225612  MAPE: 3.086316175360681  L2+L1 loss: 8.45
Epoch [42/150], Batch loss: 10.206
Epoch: 42 RMSE:  10.691009996633843  MAPE: 2.899396504775465  L2+L1 loss: 7.88
Epoch [43/150], Batch loss: 9.885
Epoch: 43 RMSE:  10.34372528479094  MAPE: 2.6261812267565823  L2+L1 loss: 7.013
Epoch [44/150], Batch loss: 9.271
Epoch: 44 RMSE:  9.888443316266017  MAPE: 2.200592618701011  L2+L1 loss: 5.775
Epoch [45/150], Batch loss: 8.61
Epoch: 45 RMSE:  9.51009637528034  MAPE: 1.81632090438773  L2+L1 loss: 4.748
Epoch [46/150], Batch loss: 8.33
Epoch: 46 RMSE:  9.26806563439826  MAPE: 1.5497308239290997  L2+L1 loss: 4.114
Epoch [47/150], Batch loss: 8.203
Epoch: 47 RMSE:  9.162673007480366  MAPE: 1.3788654104935822  L2+L1 loss: 3.773
Epoch [48/150], Batch loss: 7.175
Epoch: 48 RMSE:  8.638019105536657  MAPE: 0.6702219905889693  L2+L1 loss: 2.605
Epoch [49/150], Batch loss: 6.757
Epoch: 49 RMSE:  8.581337348351957  MAPE: 0.5708382508250538  L2+L1 loss: 2.429
Epoch [50/150], Batch loss: 6.398
Epoch: 50 RMSE:  8.558663215803474  MAPE: 0.5802506274020428  L2+L1 loss: 2.344
Epoch [51/150], Batch loss: 6.595
Epoch: 51 RMSE:  8.526757372132574  MAPE: 0.5556549691315318  L2+L1 loss: 2.297
Epoch [52/150], Batch loss: 6.682
Epoch: 52 RMSE:  8.503543009457504  MAPE: 0.5717964432990794  L2+L1 loss: 2.349
Epoch [53/150], Batch loss: 6.384
Epoch: 53 RMSE:  8.476025594208778  MAPE: 0.5586079828592974  L2+L1 loss: 2.218
Epoch [54/150], Batch loss: 6.306
Epoch: 54 RMSE:  8.447768540990813  MAPE: 0.549255714244096  L2+L1 loss: 2.202
Epoch [55/150], Batch loss: 6.619
Epoch: 55 RMSE:  8.42257146478705  MAPE: 0.5496296770956904  L2+L1 loss: 2.109
Epoch [56/150], Batch loss: 6.335
Epoch: 56 RMSE:  8.394319559450423  MAPE: 0.5344055608075531  L2+L1 loss: 2.041
Epoch [57/150], Batch loss: 6.414
Epoch: 57 RMSE:  8.372658242810477  MAPE: 0.5469889274339408  L2+L1 loss: 2.117
Epoch [58/150], Batch loss: 6.186
Epoch: 58 RMSE:  8.349579012837387  MAPE: 0.5655440085262484  L2+L1 loss: 2.159
Epoch [59/150], Batch loss: 6.327
Epoch: 59 RMSE:  8.32265548046198  MAPE: 0.5609534051975676  L2+L1 loss: 2.085
Epoch [60/150], Batch loss: 6.047
Epoch: 60 RMSE:  8.300153161834746  MAPE: 0.5602315416350179  L2+L1 loss: 2.048
Epoch [61/150], Batch loss: 6.333
Epoch: 61 RMSE:  8.278461975301138  MAPE: 0.5856443571519427  L2+L1 loss: 2.08
Epoch [62/150], Batch loss: 6.293
Epoch: 62 RMSE:  8.254104634787417  MAPE: 0.5541987385612329  L2+L1 loss: 1.954
Epoch [63/150], Batch loss: 6.222
Epoch: 63 RMSE:  8.23493611879067  MAPE: 0.5915109575700239  L2+L1 loss: 2.071
Epoch [64/150], Batch loss: 6.391
Epoch: 64 RMSE:  8.222356745024591  MAPE: 0.6554213134546764  L2+L1 loss: 2.269
Epoch [65/150], Batch loss: 6.376
Epoch: 65 RMSE:  8.192734585745939  MAPE: 0.6310950019724566  L2+L1 loss: 2.148
Epoch [66/150], Batch loss: 5.976
Epoch: 66 RMSE:  8.166018371358922  MAPE: 0.5906974099112509  L2+L1 loss: 1.962
Epoch [67/150], Batch loss: 6.231
Epoch: 67 RMSE:  8.146186246831737  MAPE: 0.5968973899470978  L2+L1 loss: 1.967
Epoch [68/150], Batch loss: 6.189
Epoch: 68 RMSE:  8.126672241660065  MAPE: 0.6225847118837986  L2+L1 loss: 2.017
Epoch [69/150], Batch loss: 5.761
Epoch: 69 RMSE:  8.106448885733673  MAPE: 0.6333704201847596  L2+L1 loss: 2.011
Epoch [70/150], Batch loss: 6.036
Epoch: 70 RMSE:  8.0856375784757  MAPE: 0.6173263061003925  L2+L1 loss: 1.965
Epoch [71/150], Batch loss: 6.202
Epoch: 71 RMSE:  8.065252214848217  MAPE: 0.6220828528414888  L2+L1 loss: 1.924
Epoch [72/150], Batch loss: 5.772
Epoch: 72 RMSE:  8.046655887053976  MAPE: 0.6284750676479771  L2+L1 loss: 1.924
Epoch [73/150], Batch loss: 5.736
Epoch: 73 RMSE:  6.234758378520095  MAPE: 2.0220002814233737  L2+L1 loss: 5.144
Epoch [74/150], Batch loss: 4.402
Epoch: 74 RMSE:  3.3311251770455663  MAPE: 0.7187850109750864  L2+L1 loss: 2.802
Epoch [75/150], Batch loss: 2.854
Epoch: 75 RMSE:  2.61771183391333  MAPE: 0.625199876682748  L2+L1 loss: 2.303
Epoch [76/150], Batch loss: 2.822
Epoch: 76 RMSE:  3.8664585308014856  MAPE: 0.693975514757235  L2+L1 loss: 2.919
Epoch [77/150], Batch loss: 2.639
Epoch: 77 RMSE:  2.9626963374090622  MAPE: 0.7139892217557107  L2+L1 loss: 2.357
Epoch [78/150], Batch loss: 2.246
Epoch: 78 RMSE:  2.105915887602955  MAPE: 0.44101665978828575  L2+L1 loss: 1.174
Epoch [79/150], Batch loss: 1.956
Epoch: 79 RMSE:  3.6219455990408616  MAPE: 1.2152189511205869  L2+L1 loss: 3.173
Epoch [80/150], Batch loss: 2.056
Epoch: 80 RMSE:  1.337776664949813  MAPE: 0.11373612129369584  L2+L1 loss: 0.786
Epoch [81/150], Batch loss: 1.443
Epoch: 81 RMSE:  1.3126892107784012  MAPE: 0.10850500592073065  L2+L1 loss: 0.742
Epoch [82/150], Batch loss: 1.345
Epoch: 82 RMSE:  1.4442634104977172  MAPE: 0.12055475133567861  L2+L1 loss: 0.845
Epoch [83/150], Batch loss: 1.33
Epoch: 83 RMSE:  1.2386017331640502  MAPE: 0.10999443806603573  L2+L1 loss: 0.717
Epoch [84/150], Batch loss: 1.323
Epoch: 84 RMSE:  1.173226211542136  MAPE: 0.1286225637839238  L2+L1 loss: 0.807
Epoch [85/150], Batch loss: 1.227
Epoch: 85 RMSE:  1.363834674968665  MAPE: 0.15817051536356344  L2+L1 loss: 0.978
Epoch [86/150], Batch loss: 1.208
Epoch: 86 RMSE:  1.1514908744468697  MAPE: 0.10483237748624774  L2+L1 loss: 0.729
Epoch [87/150], Batch loss: 1.131
Epoch: 87 RMSE:  0.9836981333260425  MAPE: 0.09670334020606304  L2+L1 loss: 0.669
Epoch [88/150], Batch loss: 1.077
Epoch: 88 RMSE:  0.9547808030293027  MAPE: 0.09721866409304393  L2+L1 loss: 0.69
Epoch [89/150], Batch loss: 1.033
Epoch: 89 RMSE:  0.7960773406415974  MAPE: 0.10533967574363345  L2+L1 loss: 0.657
Epoch [90/150], Batch loss: 1.032
Epoch: 90 RMSE:  0.9171125171763711  MAPE: 0.08802337005084193  L2+L1 loss: 0.632
Epoch [91/150], Batch loss: 1.005
Epoch: 91 RMSE:  1.1140216138979084  MAPE: 0.13371882608661467  L2+L1 loss: 0.804
Epoch [92/150], Batch loss: 0.95
Epoch: 92 RMSE:  0.6103030392893611  MAPE: 0.09277990350946265  L2+L1 loss: 0.574
Epoch [93/150], Batch loss: 0.926
Epoch: 93 RMSE:  0.7244975259685944  MAPE: 0.11111889326334357  L2+L1 loss: 0.683
Epoch [94/150], Batch loss: 0.781
Epoch: 94 RMSE:  0.6430891484903867  MAPE: 0.11786607920233237  L2+L1 loss: 0.696
Epoch [95/150], Batch loss: 0.863
Epoch: 95 RMSE:  0.5510578186845603  MAPE: 0.1301027906345666  L2+L1 loss: 0.593
Epoch [96/150], Batch loss: 0.79
Epoch: 96 RMSE:  0.5201563341326564  MAPE: 0.12032245084765834  L2+L1 loss: 0.659
Epoch [97/150], Batch loss: 0.775
Epoch: 97 RMSE:  1.0780205520723576  MAPE: 0.09975915851502988  L2+L1 loss: 0.706
Epoch [98/150], Batch loss: 0.728
Epoch: 98 RMSE:  0.5095240505159028  MAPE: 0.12095229833311996  L2+L1 loss: 0.639
Epoch [99/150], Batch loss: 0.714
Epoch: 99 RMSE:  0.5934571817438405  MAPE: 0.12361704491487575  L2+L1 loss: 0.566
Epoch [100/150], Batch loss: 0.654
Epoch: 100 RMSE:  0.5825990595911481  MAPE: 0.08281258194677135  L2+L1 loss: 0.541
Epoch [101/150], Batch loss: 0.59
Epoch: 101 RMSE:  0.381535925399225  MAPE: 0.12221168902426925  L2+L1 loss: 0.511
Epoch [102/150], Batch loss: 0.611
Epoch: 102 RMSE:  0.3458559806638038  MAPE: 0.07752980199583505  L2+L1 loss: 0.48
Epoch [103/150], Batch loss: 0.565
Epoch: 103 RMSE:  0.5022009064620013  MAPE: 0.07665656509315177  L2+L1 loss: 0.513
Epoch [104/150], Batch loss: 0.557
Epoch: 104 RMSE:  0.53871277385774  MAPE: 0.12269251762727962  L2+L1 loss: 0.664
Epoch [105/150], Batch loss: 0.484
Epoch: 105 RMSE:  0.286680872404413  MAPE: 0.07165202560268119  L2+L1 loss: 0.461
Epoch [106/150], Batch loss: 0.584
Epoch: 106 RMSE:  0.5620428101394496  MAPE: 0.09294955646834079  L2+L1 loss: 0.617
Epoch [107/150], Batch loss: 0.529
Epoch: 107 RMSE:  0.3333033688954479  MAPE: 0.08824458652213718  L2+L1 loss: 0.495
Epoch [108/150], Batch loss: 0.474
Epoch: 108 RMSE:  0.40718777395225925  MAPE: 0.11790687895340152  L2+L1 loss: 0.624
Epoch [109/150], Batch loss: 0.483
Epoch: 109 RMSE:  0.4230083490894555  MAPE: 0.08680628870749596  L2+L1 loss: 0.531
Epoch [110/150], Batch loss: 0.486
Epoch: 110 RMSE:  0.3255057637708025  MAPE: 0.08618817616066939  L2+L1 loss: 0.469
Epoch [111/150], Batch loss: 0.464
Epoch: 111 RMSE:  0.4810561224374133  MAPE: 0.10324426575899687  L2+L1 loss: 0.6
Epoch [112/150], Batch loss: 0.494
Epoch: 112 RMSE:  0.3665205654315744  MAPE: 0.10962067032705698  L2+L1 loss: 0.587
Epoch [113/150], Batch loss: 0.44
Epoch: 113 RMSE:  0.29433178796447645  MAPE: 0.08231789581864389  L2+L1 loss: 0.464
Epoch [114/150], Batch loss: 0.396
Epoch: 114 RMSE:  0.28892205405116667  MAPE: 0.0700204013324754  L2+L1 loss: 0.438
Epoch [115/150], Batch loss: 0.433
Epoch: 115 RMSE:  0.39717338527639695  MAPE: 0.12971308688710925  L2+L1 loss: 0.66
Epoch [116/150], Batch loss: 0.428
Epoch: 116 RMSE:  0.37588349733292353  MAPE: 0.09794971474351916  L2+L1 loss: 0.473
Epoch [117/150], Batch loss: 0.5
Epoch: 117 RMSE:  0.26932335997683815  MAPE: 0.06764045112137253  L2+L1 loss: 0.422
Epoch [118/150], Batch loss: 0.397
Epoch: 118 RMSE:  0.3101071126534492  MAPE: 0.09289283566575861  L2+L1 loss: 0.496
Epoch [119/150], Batch loss: 0.371
Epoch: 119 RMSE:  0.3191601186535791  MAPE: 0.09250912913064781  L2+L1 loss: 0.515
Epoch [120/150], Batch loss: 0.364
Epoch: 120 RMSE:  0.24574552376916942  MAPE: 0.07133331258102714  L2+L1 loss: 0.414
Epoch [121/150], Batch loss: 0.335
Epoch: 121 RMSE:  0.25793911058238606  MAPE: 0.07133819339706456  L2+L1 loss: 0.419
Epoch [122/150], Batch loss: 0.325
Epoch: 122 RMSE:  0.2488464588761915  MAPE: 0.07000761806428712  L2+L1 loss: 0.417
Epoch [123/150], Batch loss: 0.317
Epoch: 123 RMSE:  0.25111807866680336  MAPE: 0.07147276623999965  L2+L1 loss: 0.416
Epoch [124/150], Batch loss: 0.317
Epoch: 124 RMSE:  0.274423348909026  MAPE: 0.07680097571083029  L2+L1 loss: 0.446
Epoch [125/150], Batch loss: 0.33
Epoch: 125 RMSE:  0.24433600905571284  MAPE: 0.06958266367265736  L2+L1 loss: 0.409
Epoch [126/150], Batch loss: 0.324
Epoch: 126 RMSE:  0.25574381160099074  MAPE: 0.07360675375344732  L2+L1 loss: 0.422
Epoch [127/150], Batch loss: 0.322
Epoch: 127 RMSE:  0.2453831370901847  MAPE: 0.07062936654525358  L2+L1 loss: 0.411
Epoch [128/150], Batch loss: 0.326
Epoch: 128 RMSE:  0.2692383696242188  MAPE: 0.0735199817845559  L2+L1 loss: 0.441
Epoch [129/150], Batch loss: 0.325
Epoch: 129 RMSE:  0.24530190535724897  MAPE: 0.07010093598113384  L2+L1 loss: 0.418
Epoch [130/150], Batch loss: 0.312
Epoch: 130 RMSE:  0.24640755578508977  MAPE: 0.07083476056082266  L2+L1 loss: 0.417
Epoch [131/150], Batch loss: 0.319
Epoch: 131 RMSE:  0.24964446084750802  MAPE: 0.0673030896600983  L2+L1 loss: 0.414
Epoch [132/150], Batch loss: 0.324
Epoch: 132 RMSE:  0.24325556946560206  MAPE: 0.06919611050436723  L2+L1 loss: 0.408
Epoch [133/150], Batch loss: 0.324
Epoch: 133 RMSE:  0.2433393276463141  MAPE: 0.07035441797482407  L2+L1 loss: 0.41
Epoch [134/150], Batch loss: 0.318
Epoch: 134 RMSE:  0.2388643467759444  MAPE: 0.06950942712159631  L2+L1 loss: 0.406
Epoch [135/150], Batch loss: 0.323
Epoch: 135 RMSE:  0.24375991087322474  MAPE: 0.06911377257291282  L2+L1 loss: 0.411
Epoch [136/150], Batch loss: 0.304
Epoch: 136 RMSE:  0.2701751697417594  MAPE: 0.0737502909667241  L2+L1 loss: 0.434
Epoch [137/150], Batch loss: 0.312
Epoch: 137 RMSE:  0.24194166726874072  MAPE: 0.07023209617786544  L2+L1 loss: 0.403
Epoch [138/150], Batch loss: 0.306
Epoch: 138 RMSE:  0.2537578041644504  MAPE: 0.07241687749443618  L2+L1 loss: 0.425
Epoch [139/150], Batch loss: 0.318
Epoch: 139 RMSE:  0.2368524690855714  MAPE: 0.0686246384951161  L2+L1 loss: 0.404
Epoch [140/150], Batch loss: 0.315
Epoch: 140 RMSE:  0.24049375306532414  MAPE: 0.06750925142409052  L2+L1 loss: 0.404
Epoch [141/150], Batch loss: 0.312
Epoch: 141 RMSE:  0.24583373298591935  MAPE: 0.06892217497697108  L2+L1 loss: 0.409
Epoch [142/150], Batch loss: 0.315
Epoch: 142 RMSE:  0.2404116604368384  MAPE: 0.06699503006104152  L2+L1 loss: 0.407
Epoch [143/150], Batch loss: 0.324
Epoch: 143 RMSE:  0.24359117804479638  MAPE: 0.07619006735980528  L2+L1 loss: 0.412
Epoch [144/150], Batch loss: 0.317
Epoch: 144 RMSE:  0.24617115546631754  MAPE: 0.06856777636380809  L2+L1 loss: 0.409
Epoch [145/150], Batch loss: 0.309
Epoch: 145 RMSE:  0.24494474760557378  MAPE: 0.07037561362871507  L2+L1 loss: 0.415
Epoch [146/150], Batch loss: 0.31
Epoch: 146 RMSE:  0.2342441077468941  MAPE: 0.06825378045588452  L2+L1 loss: 0.404
Epoch [147/150], Batch loss: 0.303
Epoch: 147 RMSE:  0.24209898531423776  MAPE: 0.06470735612974994  L2+L1 loss: 0.408
Epoch [148/150], Batch loss: 0.306
Epoch: 148 RMSE:  0.2421205910703494  MAPE: 0.06926334810082552  L2+L1 loss: 0.409
Epoch [149/150], Batch loss: 0.319
Epoch: 149 RMSE:  0.2475123243110277  MAPE: 0.07253604209820884  L2+L1 loss: 0.434


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
2.3490467 , 2.1975
0.8714533 , 1.49456
4.278307 , 4.2378
3.6852264 , 3.1544
4.698004 , 4.3969
3.801423 , 3.9608
7.547186 , 7.2682
4.280261 , 4.176
3.230259 , 3.1404
3.2054615 , 3.0976
2.9828644 , 2.9382
2.9834595 , 2.8236
3.457161 , 3.3888
3.4585552 , 3.4539
1.1746311 , 1.3411
2.7595425 , 2.4946
2.8288212 , 2.5904
3.3847313 , 3.1845
3.0762873 , 3.0608
5.151333 , 5.4659
2.7443562 , 2.5075
6.156784 , 5.7248
13.654556 , 12.5272
59.961323 , 57.3407
3.4325733 , 3.4587
3.5982876 , 3.6094
3.3594885 , 3.2649
2.7667112 , 2.5934
5.194708 , 4.9293
6.818654 , 6.9187
1.7615051 , 1.2506
2.974949 , 2.8963
2.8961344 , 2.8334
2.1771317 , 2.2238
3.3120537 , 3.2483
6.6398306 , 6.6084
2.1066895 , 2.2953
7.666708 , 7.9313
1.1583223 , 1.0468
3.2364082 , 3.0435
1.3356895 , 1.0482
3.363452 , 3.2639
6.5647163 , 6.8166
3.5966816 , 3.3272
3.415658 , 3.4988
3.745553 , 3.7695
74.964966 , 76.3995
4.89464 , 4.6992
4.045478 , 3.7433
3.3402433 , 3.1712
2.4346256 , 2.4061
2.6694927 , 2.4765
2.8821716 , 2.6768
2.4423542 , 2.3918
1.5298424 , 1.5405
27.987793 , 28.158
3.669016 , 3.5875
1.6702623 , 1.5844
5.972167 , 6.0137
5.374133 , 5.6513
13.052607 , 12.0012
7.905451 , 7.8497
4.0546265 , 4.1098
2.8550453 , 2.6601
2.1038008 , 2.0664
3.8888988 , 3.8664
3.8620882 , 3.7838
7.7381268 , 6.71
2.5137653 , 2.203
2.1338587 , 1.966
3.360406 , 3.1623
9.268909 , 9.4378
0.8166828 , 1.0358
2.9357119 , 2.919
31.466223 , 31.738
3.2204285 , 3.2034
3.338006 , 3.164
1.9763203 , 2.0109
1.999691 , 2.0046
35.126602 , 34.6423
2.9523478 , 2.6181
4.29443 , 4.2846
2.800848 , 2.6892
2.2447062 , 2.0967
3.0385609 , 3.0113
3.4910889 , 3.4979
1.7919817 , 1.8405
1.821064 , 1.5944
5.7084045 , 5.675
5.4389067 , 5.2948
3.1459055 , 2.9934
3.9990873 , 3.9809
7.940077 , 7.7079
2.8607464 , 2.9225
1.8517122 , 1.5666
9.716785 , 9.2418
7.15026 , 7.1969
4.419012 , 4.4982
3.6609583 , 3.5838
1.8784361 , 1.8986
7.5090227 , 7.6486
1.1253996 , 1.0699
2.2527637 , 2.2104
2.8265276 , 2.886
1.5815372 , 1.6163
2.812954 , 2.7642
4.5637045 , 4.5595
3.4862366 , 3.3952
2.1773167 , 1.6349
6.2256193 , 6.1488
5.671584 , 5.6189
1.0455666 , 1.2992
5.1752625 , 4.8817
2.0716848 , 2.0935
1.9198647 , 1.4698
1.4383821 , 1.2334
1.133359 , 1.1403
24.806463 , 25.8916
2.9581804 , 2.8367
3.3207808 , 3.3522
2.4528332 , 2.3525
3.4780588 , 3.4315
3.3236504 , 3.2589
3.527522 , 3.5579
1.9870911 , 1.6599
3.5871696 , 3.5168
3.096014 , 2.9464
3.271079 , 3.161
1.6414995 , 1.6074
3.75132 , 3.7853
1.6121559 , 1.5812
67.11185 , 66.1756
1.3669224 , 1.3644
2.8421688 , 2.8398
2.248558 , 2.2925
2.9194927 , 2.8426
23.189991 , 23.555
2.8530693 , 2.7762
2.5449657 , 2.3871
4.200227 , 4.2336
3.605752 , 3.4625
6.1048975 , 5.847
3.5088863 , 3.4413
4.057144 , 3.9805
5.3401794 , 5.4704
8.604242 , 8.6721
4.3023653 , 4.5385
2.5592785 , 2.474
2.3640156 , 2.3774
3.4403028 , 3.3524
3.5417585 , 3.5902
2.9943504 , 2.9754
1.8128986 , 1.8762
4.065114 , 3.7726
3.116331 , 3.1001
3.0356388 , 2.9379
3.6288462 , 3.4343
3.5807972 , 3.4431
2.148901 , 2.0667
3.9312954 , 3.7749
3.907384 , 3.7994
1.8828526 , 1.7083
2.699669 , 2.4751
4.026639 , 4.2007
0.74915886 , 1.1002
3.8466072 , 3.7058
11.417025 , 11.175
4.1215487 , 4.1494
4.2602444 , 3.9217
1.661005 , 1.5978
2.7298985 , 2.5855
1.6235685 , 1.2647
3.2265139 , 3.1946
2.1903324 , 2.1816
3.1952066 , 3.1368
1.9926224 , 2.126
5.637969 , 5.956
10.037878 , 8.4028
8.948431 , 8.3841
0.0 , 0.023798
5.214239 , 5.2423
1.4450932 , 1.3177
6.4417353 , 6.0139
1.4298334 , 1.2819
2.5360775 , 2.4126
1.2251587 , 1.148
2.308508 , 2.3349
3.19802 , 3.1196
2.7545109 , 2.7823
1.7650766 , 1.6397
3.034172 , 2.9818
2.7351713 , 2.8154
3.5971556 , 3.523
3.4461088 , 3.3901
1.3821774 , 1.1919
2.2679253 , 2.2032
3.7282562 , 3.7232
3.2334213 , 3.1645
1.7634811 , 1.8773
3.262436 , 3.096
3.3433514 , 3.3493
3.0502186 , 2.9059
1.9083443 , 1.8864
2.8326416 , 2.6613
1.7974949 , 1.7252
2.631054 , 2.5572
3.7840137 , 3.5395
3.155201 , 3.1007
2.5437956 , 2.3715
5.8865175 , 5.786
3.5842094 , 3.5017
2.6994152 , 2.4797
1.5253334 , 1.3858
70.426575 , 71.2594
34.58659 , 34.133
1.8116798 , 2.047
4.5602484 , 4.4108
5.3307114 , 4.9498
2.8451548 , 2.9289
3.5256147 , 3.3384
4.7291126 , 4.4765
1.8312092 , 1.9816
3.2042532 , 3.1733
7.5611076 , 7.6696
3.3516512 , 3.2628
2.2733707 , 2.3553
1.9229116 , 1.9107
5.4766855 , 5.5146
4.69623 , 5.2916
2.868473 , 2.822
3.651987 , 3.4187
2.7244635 , 2.6335
3.4109173 , 3.2774
2.5943012 , 2.3155
7.382965 , 6.6155
21.595177 , 21.2415
2.8008595 , 2.6665
3.5618458 , 3.62
8.045872 , 8.0493
3.4823198 , 3.2447
2.4118872 , 2.21
3.0468864 , 2.7062
3.2040224 , 3.145
6.914488 , 7.1509
1.9788876 , 1.9599
2.9399624 , 2.868
2.443386 , 2.2327
4.5221176 , 4.6327
2.8264675 , 2.8927
1.3087816 , 1.5077
1.3682661 , 1.2484
2.9501944 , 2.8269
6.5751534 , 6.6888
4.378334 , 4.4329
5.003723 , 4.7769
27.991663 , 28.8316
2.9035873 , 2.8099
1.4210472 , 1.4988
5.6163836 , 5.3486
4.6122293 , 4.3472
5.7337837 , 5.6456
2.0596714 , 1.8713
1.6787205 , 2.0272
3.8860674 , 3.8957
3.1164722 , 3.0188
2.8094673 , 2.7536
4.2146883 , 4.1423
1.7516365 , 1.8708
1.1423607 , 1.1977
4.0506325 , 4.0572
4.769656 , 4.4627
1.7777462 , 1.6035
2.7469835 , 2.6698
1.7184982 , 1.2801
1.8716879 , 1.8912
2.1299562 , 2.1452
RMSE:  0.32132035232464784  MAPE: 0.05614950647079244
5: ground truth total-  223  predicted total -  223
100: ground truth total-  53  predicted total -  53
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
0.82905674 , 0.498513
0.81046486 , 0.666807
0.720808 , 0.34304
0.94556046 , 1.460194
0.92569923 , 0.311075
0.45676994 , 0.095839
0.8679943 , 0.256924
1.149004 , 1.164927
0.7997532 , 0.223399
0.9289837 , 1.61217
0.93673897 , 0.680592
1.0202665 , 0.830288
0.7162237 , 0.184151
1.0102539 , 1.324008
0.90011597 , 1.296402
0.8629122 , 0.983502
0.36648655 , 0.184135
0.4267273 , 0.255276
0.60853386 , 0.128085
0.44962788 , 0.276014
1.0937748 , 1.645305
1.0885172 , 0.991123
0.9025574 , 0.978483
0.39963722 , 0.212479
RMSE:  0.3846530156662477  MAPE: 1.0443089545388184
