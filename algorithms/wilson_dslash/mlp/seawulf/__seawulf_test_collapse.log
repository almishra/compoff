['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1364
19
<class 'numpy.dtype'> float64
1364
train batches:  62  validate samples: 109  test samples: 272
Epoch [0/150], Batch loss: 8.889
Epoch: 0 RMSE:  4.44772887311483  MAPE: 1.1553710440369764  L2+L1 loss: 3.645
Epoch [1/150], Batch loss: 8.102
Epoch: 1 RMSE:  4.41069309175773  MAPE: 1.0764180898282592  L2+L1 loss: 3.43
Epoch [2/150], Batch loss: 7.463
Epoch: 2 RMSE:  2.971246734125375  MAPE: 0.5249984720187632  L2+L1 loss: 2.192
Epoch [3/150], Batch loss: 6.804
Epoch: 3 RMSE:  3.3704401395805332  MAPE: 0.7173682870936142  L2+L1 loss: 2.716
Epoch [4/150], Batch loss: 8.521
Epoch: 4 RMSE:  3.3523073182690433  MAPE: 0.6904494440647583  L2+L1 loss: 2.668
Epoch [5/150], Batch loss: 4.594
Epoch: 5 RMSE:  4.381089598730828  MAPE: 0.641670074807784  L2+L1 loss: 3.215
Epoch [6/150], Batch loss: 2.796
Epoch: 6 RMSE:  2.894693531928375  MAPE: 0.4533040832633733  L2+L1 loss: 2.493
Epoch [7/150], Batch loss: 2.587
Epoch: 7 RMSE:  2.2793994462101166  MAPE: 0.30442090440791547  L2+L1 loss: 1.859
Epoch [8/150], Batch loss: 2.213
Epoch: 8 RMSE:  4.544130925084214  MAPE: 0.5016011464386254  L2+L1 loss: 2.779
Epoch [9/150], Batch loss: 3.542
Epoch: 9 RMSE:  2.1516108113594945  MAPE: 0.28444484621954963  L2+L1 loss: 1.411
Epoch [10/150], Batch loss: 6.096
Epoch: 10 RMSE:  2.5527693639034537  MAPE: 0.36841190278615205  L2+L1 loss: 1.699
Epoch [11/150], Batch loss: 3.493
Epoch: 11 RMSE:  1.7674331857374395  MAPE: 0.3377925041770018  L2+L1 loss: 1.524
Epoch [12/150], Batch loss: 3.385
Epoch: 12 RMSE:  3.1997477276291524  MAPE: 0.8733589403109553  L2+L1 loss: 2.658
Epoch [13/150], Batch loss: 4.256
Epoch: 13 RMSE:  1.0869829949164944  MAPE: 0.2761649572368206  L2+L1 loss: 1.218
Epoch [14/150], Batch loss: 6.789
Epoch: 14 RMSE:  4.158483815538564  MAPE: 0.7124596199995953  L2+L1 loss: 2.757
Epoch [15/150], Batch loss: 7.048
Epoch: 15 RMSE:  3.3218797165738523  MAPE: 0.19002136987990448  L2+L1 loss: 1.373
Epoch [16/150], Batch loss: 6.461
Epoch: 16 RMSE:  2.898733135033548  MAPE: 0.18593180607126955  L2+L1 loss: 1.157
Epoch [17/150], Batch loss: 6.184
Epoch: 17 RMSE:  2.6046414172641126  MAPE: 0.2516224731332916  L2+L1 loss: 1.338
Epoch [18/150], Batch loss: 6.341
Epoch: 18 RMSE:  4.094856544037762  MAPE: 0.9353032508248053  L2+L1 loss: 2.934
Epoch [19/150], Batch loss: 8.823
Epoch: 19 RMSE:  11.05218678133717  MAPE: 4.013688705972274  L2+L1 loss: 11.045
Epoch [20/150], Batch loss: 8.301
Epoch: 20 RMSE:  2.860119277138748  MAPE: 0.3801403380718233  L2+L1 loss: 1.796
Epoch [21/150], Batch loss: 6.113
Epoch: 21 RMSE:  2.584561058616348  MAPE: 0.3396483952964607  L2+L1 loss: 1.582
Epoch [22/150], Batch loss: 4.471
Epoch: 22 RMSE:  3.452427879679434  MAPE: 0.5553379724835387  L2+L1 loss: 2.339
Epoch [23/150], Batch loss: 3.141
Epoch: 23 RMSE:  3.3206103272984393  MAPE: 0.39656284240188916  L2+L1 loss: 2.313
Epoch [24/150], Batch loss: 8.823
Epoch: 24 RMSE:  5.918786336413028  MAPE: 2.246650619030015  L2+L1 loss: 6.072
Epoch [25/150], Batch loss: 4.26
Epoch: 25 RMSE:  1.0356009586405703  MAPE: 0.20382767028417778  L2+L1 loss: 1.034
Epoch [26/150], Batch loss: 3.188
Epoch: 26 RMSE:  2.5661004034333903  MAPE: 0.1660034998841463  L2+L1 loss: 1.208
Epoch [27/150], Batch loss: 3.297
Epoch: 27 RMSE:  0.8934451841976168  MAPE: 0.11190261879593102  L2+L1 loss: 0.767
Epoch [28/150], Batch loss: 2.969
Epoch: 28 RMSE:  1.3420233103432422  MAPE: 0.1190282024056385  L2+L1 loss: 0.806
Epoch [29/150], Batch loss: 5.875
Epoch: 29 RMSE:  2.4255447844576468  MAPE: 0.15022283861920657  L2+L1 loss: 1.006
Epoch [30/150], Batch loss: 5.625
Epoch: 30 RMSE:  2.3735368681108575  MAPE: 0.11578590742942783  L2+L1 loss: 0.91
Epoch [31/150], Batch loss: 5.55
Epoch: 31 RMSE:  2.341601206882636  MAPE: 0.105266649309292  L2+L1 loss: 0.876
Epoch [32/150], Batch loss: 5.506
Epoch: 32 RMSE:  2.30883602900671  MAPE: 0.11316612742897805  L2+L1 loss: 0.883
Epoch [33/150], Batch loss: 5.507
Epoch: 33 RMSE:  2.2756223076833235  MAPE: 0.10575244996239011  L2+L1 loss: 0.916
Epoch [34/150], Batch loss: 5.833
Epoch: 34 RMSE:  2.2374041573141854  MAPE: 0.09176593884748022  L2+L1 loss: 0.854
Epoch [35/150], Batch loss: 5.711
Epoch: 35 RMSE:  2.2003075919452693  MAPE: 0.07556655333282176  L2+L1 loss: 0.785
Epoch [36/150], Batch loss: 5.976
Epoch: 36 RMSE:  2.1982101979960267  MAPE: 0.14831732440894507  L2+L1 loss: 1.021
Epoch [37/150], Batch loss: 5.411
Epoch: 37 RMSE:  2.1655045423095722  MAPE: 0.15041734621525446  L2+L1 loss: 1.011
Epoch [38/150], Batch loss: 5.433
Epoch: 38 RMSE:  2.1029454040232296  MAPE: 0.07223267053325845  L2+L1 loss: 0.734
Epoch [39/150], Batch loss: 5.491
Epoch: 39 RMSE:  2.0721157030444846  MAPE: 0.07878807784881212  L2+L1 loss: 0.771
Epoch [40/150], Batch loss: 5.009
Epoch: 40 RMSE:  2.0321108428865595  MAPE: 0.06442913851805218  L2+L1 loss: 0.713
Epoch [41/150], Batch loss: 5.238
Epoch: 41 RMSE:  2.006735215400462  MAPE: 0.09015841962164155  L2+L1 loss: 0.815
Epoch [42/150], Batch loss: 4.851
Epoch: 42 RMSE:  1.9694040195765001  MAPE: 0.09849284843535412  L2+L1 loss: 0.776
Epoch [43/150], Batch loss: 4.864
Epoch: 43 RMSE:  2.2719142557687633  MAPE: 0.2514111666712617  L2+L1 loss: 1.224
Epoch [44/150], Batch loss: 1.958
Epoch: 44 RMSE:  1.3706613320769085  MAPE: 0.24042664599377575  L2+L1 loss: 1.193
Epoch [45/150], Batch loss: 1.058
Epoch: 45 RMSE:  1.1765361256319646  MAPE: 0.07595691155089754  L2+L1 loss: 0.671
Epoch [46/150], Batch loss: 0.973
Epoch: 46 RMSE:  0.9301140184134437  MAPE: 0.07458572854648819  L2+L1 loss: 0.635
Epoch [47/150], Batch loss: 0.954
Epoch: 47 RMSE:  1.3027833835543257  MAPE: 0.3595655956659988  L2+L1 loss: 1.386
Epoch [48/150], Batch loss: 1.34
Epoch: 48 RMSE:  0.7588060446331542  MAPE: 0.09811914270174454  L2+L1 loss: 0.663
Epoch [49/150], Batch loss: 0.721
Epoch: 49 RMSE:  0.7135032872163293  MAPE: 0.0956277071418308  L2+L1 loss: 0.684
Epoch [50/150], Batch loss: 0.712
Epoch: 50 RMSE:  0.7348512721137748  MAPE: 0.08351705915965991  L2+L1 loss: 0.63
Epoch [51/150], Batch loss: 0.653
Epoch: 51 RMSE:  0.6922006594897634  MAPE: 0.14574192483074108  L2+L1 loss: 0.859
Epoch [52/150], Batch loss: 0.723
Epoch: 52 RMSE:  0.5019600494042293  MAPE: 0.05981579976999022  L2+L1 loss: 0.5
Epoch [53/150], Batch loss: 0.874
Epoch: 53 RMSE:  0.573087939085114  MAPE: 0.05650182464891798  L2+L1 loss: 0.474
Epoch [54/150], Batch loss: 0.616
Epoch: 54 RMSE:  0.4509839122737159  MAPE: 0.07652095480949589  L2+L1 loss: 0.533
Epoch [55/150], Batch loss: 0.623
Epoch: 55 RMSE:  0.43588717304088814  MAPE: 0.10671441563316317  L2+L1 loss: 0.629
Epoch [56/150], Batch loss: 1.461
Epoch: 56 RMSE:  0.5751161542815282  MAPE: 0.11577264603992363  L2+L1 loss: 0.699
Epoch [57/150], Batch loss: 0.666
Epoch: 57 RMSE:  0.523989228240074  MAPE: 0.07132291083093893  L2+L1 loss: 0.521
Epoch [58/150], Batch loss: 0.608
Epoch: 58 RMSE:  0.5338032316751558  MAPE: 0.17630981736064155  L2+L1 loss: 0.839
Epoch [59/150], Batch loss: 0.898
Epoch: 59 RMSE:  0.6995060630898996  MAPE: 0.11076010783759284  L2+L1 loss: 0.718
Epoch [60/150], Batch loss: 0.482
Epoch: 60 RMSE:  0.24936121574074666  MAPE: 0.04777151306412508  L2+L1 loss: 0.366
Epoch [61/150], Batch loss: 0.348
Epoch: 61 RMSE:  0.2637725921714067  MAPE: 0.04776020463196289  L2+L1 loss: 0.364
Epoch [62/150], Batch loss: 0.354
Epoch: 62 RMSE:  0.2276996986377217  MAPE: 0.04998190471841594  L2+L1 loss: 0.381
Epoch [63/150], Batch loss: 0.339
Epoch: 63 RMSE:  0.21511406780938766  MAPE: 0.04577741486784149  L2+L1 loss: 0.352
Epoch [64/150], Batch loss: 0.344
Epoch: 64 RMSE:  0.23445361052555636  MAPE: 0.061781390951396  L2+L1 loss: 0.445
Epoch [65/150], Batch loss: 0.308
Epoch: 65 RMSE:  0.21427841587230725  MAPE: 0.04539649844736971  L2+L1 loss: 0.354
Epoch [66/150], Batch loss: 0.313
Epoch: 66 RMSE:  0.24906553370150775  MAPE: 0.06371025030477365  L2+L1 loss: 0.433
Epoch [67/150], Batch loss: 0.333
Epoch: 67 RMSE:  0.20309148728259552  MAPE: 0.04360833835333115  L2+L1 loss: 0.346
Epoch [68/150], Batch loss: 0.304
Epoch: 68 RMSE:  0.22301635561559754  MAPE: 0.052627815525623386  L2+L1 loss: 0.39
Epoch [69/150], Batch loss: 0.301
Epoch: 69 RMSE:  0.20942235300785908  MAPE: 0.058255571272493645  L2+L1 loss: 0.42
Epoch [70/150], Batch loss: 0.294
Epoch: 70 RMSE:  0.19879251173759366  MAPE: 0.044947937411052734  L2+L1 loss: 0.351
Epoch [71/150], Batch loss: 0.3
Epoch: 71 RMSE:  0.20102648051692731  MAPE: 0.05356634398710518  L2+L1 loss: 0.401
Epoch [72/150], Batch loss: 0.287
Epoch: 72 RMSE:  0.1955571154628537  MAPE: 0.04017432774803641  L2+L1 loss: 0.334
Epoch [73/150], Batch loss: 0.306
Epoch: 73 RMSE:  0.18111159677068284  MAPE: 0.03960274229246485  L2+L1 loss: 0.329
Epoch [74/150], Batch loss: 0.283
Epoch: 74 RMSE:  0.2092407392224744  MAPE: 0.048518958809736966  L2+L1 loss: 0.379
Epoch [75/150], Batch loss: 0.283
Epoch: 75 RMSE:  0.225341156562519  MAPE: 0.050726961270405026  L2+L1 loss: 0.393
Epoch [76/150], Batch loss: 0.317
Epoch: 76 RMSE:  0.19059744364123996  MAPE: 0.03919764830568486  L2+L1 loss: 0.335
Epoch [77/150], Batch loss: 0.291
Epoch: 77 RMSE:  0.1757845692972317  MAPE: 0.03971683725909921  L2+L1 loss: 0.338
Epoch [78/150], Batch loss: 0.276
Epoch: 78 RMSE:  0.18347442970812558  MAPE: 0.03793258339090203  L2+L1 loss: 0.329
Epoch [79/150], Batch loss: 0.277
Epoch: 79 RMSE:  0.17530317222083014  MAPE: 0.04001494366353952  L2+L1 loss: 0.33
Epoch [80/150], Batch loss: 0.283
Epoch: 80 RMSE:  0.18148654400335432  MAPE: 0.04709985751312009  L2+L1 loss: 0.374
Epoch [81/150], Batch loss: 0.271
Epoch: 81 RMSE:  0.20453966662417647  MAPE: 0.04402719857235743  L2+L1 loss: 0.365
Epoch [82/150], Batch loss: 0.274
Epoch: 82 RMSE:  0.17465264493741042  MAPE: 0.039044068430272  L2+L1 loss: 0.324
Epoch [83/150], Batch loss: 0.308
Epoch: 83 RMSE:  0.17226385723627535  MAPE: 0.037528930284000284  L2+L1 loss: 0.32
Epoch [84/150], Batch loss: 0.253
Epoch: 84 RMSE:  0.17786044525352218  MAPE: 0.04685401460370072  L2+L1 loss: 0.366
Epoch [85/150], Batch loss: 0.277
Epoch: 85 RMSE:  0.17133731735353797  MAPE: 0.038541380500636754  L2+L1 loss: 0.32
Epoch [86/150], Batch loss: 0.246
Epoch: 86 RMSE:  0.18781821805941948  MAPE: 0.035201758070955554  L2+L1 loss: 0.322
Epoch [87/150], Batch loss: 0.271
Epoch: 87 RMSE:  0.17761761545430074  MAPE: 0.04218185800568065  L2+L1 loss: 0.34
Epoch [88/150], Batch loss: 0.279
Epoch: 88 RMSE:  0.1955687724219763  MAPE: 0.044234948058401995  L2+L1 loss: 0.367
Epoch [89/150], Batch loss: 0.249
Epoch: 89 RMSE:  0.19726195811016933  MAPE: 0.055832649961162065  L2+L1 loss: 0.418
Epoch [90/150], Batch loss: 0.231
Epoch: 90 RMSE:  0.17122876534852868  MAPE: 0.034050249246567084  L2+L1 loss: 0.31
Epoch [91/150], Batch loss: 0.217
Epoch: 91 RMSE:  0.16653121699507314  MAPE: 0.03392704016337228  L2+L1 loss: 0.306
Epoch [92/150], Batch loss: 0.222
Epoch: 92 RMSE:  0.1644019571346701  MAPE: 0.03375508616644228  L2+L1 loss: 0.301
Epoch [93/150], Batch loss: 0.216
Epoch: 93 RMSE:  0.16063974006429868  MAPE: 0.033855033408640364  L2+L1 loss: 0.302
Epoch [94/150], Batch loss: 0.226
Epoch: 94 RMSE:  0.1639510707801062  MAPE: 0.03369107010430486  L2+L1 loss: 0.301
Epoch [95/150], Batch loss: 0.214
Epoch: 95 RMSE:  0.16131002068808814  MAPE: 0.03588536645229488  L2+L1 loss: 0.317
Epoch [96/150], Batch loss: 0.212
Epoch: 96 RMSE:  0.169363223895029  MAPE: 0.03380125374087599  L2+L1 loss: 0.307
Epoch [97/150], Batch loss: 0.221
Epoch: 97 RMSE:  0.1640297355441706  MAPE: 0.033748153902233236  L2+L1 loss: 0.302
Epoch [98/150], Batch loss: 0.222
Epoch: 98 RMSE:  0.16169512318654056  MAPE: 0.03411396830480847  L2+L1 loss: 0.301
Epoch [99/150], Batch loss: 0.217
Epoch: 99 RMSE:  0.16703635886637608  MAPE: 0.03367577052934377  L2+L1 loss: 0.305
Epoch [100/150], Batch loss: 0.217
Epoch: 100 RMSE:  0.1758149720785882  MAPE: 0.03728360491913319  L2+L1 loss: 0.325
Epoch [101/150], Batch loss: 0.22
Epoch: 101 RMSE:  0.16149510409956055  MAPE: 0.033566540950366146  L2+L1 loss: 0.3
Epoch [102/150], Batch loss: 0.215
Epoch: 102 RMSE:  0.16295949353282008  MAPE: 0.03347434243439837  L2+L1 loss: 0.299
Epoch [103/150], Batch loss: 0.214
Epoch: 103 RMSE:  0.16215056619490573  MAPE: 0.03338791271276336  L2+L1 loss: 0.299
Epoch [104/150], Batch loss: 0.224
Epoch: 104 RMSE:  0.16288966718423173  MAPE: 0.03342476132147949  L2+L1 loss: 0.3
Epoch [105/150], Batch loss: 0.222
Epoch: 105 RMSE:  0.16334850494822026  MAPE: 0.033450864881364645  L2+L1 loss: 0.301
Epoch [106/150], Batch loss: 0.213
Epoch: 106 RMSE:  0.16102857262792342  MAPE: 0.0358885107766761  L2+L1 loss: 0.319
Epoch [107/150], Batch loss: 0.207
Epoch: 107 RMSE:  0.16984077573870135  MAPE: 0.03425474626963827  L2+L1 loss: 0.31
Epoch [108/150], Batch loss: 0.214
Epoch: 108 RMSE:  0.1611420966060778  MAPE: 0.033560413445694576  L2+L1 loss: 0.301
Epoch [109/150], Batch loss: 0.213
Epoch: 109 RMSE:  0.16084740540520212  MAPE: 0.03385133074765612  L2+L1 loss: 0.303
Epoch [110/150], Batch loss: 0.219
Epoch: 110 RMSE:  0.17810212481959287  MAPE: 0.03634274035010144  L2+L1 loss: 0.325
Epoch [111/150], Batch loss: 0.211
Epoch: 111 RMSE:  0.15976383549179426  MAPE: 0.03399667165772527  L2+L1 loss: 0.308
Epoch [112/150], Batch loss: 0.211
Epoch: 112 RMSE:  0.16494153315026522  MAPE: 0.03318404976368603  L2+L1 loss: 0.301
Epoch [113/150], Batch loss: 0.213
Epoch: 113 RMSE:  0.17082597917109754  MAPE: 0.035422876590129944  L2+L1 loss: 0.315
Epoch [114/150], Batch loss: 0.211
Epoch: 114 RMSE:  0.1608375626539987  MAPE: 0.03321262259293413  L2+L1 loss: 0.301
Epoch [115/150], Batch loss: 0.215
Epoch: 115 RMSE:  0.16211544308235215  MAPE: 0.03313405553050465  L2+L1 loss: 0.299
Epoch [116/150], Batch loss: 0.219
Epoch: 116 RMSE:  0.1618079461415889  MAPE: 0.03302944284605476  L2+L1 loss: 0.299
Epoch [117/150], Batch loss: 0.213
Epoch: 117 RMSE:  0.16246402785085873  MAPE: 0.033410118799506346  L2+L1 loss: 0.304
Epoch [118/150], Batch loss: 0.213
Epoch: 118 RMSE:  0.16158537633018424  MAPE: 0.03295647219254607  L2+L1 loss: 0.298
Epoch [119/150], Batch loss: 0.218
Epoch: 119 RMSE:  0.16797005458037714  MAPE: 0.033397663290255855  L2+L1 loss: 0.305
Epoch [120/150], Batch loss: 0.212
Epoch: 120 RMSE:  0.1644471079259197  MAPE: 0.03294329182459698  L2+L1 loss: 0.299
Epoch [121/150], Batch loss: 0.2
Epoch: 121 RMSE:  0.163203522750867  MAPE: 0.0329932856926695  L2+L1 loss: 0.299
Epoch [122/150], Batch loss: 0.208
Epoch: 122 RMSE:  0.1630716552256341  MAPE: 0.033004772454404856  L2+L1 loss: 0.299
Epoch [123/150], Batch loss: 0.203
Epoch: 123 RMSE:  0.1626767027060763  MAPE: 0.032942219368197106  L2+L1 loss: 0.298
Epoch [124/150], Batch loss: 0.205
Epoch: 124 RMSE:  0.16223107705414763  MAPE: 0.03309990688332509  L2+L1 loss: 0.299
Epoch [125/150], Batch loss: 0.21
Epoch: 125 RMSE:  0.1628491571369125  MAPE: 0.032917001792467274  L2+L1 loss: 0.298
Epoch [126/150], Batch loss: 0.208
Epoch: 126 RMSE:  0.162464759767555  MAPE: 0.03291596835714029  L2+L1 loss: 0.297
Epoch [127/150], Batch loss: 0.205
Epoch: 127 RMSE:  0.16212731397862665  MAPE: 0.033073147249774636  L2+L1 loss: 0.299
Epoch [128/150], Batch loss: 0.205
Epoch: 128 RMSE:  0.162640204094609  MAPE: 0.03291275199375806  L2+L1 loss: 0.297
Epoch [129/150], Batch loss: 0.206
Epoch: 129 RMSE:  0.16207445463038506  MAPE: 0.03299565633542861  L2+L1 loss: 0.299
Epoch [130/150], Batch loss: 0.202
Epoch: 130 RMSE:  0.16203279441177515  MAPE: 0.03299110006976579  L2+L1 loss: 0.299
Epoch [131/150], Batch loss: 0.204
Epoch: 131 RMSE:  0.16181394173565491  MAPE: 0.03300979129312403  L2+L1 loss: 0.3
Epoch [132/150], Batch loss: 0.204
Epoch: 132 RMSE:  0.16174379333312125  MAPE: 0.03305234371637649  L2+L1 loss: 0.3
Epoch [133/150], Batch loss: 0.206
Epoch: 133 RMSE:  0.16223583902509658  MAPE: 0.03289601680708458  L2+L1 loss: 0.298
Epoch [134/150], Batch loss: 0.207
Epoch: 134 RMSE:  0.16200961112719706  MAPE: 0.03296514953058676  L2+L1 loss: 0.299
Epoch [135/150], Batch loss: 0.207
Epoch: 135 RMSE:  0.16210621267694386  MAPE: 0.03290463939199227  L2+L1 loss: 0.298
Epoch [136/150], Batch loss: 0.204
Epoch: 136 RMSE:  0.16210628028459145  MAPE: 0.03289373963281795  L2+L1 loss: 0.299
Epoch [137/150], Batch loss: 0.209
Epoch: 137 RMSE:  0.16210603319172964  MAPE: 0.032912530240782126  L2+L1 loss: 0.298
Epoch [138/150], Batch loss: 0.205
Epoch: 138 RMSE:  0.16209058735680726  MAPE: 0.03290927430135695  L2+L1 loss: 0.298
Epoch [139/150], Batch loss: 0.205
Epoch: 139 RMSE:  0.16243098215646667  MAPE: 0.03287854761116097  L2+L1 loss: 0.298
Epoch [140/150], Batch loss: 0.205
Epoch: 140 RMSE:  0.1621476767783353  MAPE: 0.032886713405436745  L2+L1 loss: 0.298
Epoch [141/150], Batch loss: 0.209
Epoch: 141 RMSE:  0.16250819915548975  MAPE: 0.03287445075030232  L2+L1 loss: 0.298
Epoch [142/150], Batch loss: 0.206
Epoch: 142 RMSE:  0.16237967590321456  MAPE: 0.032889262929943805  L2+L1 loss: 0.299
Epoch [143/150], Batch loss: 0.2
Epoch: 143 RMSE:  0.1627206789455553  MAPE: 0.032869593953638435  L2+L1 loss: 0.297
Epoch [144/150], Batch loss: 0.209
Epoch: 144 RMSE:  0.1620053415630657  MAPE: 0.032922274786586575  L2+L1 loss: 0.299
Epoch [145/150], Batch loss: 0.206
Epoch: 145 RMSE:  0.16206787726559121  MAPE: 0.03293402383071365  L2+L1 loss: 0.299
Epoch [146/150], Batch loss: 0.204
Epoch: 146 RMSE:  0.1618037627119751  MAPE: 0.03306731823273414  L2+L1 loss: 0.3
Epoch [147/150], Batch loss: 0.209
Epoch: 147 RMSE:  0.16196253043253164  MAPE: 0.03298805316737117  L2+L1 loss: 0.299
Epoch [148/150], Batch loss: 0.207
Epoch: 148 RMSE:  0.16191052892253036  MAPE: 0.03293163030684548  L2+L1 loss: 0.299
Epoch [149/150], Batch loss: 0.211
Epoch: 149 RMSE:  0.16197062942588134  MAPE: 0.03287634052328196  L2+L1 loss: 0.298


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
2.6682901 , 2.7577
9.30074 , 9.3887
1.5581827 , 1.4518
4.073393 , 4.1102
2.7376366 , 2.7423
2.0696888 , 2.0855
2.2633648 , 2.3119
3.710884 , 3.6755
1.9853764 , 1.832
2.4174376 , 2.54
2.2043571 , 2.16
1.145771 , 1.0811
3.020609 , 3.036
4.3529444 , 4.2112
2.2819357 , 2.202
3.7230263 , 3.5592
2.9251938 , 2.9699
2.330265 , 2.3724
1.0417423 , 1.1706
2.2252798 , 2.1421
1.5236254 , 1.6732
3.244028 , 3.1369
2.0189524 , 1.8616
2.5998297 , 2.5639
2.5056982 , 2.4964
3.988432 , 4.2724
3.2248602 , 3.2805
2.9970055 , 2.8828
3.1803045 , 3.1981
3.480914 , 3.3909
2.0270853 , 2.2259
1.7434969 , 1.6524
9.942012 , 9.538
3.3465986 , 3.2696
5.7770424 , 5.821
2.7103777 , 2.6867
5.168643 , 5.2322
6.551862 , 6.5296
2.8752728 , 2.8947
2.4404411 , 2.4843
2.6125517 , 2.7581
3.2307987 , 3.0075
4.514287 , 4.5606
7.814966 , 8.211
4.6496134 , 4.6476
2.4780445 , 2.3507
3.5264778 , 3.5077
10.881322 , 10.9957
1.5820236 , 1.5343
4.211667 , 4.2921
1.5884113 , 1.7442
6.8566957 , 6.9421
2.7191992 , 2.3897
2.4375477 , 2.4947
1.3365059 , 1.4159
2.4534292 , 2.4895
7.4846945 , 7.8129
5.8325996 , 6.0519
23.121513 , 23.7837
2.2096167 , 2.4735
3.933776 , 3.9538
69.071686 , 68.0377
3.1979485 , 3.1898
1.6880941 , 1.6255
1.1227083 , 1.0546
1.4341993 , 1.5523
4.223195 , 3.8327
6.73155 , 6.6236
2.767129 , 2.8094
3.9877958 , 3.979
2.720707 , 2.7313
4.2515173 , 3.8665
5.580621 , 5.7991
2.8416576 , 2.9144
2.2130756 , 2.0787
2.027255 , 2.1329
3.0860634 , 2.9695
2.2642508 , 2.3621
6.491334 , 6.6662
2.350813 , 2.3332
31.719175 , 32.3558
5.1094627 , 4.608
3.0138855 , 3.0043
1.863471 , 1.7665
2.892352 , 2.8947
6.314165 , 5.9858
2.5608425 , 2.5082
1.2575779 , 1.0444
1.5097656 , 1.6514
2.8656921 , 2.8769
0.59478664 , 0.833613
2.5548134 , 2.6067
3.8880959 , 3.9413
3.1351051 , 3.1279
2.2375946 , 2.1564
3.6417646 , 3.3869
25.69823 , 24.7268
3.1094885 , 3.0926
4.9804764 , 5.2039
2.9221754 , 2.9962
3.7835026 , 3.8049
4.7919436 , 4.9434
3.1310005 , 3.486
3.8796673 , 3.7825
3.1934738 , 3.1562
2.7447338 , 2.7337
4.9248514 , 4.5901
4.700982 , 4.5324
3.971877 , 4.0604
2.9820108 , 2.7045
2.3878555 , 2.3544
1.4907627 , 1.4037
2.8301487 , 2.8248
1.6650743 , 1.6865
4.214512 , 4.0057
2.84789 , 2.8026
4.425953 , 4.4191
3.362918 , 3.2022
3.5457697 , 3.3305
1.8552923 , 1.8662
3.8646994 , 3.8427
2.6453772 , 3.1255
4.1575766 , 4.1179
4.2728996 , 4.1909
4.213752 , 4.2615
3.004179 , 2.7775
1.4084377 , 1.3169
2.925418 , 2.9214
1.2091599 , 1.198
3.4596224 , 3.4709
2.0313969 , 1.9467
1.9612474 , 1.8758
2.784996 , 2.7443
1.2494574 , 1.2912
5.117485 , 5.0803
1.7917681 , 1.8944
2.7501726 , 2.7377
5.254657 , 5.6959
26.193188 , 26.2967
3.454321 , 3.5001
73.6659 , 72.8888
1.917676 , 1.855
1.7989922 , 2.0849
2.9378843 , 2.9145
6.9733915 , 7.2846
3.5657816 , 3.5626
1.8657532 , 1.7272
2.8263445 , 2.8603
4.013708 , 4.0447
3.2371988 , 3.2883
7.7442646 , 8.0159
1.8675146 , 1.8037
5.994875 , 5.6423
1.1150799 , 1.2232
1.5075636 , 1.5544
5.7428827 , 5.748
5.6769905 , 5.5607
2.8055477 , 3.0206
2.4778414 , 2.3797
2.9394531 , 2.9217
3.521264 , 3.466
3.409647 , 3.5043
2.749175 , 2.7766
1.5708199 , 1.4114
6.8477364 , 6.782
2.6122398 , 2.4503
1.6943083 , 1.5491
76.288025 , 75.2285
3.9930859 , 3.8943
2.4293318 , 2.4265
3.3696232 , 3.3829
2.7860222 , 2.7088
3.2542753 , 3.3285
3.6947641 , 3.6788
4.4967747 , 4.1505
3.4139338 , 3.3747
25.839706 , 26.3383
2.0408773 , 2.0809
3.0823402 , 3.062
3.0831013 , 2.8946
4.1742964 , 4.2779
4.2500057 , 3.9483
4.860897 , 4.934
31.312172 , 30.2786
2.0234022 , 1.9842
0.8586731 , 1.0494
5.3082323 , 5.0055
1.5105257 , 1.3821
3.062191 , 3.0497
2.8916788 , 2.8425
1.0765324 , 1.0368
3.5733862 , 3.5674
3.0394468 , 3.0974
2.6469984 , 2.6967
4.1836433 , 4.1741
2.7824545 , 2.9427
4.622629 , 4.664
2.0221539 , 2.1283
3.182684 , 3.1777
0.9218979 , 1.0378
1.0538187 , 1.3505
1.5333557 , 1.5924
1.6133156 , 1.5465
4.039774 , 4.0135
2.8019028 , 2.7016
4.4880857 , 4.264
3.3289394 , 3.3323
5.4265757 , 5.8378
4.924794 , 4.5817
6.4140816 , 6.5372
3.2759218 , 3.162
1.6441298 , 1.4943
1.365159 , 1.2967
3.48843 , 3.2649
2.7897558 , 2.8398
1.9067097 , 2.2221
2.328024 , 2.3868
7.264751 , 7.1573
3.0669422 , 3.1253
2.6114655 , 2.6154
3.7393045 , 3.7422
2.9512806 , 2.9181
5.2749996 , 5.3863
2.4427767 , 2.3745
9.457854 , 9.3249
1.0263262 , 1.0484
1.3899698 , 1.4793
2.1895266 , 2.074
2.7495718 , 2.7711
2.556839 , 2.3826
2.4330235 , 2.4244
9.51866 , 9.257
2.850089 , 2.81
1.6270561 , 1.5977
5.123726 , 5.1951
3.6205854 , 3.6369
2.8433104 , 2.8516
3.824788 , 3.6503
0.7292156 , 1.0779
2.135169 , 2.0613
2.8303442 , 2.9125
0.98690414 , 1.0244
2.9494925 , 2.9821
3.2306166 , 3.0138
3.3810968 , 3.4163
1.3668337 , 1.1999
2.5089035 , 2.5476
2.5836096 , 2.6359
4.0295887 , 4.1902
28.478008 , 28.3285
1.8790293 , 1.7458
3.5265636 , 3.5363
5.307329 , 5.3275
1.7989588 , 1.66
26.141195 , 26.3408
2.2880507 , 2.2769
12.883797 , 12.4454
4.940632 , 4.9546
2.9372072 , 3.12
2.931673 , 2.9459
1.6735773 , 1.6997
2.609726 , 2.6897
3.319147 , 3.3301
2.2429924 , 2.1368
3.6257534 , 3.6307
1.5754232 , 1.812
3.0174732 , 2.9061
1.2044001 , 1.4738
3.2277584 , 3.2787
1.5991888 , 1.6294
6.1856384 , 6.3421
2.4725018 , 2.5632
RMSE:  0.20963985636588478  MAPE: 0.03900351322487408
5: ground truth total-  227  predicted total -  227
100: ground truth total-  45  predicted total -  45
 more 100: ground truth total -  0  predicted total -  0



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
0.085066795 , 0.059363
0.5958004 , 0.607287
0.6268902 , 0.836645
0.6540537 , 0.553198
0.63029575 , 0.608092
0.689271 , 0.608808
0.0 , 0.025338
0.11843586 , 0.038725
0.11514664 , 0.080625
0.6109514 , 0.471698
0.15032196 , 0.106056
0.12034321 , 0.028886
0.13223267 , 0.095544
0.7190466 , 0.722726
0.11441326 , 0.09134
0.5553856 , 0.353503
0.051605225 , 0.048354
0.68586063 , 0.834768
0.4770708 , 0.234538
0.6741142 , 0.953731
0.72896767 , 1.054912
0.11707306 , 0.049389
0.11601162 , 0.069751
0.111026764 , 0.081157
0.6510067 , 1.060585
0.0 , 0.019244
0.551981 , 0.469273
0.18340778 , 0.134397
0.0047330856 , 0.035773
RMSE:  0.14310064495632782  MAPE: 0.5762089751743055
