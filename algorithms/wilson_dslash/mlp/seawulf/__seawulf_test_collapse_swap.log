['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
2481
19
<class 'numpy.dtype'> float64
2481
train batches:  112  validate samples: 198  test samples: 496
Epoch [0/150], Batch loss: 35.462
Epoch: 0 RMSE:  33.88612447660011  MAPE: 0.7394409104614147  L2+L1 loss: 10.963
Epoch [1/150], Batch loss: 24.787
Epoch: 1 RMSE:  33.18172700011959  MAPE: 1.468792783133625  L2+L1 loss: 10.939
Epoch [2/150], Batch loss: 25.229
Epoch: 2 RMSE:  32.84160632346272  MAPE: 2.012887732567131  L2+L1 loss: 11.17
Epoch [3/150], Batch loss: 24.358
Epoch: 3 RMSE:  32.677418223734634  MAPE: 2.390952745855027  L2+L1 loss: 11.414
Epoch [4/150], Batch loss: 24.803
Epoch: 4 RMSE:  32.646101395336004  MAPE: 2.4827712305715623  L2+L1 loss: 11.49
Epoch [5/150], Batch loss: 25.366
Epoch: 5 RMSE:  32.621994385747215  MAPE: 2.5606865573784305  L2+L1 loss: 11.557
Epoch [6/150], Batch loss: 25.199
Epoch: 6 RMSE:  32.62138669631635  MAPE: 2.5627556222783325  L2+L1 loss: 11.559
Epoch [7/150], Batch loss: 25.229
Epoch: 7 RMSE:  32.631371073615156  MAPE: 2.529523459838258  L2+L1 loss: 11.53
Epoch [8/150], Batch loss: 25.255
Epoch: 8 RMSE:  32.627413086098066  MAPE: 2.542512855646437  L2+L1 loss: 11.541
Epoch [9/150], Batch loss: 32.246
Epoch: 9 RMSE:  33.03386872814894  MAPE: 2.0305063825668994  L2+L1 loss: 11.652
Epoch [10/150], Batch loss: 26.554
Epoch: 10 RMSE:  32.74560323029614  MAPE: 2.217598977171008  L2+L1 loss: 11.29
Epoch [11/150], Batch loss: 25.205
Epoch: 11 RMSE:  32.66695417947626  MAPE: 2.4206312016504574  L2+L1 loss: 11.439
Epoch [12/150], Batch loss: 24.788
Epoch: 12 RMSE:  32.66350539427777  MAPE: 2.4306097402685043  L2+L1 loss: 11.447
Epoch [13/150], Batch loss: 25.196
Epoch: 13 RMSE:  32.62696822341348  MAPE: 2.543984940133899  L2+L1 loss: 11.542
Epoch [14/150], Batch loss: 25.654
Epoch: 14 RMSE:  32.65050250657628  MAPE: 2.469261169636346  L2+L1 loss: 11.478
Epoch [15/150], Batch loss: 25.596
Epoch: 15 RMSE:  32.66872117041899  MAPE: 2.4155574729103857  L2+L1 loss: 11.435
Epoch [16/150], Batch loss: 24.902
Epoch: 16 RMSE:  32.625744150201186  MAPE: 2.5480484126129035  L2+L1 loss: 11.545
Epoch [17/150], Batch loss: 24.91
Epoch: 17 RMSE:  32.639329667759725  MAPE: 2.503965747603667  L2+L1 loss: 11.509
Epoch [18/150], Batch loss: 25.39
Epoch: 18 RMSE:  32.65976270357374  MAPE: 2.441555010218375  L2+L1 loss: 11.455
Epoch [19/150], Batch loss: 25.09
Epoch: 19 RMSE:  32.645063229185354  MAPE: 2.485989157589593  L2+L1 loss: 11.493
Epoch [20/150], Batch loss: 29.98
Epoch: 20 RMSE:  32.762678380958  MAPE: 2.1784634770407956  L2+L1 loss: 11.266
Epoch [21/150], Batch loss: 25.068
Epoch: 21 RMSE:  32.68693966284637  MAPE: 2.364750534516314  L2+L1 loss: 11.392
Epoch [22/150], Batch loss: 25.043
Epoch: 22 RMSE:  32.642954521276216  MAPE: 2.4925599150737  L2+L1 loss: 11.499
Epoch [23/150], Batch loss: 27.29
Epoch: 23 RMSE:  32.64821907706695  MAPE: 2.4762413956815275  L2+L1 loss: 11.484
Epoch [24/150], Batch loss: 25.049
Epoch: 24 RMSE:  32.66181024111838  MAPE: 2.436164579025059  L2+L1 loss: 11.451
Epoch [25/150], Batch loss: 25.229
Epoch: 25 RMSE:  32.63243587607004  MAPE: 2.526061408379726  L2+L1 loss: 11.528
Epoch [26/150], Batch loss: 30.21
Epoch: 26 RMSE:  32.64504241521765  MAPE: 2.486053787065749  L2+L1 loss: 11.493
Epoch [27/150], Batch loss: 25.349
Epoch: 27 RMSE:  32.63162311824743  MAPE: 2.528702750158515  L2+L1 loss: 11.53
Epoch [28/150], Batch loss: 24.483
Epoch: 28 RMSE:  32.63564142256777  MAPE: 2.5157195632956433  L2+L1 loss: 11.519
Epoch [29/150], Batch loss: 25.211
Epoch: 29 RMSE:  32.645791992306144  MAPE: 2.483729101497645  L2+L1 loss: 11.491
Epoch [30/150], Batch loss: 24.572
Epoch: 30 RMSE:  32.64644389565106  MAPE: 2.4817120409470514  L2+L1 loss: 11.489
Epoch [31/150], Batch loss: 25.138
Epoch: 31 RMSE:  32.64485597213387  MAPE: 2.486632912328071  L2+L1 loss: 11.494
Epoch [32/150], Batch loss: 24.816
Epoch: 32 RMSE:  32.64643832565775  MAPE: 2.4817292566590408  L2+L1 loss: 11.489
Epoch [33/150], Batch loss: 24.609
Epoch: 33 RMSE:  32.644532789155434  MAPE: 2.4876376325687466  L2+L1 loss: 11.495
Epoch [34/150], Batch loss: 24.921
Epoch: 34 RMSE:  32.64547152982923  MAPE: 2.4847222505220663  L2+L1 loss: 11.492
Epoch [35/150], Batch loss: 24.74
Epoch: 35 RMSE:  32.64410212079919  MAPE: 2.488978200305604  L2+L1 loss: 11.496
Epoch [36/150], Batch loss: 25.09
Epoch: 36 RMSE:  32.6425600273661  MAPE: 2.493794366290755  L2+L1 loss: 11.5
Epoch [37/150], Batch loss: 24.092
Epoch: 37 RMSE:  32.64342935253953  MAPE: 2.491076259369982  L2+L1 loss: 11.498
Epoch [38/150], Batch loss: 24.835
Epoch: 38 RMSE:  32.64292140100386  MAPE: 2.492663491570421  L2+L1 loss: 11.499
Epoch [39/150], Batch loss: 25.998
Epoch: 39 RMSE:  32.64222637430047  MAPE: 2.4948397269007168  L2+L1 loss: 11.501
Epoch [40/150], Batch loss: 25.241
Epoch: 40 RMSE:  32.640050779613354  MAPE: 2.5016853713270764  L2+L1 loss: 11.507
Epoch [41/150], Batch loss: 24.438
Epoch: 41 RMSE:  32.63981523832771  MAPE: 2.5024295980896216  L2+L1 loss: 11.508
Epoch [42/150], Batch loss: 25.766
Epoch: 42 RMSE:  23.758508581095512  MAPE: 2.1887328706332587  L2+L1 loss: 9.58
Epoch [43/150], Batch loss: 11.345
Epoch: 43 RMSE:  6.439912721120789  MAPE: 0.5164090569199189  L2+L1 loss: 2.348
Epoch [44/150], Batch loss: 10.177
Epoch: 44 RMSE:  8.175900396282437  MAPE: 0.33084520145867885  L2+L1 loss: 2.771
Epoch [45/150], Batch loss: 6.693
Epoch: 45 RMSE:  8.039110190435922  MAPE: 0.36677425996877744  L2+L1 loss: 2.768
Epoch [46/150], Batch loss: 5.774
Epoch: 46 RMSE:  6.300927261588281  MAPE: 0.2868232945368313  L2+L1 loss: 2.115
Epoch [47/150], Batch loss: 4.625
Epoch: 47 RMSE:  4.379347351250597  MAPE: 0.2826739524992244  L2+L1 loss: 1.557
Epoch [48/150], Batch loss: 4.928
Epoch: 48 RMSE:  4.033660449192749  MAPE: 0.33956874149645677  L2+L1 loss: 1.452
Epoch [49/150], Batch loss: 3.856
Epoch: 49 RMSE:  2.3858943374352215  MAPE: 0.33222133983621244  L2+L1 loss: 1.218
Epoch [50/150], Batch loss: 5.349
Epoch: 50 RMSE:  6.698484335995732  MAPE: 0.38536947407407807  L2+L1 loss: 2.32
Epoch [51/150], Batch loss: 4.442
Epoch: 51 RMSE:  3.805096434232341  MAPE: 0.2577392623765061  L2+L1 loss: 1.463
Epoch [52/150], Batch loss: 5.022
Epoch: 52 RMSE:  1.8251907698547434  MAPE: 0.3170039156105116  L2+L1 loss: 0.99
Epoch [53/150], Batch loss: 3.957
Epoch: 53 RMSE:  6.19839989127176  MAPE: 0.34335479105405275  L2+L1 loss: 2.157
Epoch [54/150], Batch loss: 5.755
Epoch: 54 RMSE:  2.0960142817388694  MAPE: 0.31334171031533764  L2+L1 loss: 1.032
Epoch [55/150], Batch loss: 4.371
Epoch: 55 RMSE:  1.9188093689392107  MAPE: 0.28268067000696007  L2+L1 loss: 1.142
Epoch [56/150], Batch loss: 4.459
Epoch: 56 RMSE:  2.7426553046747895  MAPE: 0.24883456820546956  L2+L1 loss: 1.319
Epoch [57/150], Batch loss: 4.372
Epoch: 57 RMSE:  4.186321522918826  MAPE: 0.4345232738888238  L2+L1 loss: 1.716
Epoch [58/150], Batch loss: 6.55
Epoch: 58 RMSE:  3.559533307460661  MAPE: 0.3480024992741966  L2+L1 loss: 1.515
Epoch [59/150], Batch loss: 3.893
Epoch: 59 RMSE:  1.7048880383474807  MAPE: 0.23700334703682854  L2+L1 loss: 0.977
Epoch [60/150], Batch loss: 2.785
Epoch: 60 RMSE:  1.735621094760979  MAPE: 0.19322638499495134  L2+L1 loss: 0.621
Epoch [61/150], Batch loss: 2.435
Epoch: 61 RMSE:  1.4575357948123353  MAPE: 0.19149992482073425  L2+L1 loss: 0.561
Epoch [62/150], Batch loss: 2.462
Epoch: 62 RMSE:  1.4635959472147595  MAPE: 0.18888597429534623  L2+L1 loss: 0.546
Epoch [63/150], Batch loss: 2.385
Epoch: 63 RMSE:  1.334214076973613  MAPE: 0.18730347565179098  L2+L1 loss: 0.482
Epoch [64/150], Batch loss: 2.344
Epoch: 64 RMSE:  1.3431477727692207  MAPE: 0.19433123541591524  L2+L1 loss: 0.532
Epoch [65/150], Batch loss: 2.288
Epoch: 65 RMSE:  1.3819287021159417  MAPE: 0.18970350531061486  L2+L1 loss: 0.504
Epoch [66/150], Batch loss: 2.261
Epoch: 66 RMSE:  1.29251063129113  MAPE: 0.188599348037795  L2+L1 loss: 0.475
Epoch [67/150], Batch loss: 2.2
Epoch: 67 RMSE:  1.3570464977867869  MAPE: 0.19192768792431342  L2+L1 loss: 0.5
Epoch [68/150], Batch loss: 2.325
Epoch: 68 RMSE:  1.168456122026387  MAPE: 0.19268084268463312  L2+L1 loss: 0.537
Epoch [69/150], Batch loss: 2.256
Epoch: 69 RMSE:  1.1184490548803443  MAPE: 0.19243455278849683  L2+L1 loss: 0.479
Epoch [70/150], Batch loss: 2.162
Epoch: 70 RMSE:  1.2173300056518213  MAPE: 0.20011160209699005  L2+L1 loss: 0.595
Epoch [71/150], Batch loss: 2.204
Epoch: 71 RMSE:  1.1011728507181338  MAPE: 0.18623893058979013  L2+L1 loss: 0.439
Epoch [72/150], Batch loss: 2.162
Epoch: 72 RMSE:  1.191110770433519  MAPE: 0.18504264968992234  L2+L1 loss: 0.448
Epoch [73/150], Batch loss: 2.303
Epoch: 73 RMSE:  1.2370373636255099  MAPE: 0.18553861704251257  L2+L1 loss: 0.462
Epoch [74/150], Batch loss: 2.15
Epoch: 74 RMSE:  0.9958422870564633  MAPE: 0.1283893710977255  L2+L1 loss: 0.4
Epoch [75/150], Batch loss: 1.977
Epoch: 75 RMSE:  0.9246859249621769  MAPE: 0.07459320461186834  L2+L1 loss: 0.396
Epoch [76/150], Batch loss: 2.051
Epoch: 76 RMSE:  0.9205906353526285  MAPE: 0.04646530745474282  L2+L1 loss: 0.427
Epoch [77/150], Batch loss: 1.956
Epoch: 77 RMSE:  1.0193647804774106  MAPE: 0.16874920267791982  L2+L1 loss: 0.428
Epoch [78/150], Batch loss: 2.017
Epoch: 78 RMSE:  1.754077709476118  MAPE: 0.04571585786628018  L2+L1 loss: 0.58
Epoch [79/150], Batch loss: 2.014
Epoch: 79 RMSE:  0.8600764197134176  MAPE: 0.0808656616344829  L2+L1 loss: 0.448
Epoch [80/150], Batch loss: 1.907
Epoch: 80 RMSE:  0.8509810261402215  MAPE: 0.04112817195366997  L2+L1 loss: 0.379
Epoch [81/150], Batch loss: 1.965
Epoch: 81 RMSE:  0.929772936603647  MAPE: 0.02045902400769466  L2+L1 loss: 0.414
Epoch [82/150], Batch loss: 1.933
Epoch: 82 RMSE:  0.8567278284610357  MAPE: 0.03140845110477278  L2+L1 loss: 0.379
Epoch [83/150], Batch loss: 2.026
Epoch: 83 RMSE:  0.9915554520097205  MAPE: 0.03810754732928587  L2+L1 loss: 0.481
Epoch [84/150], Batch loss: 2.004
Epoch: 84 RMSE:  0.8889674504922741  MAPE: 0.06488562153298172  L2+L1 loss: 0.429
Epoch [85/150], Batch loss: 1.863
Epoch: 85 RMSE:  0.9588994407861002  MAPE: 0.046502441772000955  L2+L1 loss: 0.4
Epoch [86/150], Batch loss: 1.945
Epoch: 86 RMSE:  0.897917429484665  MAPE: 0.04352810516415354  L2+L1 loss: 0.341
Epoch [87/150], Batch loss: 1.84
Epoch: 87 RMSE:  0.8444900206467889  MAPE: 0.04176758764598045  L2+L1 loss: 0.435
Epoch [88/150], Batch loss: 1.831
Epoch: 88 RMSE:  0.8750933551613471  MAPE: 0.05690796037248875  L2+L1 loss: 0.376
Epoch [89/150], Batch loss: 1.841
Epoch: 89 RMSE:  0.7041596829978348  MAPE: 0.09492480308538065  L2+L1 loss: 0.375
Epoch [90/150], Batch loss: 1.521
Epoch: 90 RMSE:  0.7499429387402563  MAPE: 0.07804199735017138  L2+L1 loss: 0.314
Epoch [91/150], Batch loss: 1.551
Epoch: 91 RMSE:  0.6741168018049752  MAPE: 0.07408084398057206  L2+L1 loss: 0.294
Epoch [92/150], Batch loss: 1.559
Epoch: 92 RMSE:  0.659304113862587  MAPE: 0.06574676847722563  L2+L1 loss: 0.299
Epoch [93/150], Batch loss: 1.563
Epoch: 93 RMSE:  0.6567229709008601  MAPE: 0.06471234070941476  L2+L1 loss: 0.291
Epoch [94/150], Batch loss: 1.531
Epoch: 94 RMSE:  0.639493357051823  MAPE: 0.05800439750229201  L2+L1 loss: 0.284
Epoch [95/150], Batch loss: 1.516
Epoch: 95 RMSE:  0.6803830082467537  MAPE: 0.05594051247919911  L2+L1 loss: 0.28
Epoch [96/150], Batch loss: 1.542
Epoch: 96 RMSE:  0.6414200663415531  MAPE: 0.05172208252467296  L2+L1 loss: 0.283
Epoch [97/150], Batch loss: 1.523
Epoch: 97 RMSE:  0.6263475508411979  MAPE: 0.04881092657771356  L2+L1 loss: 0.278
Epoch [98/150], Batch loss: 1.538
Epoch: 98 RMSE:  0.6475661900448728  MAPE: 0.048021993519069266  L2+L1 loss: 0.287
Epoch [99/150], Batch loss: 1.501
Epoch: 99 RMSE:  0.619519122551954  MAPE: 0.04502899511831182  L2+L1 loss: 0.277
Epoch [100/150], Batch loss: 1.507
Epoch: 100 RMSE:  0.6800364712402044  MAPE: 0.04587519721191021  L2+L1 loss: 0.277
Epoch [101/150], Batch loss: 1.505
Epoch: 101 RMSE:  0.6263825153162901  MAPE: 0.04298160275674718  L2+L1 loss: 0.287
Epoch [102/150], Batch loss: 1.517
Epoch: 102 RMSE:  0.6092631308684187  MAPE: 0.043089477343600115  L2+L1 loss: 0.293
Epoch [103/150], Batch loss: 1.488
Epoch: 103 RMSE:  0.6054764509296026  MAPE: 0.035471969385582836  L2+L1 loss: 0.274
Epoch [104/150], Batch loss: 1.478
Epoch: 104 RMSE:  0.6238237913645603  MAPE: 0.035968955684190285  L2+L1 loss: 0.28
Epoch [105/150], Batch loss: 1.488
Epoch: 105 RMSE:  0.6395607481724953  MAPE: 0.03813864225646291  L2+L1 loss: 0.286
Epoch [106/150], Batch loss: 1.464
Epoch: 106 RMSE:  0.608710132385818  MAPE: 0.03758425340099206  L2+L1 loss: 0.283
Epoch [107/150], Batch loss: 1.473
Epoch: 107 RMSE:  0.6142654240210199  MAPE: 0.03786049322248114  L2+L1 loss: 0.283
Epoch [108/150], Batch loss: 1.472
Epoch: 108 RMSE:  0.6446723990472942  MAPE: 0.03455194342599673  L2+L1 loss: 0.279
Epoch [109/150], Batch loss: 1.458
Epoch: 109 RMSE:  0.7384349201684504  MAPE: 0.12050724399212234  L2+L1 loss: 0.296
Epoch [110/150], Batch loss: 1.53
Epoch: 110 RMSE:  0.5976922776618625  MAPE: 0.03055634815303951  L2+L1 loss: 0.278
Epoch [111/150], Batch loss: 1.465
Epoch: 111 RMSE:  0.6056268605376134  MAPE: 0.045616048392613906  L2+L1 loss: 0.287
Epoch [112/150], Batch loss: 1.484
Epoch: 112 RMSE:  0.6344458542290768  MAPE: 0.07311520071893961  L2+L1 loss: 0.277
Epoch [113/150], Batch loss: 1.498
Epoch: 113 RMSE:  0.6064536045856819  MAPE: 0.02137085866604014  L2+L1 loss: 0.273
Epoch [114/150], Batch loss: 1.463
Epoch: 114 RMSE:  0.6191829975881951  MAPE: 0.05215001453473567  L2+L1 loss: 0.283
Epoch [115/150], Batch loss: 1.459
Epoch: 115 RMSE:  0.6028445836429647  MAPE: 0.035407609672210884  L2+L1 loss: 0.273
Epoch [116/150], Batch loss: 1.488
Epoch: 116 RMSE:  0.5776646302109444  MAPE: 0.01937746371674691  L2+L1 loss: 0.264
Epoch [117/150], Batch loss: 1.455
Epoch: 117 RMSE:  0.5800182316735538  MAPE: 0.021802812532860302  L2+L1 loss: 0.268
Epoch [118/150], Batch loss: 1.446
Epoch: 118 RMSE:  0.5844414356623219  MAPE: 0.025964416406950398  L2+L1 loss: 0.267
Epoch [119/150], Batch loss: 1.44
Epoch: 119 RMSE:  0.5889608335070194  MAPE: 0.02861177861107175  L2+L1 loss: 0.274
Epoch [120/150], Batch loss: 1.381
Epoch: 120 RMSE:  0.5868211098496624  MAPE: 0.028475201133140227  L2+L1 loss: 0.268
Epoch [121/150], Batch loss: 1.406
Epoch: 121 RMSE:  0.5857190980405887  MAPE: 0.028409974011882475  L2+L1 loss: 0.27
Epoch [122/150], Batch loss: 1.415
Epoch: 122 RMSE:  0.587383922315417  MAPE: 0.02862976297632875  L2+L1 loss: 0.272
Epoch [123/150], Batch loss: 1.441
Epoch: 123 RMSE:  0.5858592366320863  MAPE: 0.028677109591530796  L2+L1 loss: 0.272
Epoch [124/150], Batch loss: 1.412
Epoch: 124 RMSE:  0.5846205040273529  MAPE: 0.028664999976886846  L2+L1 loss: 0.267
Epoch [125/150], Batch loss: 1.4
Epoch: 125 RMSE:  0.5844438441336872  MAPE: 0.028455077066307472  L2+L1 loss: 0.267
Epoch [126/150], Batch loss: 1.405
Epoch: 126 RMSE:  0.5842601543715003  MAPE: 0.028825474027127252  L2+L1 loss: 0.269
Epoch [127/150], Batch loss: 1.409
Epoch: 127 RMSE:  0.5843749638205011  MAPE: 0.028329643615687666  L2+L1 loss: 0.265
Epoch [128/150], Batch loss: 1.407
Epoch: 128 RMSE:  0.5848935339324864  MAPE: 0.028395426704832454  L2+L1 loss: 0.266
Epoch [129/150], Batch loss: 1.409
Epoch: 129 RMSE:  0.585128177774348  MAPE: 0.02884550554958189  L2+L1 loss: 0.267
Epoch [130/150], Batch loss: 1.417
Epoch: 130 RMSE:  0.5850472111200884  MAPE: 0.028544619281120273  L2+L1 loss: 0.267
Epoch [131/150], Batch loss: 1.404
Epoch: 131 RMSE:  0.5853070871528211  MAPE: 0.02843068622404508  L2+L1 loss: 0.27
Epoch [132/150], Batch loss: 1.418
Epoch: 132 RMSE:  0.5859379053632773  MAPE: 0.02842345508475536  L2+L1 loss: 0.267
Epoch [133/150], Batch loss: 1.377
Epoch: 133 RMSE:  0.5856676169656984  MAPE: 0.028856170352614837  L2+L1 loss: 0.271
Epoch [134/150], Batch loss: 1.411
Epoch: 134 RMSE:  0.5857083870418488  MAPE: 0.028665552640714013  L2+L1 loss: 0.269
Epoch [135/150], Batch loss: 1.424
Epoch: 135 RMSE:  0.586269810834346  MAPE: 0.028347822132261214  L2+L1 loss: 0.268
Epoch [136/150], Batch loss: 1.417
Epoch: 136 RMSE:  0.5864688803275917  MAPE: 0.028539746338871172  L2+L1 loss: 0.269
Epoch [137/150], Batch loss: 1.418
Epoch: 137 RMSE:  0.5871317301183232  MAPE: 0.0287213938001787  L2+L1 loss: 0.27
Epoch [138/150], Batch loss: 1.407
Epoch: 138 RMSE:  0.5874428938252557  MAPE: 0.028360060201402644  L2+L1 loss: 0.266
Epoch [139/150], Batch loss: 1.398
Epoch: 139 RMSE:  0.5863332708273421  MAPE: 0.028408902988460525  L2+L1 loss: 0.265
Epoch [140/150], Batch loss: 1.41
Epoch: 140 RMSE:  0.5867052291951653  MAPE: 0.028385124871791477  L2+L1 loss: 0.266
Epoch [141/150], Batch loss: 1.375
Epoch: 141 RMSE:  0.5855513235658789  MAPE: 0.028172180302925166  L2+L1 loss: 0.267
Epoch [142/150], Batch loss: 1.421
Epoch: 142 RMSE:  0.5860915576831217  MAPE: 0.028310142729619164  L2+L1 loss: 0.267
Epoch [143/150], Batch loss: 1.419
Epoch: 143 RMSE:  0.5869649553285567  MAPE: 0.028259614841819703  L2+L1 loss: 0.266
Epoch [144/150], Batch loss: 1.387
Epoch: 144 RMSE:  0.5876334830754915  MAPE: 0.028316559065287857  L2+L1 loss: 0.268
Epoch [145/150], Batch loss: 1.391
Epoch: 145 RMSE:  0.5857948409152625  MAPE: 0.028228272219646106  L2+L1 loss: 0.269
Epoch [146/150], Batch loss: 1.388
Epoch: 146 RMSE:  0.5872448487799752  MAPE: 0.028120163369936904  L2+L1 loss: 0.265
Epoch [147/150], Batch loss: 1.392
Epoch: 147 RMSE:  0.5880788895768677  MAPE: 0.028214241831744497  L2+L1 loss: 0.268
Epoch [148/150], Batch loss: 1.397
Epoch: 148 RMSE:  0.5865510891446691  MAPE: 0.028463144669303154  L2+L1 loss: 0.27
Epoch [149/150], Batch loss: 1.417
Epoch: 149 RMSE:  0.5878279232783628  MAPE: 0.028185648480794353  L2+L1 loss: 0.266


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
2.32798 , 2.3276
6.421606 , 6.4758
1.898819 , 1.8774
1.2936497 , 1.2932
2.1591568 , 2.1621
2.5969806 , 2.6227
2.0789237 , 2.0863
1.4743786 , 1.4966
33.090855 , 32.7031
129.09596 , 132.6177
3.0151792 , 3.0864
1.2674446 , 1.279
1.9339638 , 1.9232
8.263581 , 8.412
2.25664 , 2.1578
7.4473495 , 7.3929
8.451444 , 8.5728
1.4635353 , 1.461
12.048831 , 12.0666
1.8642459 , 1.8605
1.9108858 , 1.8832
8.376629 , 8.4317
1.2126856 , 28.006811
7.7580233 , 7.7448
27.617249 , 27.7129
6.2851863 , 6.2494
1.6414905 , 1.6979
2.4368525 , 2.4274
12.907791 , 12.8972
2.1092243 , 2.1237
2.0529232 , 2.056
18.455757 , 18.5395
19.502792 , 19.6994
1.6541529 , 1.6407
1.5950446 , 1.5867
12.786333 , 12.7879
2.8432608 , 2.8567
17.195475 , 17.3261
11.134013 , 10.9644
1.0852242 , 1.0587
10.797634 , 10.8481
17.31712 , 17.2824
2.6473136 , 2.6538
11.181982 , 10.9614
7.678501 , 7.6885
9.850912 , 9.8353
14.172096 , 13.793
1.370038 , 1.3618
8.908659 , 8.9284
1.5044537 , 1.4969
11.647206 , 11.6265
1.2103319 , 12.213624
10.755282 , 10.8027
13.195417 , 13.201
2.4376974 , 2.3975
38.18459 , 38.6203
8.0332 , 8.0948
0.98578644 , 1.013
5.343384 , 5.3456
11.743882 , 11.6876
1.6076574 , 1.5974
3.6124482 , 3.7016
15.145023 , 14.8588
11.190545 , 11.3131
1.4015493 , 0.170318
10.556644 , 10.8342
96.654816 , 99.2041
1.9255967 , 1.9557
21.178646 , 21.48
1.3045864 , 1.3029
2.012508 , 1.9994
1.8272696 , 1.8337
15.0179 , 14.9592
14.237602 , 14.222
22.006992 , 22.0222
12.152996 , 12.1485
10.282814 , 10.2466
1.8395624 , 1.8151
9.213153 , 9.2071
8.856186 , 8.9756
4.4622264 , 4.327
12.2929125 , 12.2864
22.248617 , 22.3906
2.0218935 , 1.9723
17.778202 , 17.9235
3.6678348 , 3.632
2.2353454 , 2.222
16.99318 , 16.7386
9.555393 , 9.4638
2.795247 , 2.8164
15.624542 , 15.928
16.02296 , 16.1016
12.270672 , 12.3384
1.1195021 , 1.131
1.9781938 , 1.9737
2.128799 , 2.0942
1.5024786 , 1.5
4.4503074 , 4.583
1.5980663 , 1.6017
17.95232 , 17.9469
2.0627937 , 2.0842
1.437005 , 1.4265
7.468413 , 7.4999
1.9984951 , 1.9863
2.282701 , 2.2433
1.8562331 , 1.9317
6.64079 , 6.655
12.668158 , 12.6831
6.8128414 , 6.8079
2.6756682 , 2.6722
16.370972 , 16.3366
7.472718 , 7.5194
1.6993842 , 1.6885
8.35726 , 8.4597
1.913476 , 1.9042
1.3579559 , 1.3579
21.14832 , 21.4229
9.576031 , 9.5897
1.8703032 , 1.855
14.623861 , 14.6271
1.9785371 , 1.9776
2.0953503 , 2.0939
6.5180545 , 6.4919
10.856424 , 10.8541
1.2063046 , 2.71954
2.0409017 , 2.0318
5.6471305 , 5.6269
2.153419 , 2.1558
22.887302 , 23.1548
1.7687674 , 1.763
4.729459 , 4.8281
2.0466213 , 2.0347
1.6526651 , 1.6298
2.0071678 , 2.0042
8.855545 , 8.8104
1.3907413 , 1.3725
3.4305568 , 3.4822
3.125689 , 3.0686
1.7995005 , 1.7904
7.294349 , 7.2176
10.344829 , 10.351
2.271875 , 2.27
1.2922945 , 1.2788
14.311104 , 14.2762
12.627849 , 12.6577
5.756789 , 5.7293
2.0344625 , 2.018
1.7599635 , 1.7607
112.40848 , 111.1307
6.1365137 , 6.0965
4.7755957 , 4.8826
1.9453988 , 1.9134
1.6277695 , 1.6363
8.93788 , 8.8885
23.66698 , 23.5537
10.992644 , 10.9681
12.212053 , 12.2308
1.4032335 , 0.248008
32.723972 , 32.8017
10.903284 , 10.9002
11.191531 , 11.2148
1.2530146 , 1.2111
7.7169256 , 7.7015
1.8785243 , 1.8751
10.00732 , 9.9884
5.4978514 , 5.5493
4.3834724 , 4.3549
1.4549589 , 1.4804
4.422419 , 4.5603
12.273569 , 12.1814
2.3927522 , 2.3829
16.091621 , 15.9965
1.9036102 , 1.8909
2.200644 , 2.191
7.40928 , 7.3807
8.780665 , 8.87
2.0402513 , 2.0398
6.6950808 , 6.7074
6.4272833 , 6.4046
2.0326395 , 2.0306
307.01294 , 303.4563
7.523344 , 7.4612
1.7629466 , 1.7352
13.099097 , 13.138
20.496418 , 20.4003
1.4024534 , 0.211042
3.0459108 , 3.0364
2.8897266 , 2.8867
13.778623 , 13.7389
4.518534 , 4.6984
33.5873 , 33.4861
1.467864 , 1.4783
1.7589717 , 1.7411
109.77183 , 110.1863
1.4234638 , 1.4146
10.032587 , 10.0876
1.3949986 , 1.3982
1.74364 , 1.731
13.24633 , 13.2509
1.4078493 , 1.4153
2.430956 , 2.4082
1.2354803 , 1.222
12.947475 , 12.9898
1.6059046 , 1.6074
19.863476 , 19.6288
12.215798 , 12.185
16.090422 , 16.0297
1.6849618 , 1.689
6.998501 , 7.0116
2.1710024 , 2.1465
1.6377892 , 1.656
1.4507313 , 1.4475
1.3552532 , 1.3297
1.3077774 , 1.2641
3.330937 , 3.3161
10.021633 , 10.0492
8.021831 , 8.0487
24.998793 , 25.0246
21.34954 , 21.5935
17.568247 , 17.6437
1.3018961 , 1.3047
8.751366 , 8.7951
10.447656 , 10.4286
1.8266225 , 1.8046
1.5727329 , 1.5919
11.9861765 , 11.9844
3.1532192 , 3.1728
7.713562 , 7.7428
1.4082766 , 1.4047
1.9979839 , 1.9904
1.5174599 , 1.5034
1.2235088 , 1.2157
2.3947463 , 2.3699
4.644057 , 4.74
1.9423499 , 1.9115
6.298289 , 6.3308
7.7354517 , 7.7824
3.6927142 , 3.6379
2.4491897 , 2.4476
10.243405 , 10.3205
13.0643215 , 13.1209
9.737637 , 9.7296
8.761398 , 8.7851
7.5354824 , 7.5265
1.5382204 , 1.5387
2.5669723 , 2.5412
1.6894026 , 1.6894
2.7208338 , 2.694
2.452117 , 2.4554
2.143289 , 2.1812
14.76153 , 14.7647
1.7401094 , 1.7195
6.5873747 , 6.6324
7.752495 , 7.822
1.1147861 , 1.1463
3.0326996 , 3.0752
20.563068 , 20.4746
10.477955 , 10.4658
1.433732 , 1.4106
12.445963 , 12.4476
20.465258 , 20.5124
10.71338 , 10.7048
10.63854 , 10.6323
5.0859036 , 5.0702
1.7855697 , 1.7741
3.655562 , 3.6564
9.203609 , 9.2249
6.127087 , 6.1303
11.190475 , 11.1922
2.6293879 , 2.6285
149.08507 , 148.1429
12.176535 , 12.1443
5.6371303 , 5.5103
16.120989 , 16.0477
1.7766981 , 1.7582
1.2071238 , 1.2234
1.4974375 , 1.4854
4.228774 , 4.4376
15.19692 , 15.1959
89.373604 , 90.5703
20.342812 , 20.2637
1.8317366 , 1.8012
5.6722145 , 5.6261
24.38383 , 24.4608
25.295378 , 25.2442
7.7260866 , 7.8494
3.364265 , 3.6569
23.57083 , 23.8886
13.805667 , 13.7641
2.831143 , 2.7748
1.471591 , 1.4602
6.291939 , 6.3145
1.2073803 , 1.2104
2.2307706 , 2.2049
9.163487 , 9.2186
8.848605 , 8.7917
11.312489 , 11.3181
8.862507 , 8.9409
332.88187 , 346.6045
1.4204264 , 1.4536
2.5717583 , 2.577
12.597862 , 12.6102
7.9857497 , 8.0293
10.459517 , 10.4634
3.1101475 , 2.9994
7.4750433 , 7.5377
11.402306 , 11.3922
2.1933284 , 2.1986
2.63982 , 2.6482
1.5736151 , 1.576
1.6731863 , 1.6572
20.58031 , 20.7031
2.507175 , 2.515
10.4872465 , 10.5247
4.7261043 , 4.7059
2.8302083 , 2.8109
10.909774 , 10.9138
1.8061781 , 1.7947
9.586603 , 9.5515
0.94810677 , 1.0139
112.8864 , 110.5791
1.8830962 , 1.8711
12.533875 , 12.5419
6.962471 , 6.9252
6.624482 , 6.5775
6.9296827 , 6.9339
11.833746 , 11.8195
2.0022764 , 2.0087
1.3450804 , 1.3351
12.33834 , 12.3323
6.5962086 , 6.5632
7.205735 , 7.1864
321.20923 , 324.5528
12.70891 , 12.7277
12.167083 , 12.3328
11.83893 , 11.8272
1.9373231 , 1.9278
6.3109345 , 6.2643
3.950859 , 3.8658
120.25554 , 121.2312
9.194373 , 9.2421
2.1209574 , 2.0987
19.720812 , 19.6259
1.2804699 , 1.2699
2.4997978 , 2.497
1.203949 , 1.008058
2.2380676 , 2.2494
5.9700184 , 5.9161
36.768196 , 37.1198
1.8384724 , 1.8276
1.5782452 , 1.5713
1.6148295 , 1.6169
1.7782025 , 1.7762
4.8943753 , 4.9741
2.4271464 , 2.4116
2.9117446 , 2.8948
11.583078 , 11.6881
10.518499 , 10.6023
1.8674297 , 1.8476
8.545959 , 8.5526
10.356932 , 10.1873
2.6003017 , 2.6175
10.867577 , 10.8947
8.418685 , 8.4778
21.043732 , 21.2664
7.76643 , 7.7772
1.1774693 , 1.1732
2.2269263 , 2.2183
1.3740664 , 1.367
9.227035 , 9.2592
1.6701837 , 1.6877
7.4147806 , 7.3419
17.517982 , 17.7536
2.3340993 , 2.3355
8.897374 , 8.8903
14.006653 , 13.9762
1.5258818 , 1.5601
28.457275 , 28.2982
33.899456 , 34.0068
9.996696 , 9.9909
1.2545242 , 1.2336
1.7964573 , 1.7224
37.9735 , 38.66
9.761876 , 9.746
3.862136 , 3.9428
8.803685 , 8.845
1.2326355 , 1.2223
1.1927414 , 1.1892
133.27434 , 134.8122
1.4885798 , 1.5139
33.93356 , 34.285
1.1515923 , 1.1606
4.877155 , 4.8833
13.42103 , 13.4647
3.3762102 , 3.4117
8.049253 , 8.1328
1.8977904 , 1.8924
44.508648 , 44.8343
1.7805843 , 1.7725
13.264477 , 13.2509
2.8345847 , 2.8229
9.185009 , 9.2106
1.637289 , 1.6466
10.866944 , 10.8625
1.8170776 , 1.8374
2.2318087 , 2.221
12.959191 , 12.9791
2.9691463 , 2.9582
2.1817722 , 2.1797
1.9494982 , 1.9578
1.4812269 , 1.4777
2.1910238 , 2.19
1.02143 , 1.0469
2.728557 , 2.731
11.371073 , 11.386
10.0928955 , 10.1753
2.2488117 , 2.3487
5.0581183 , 5.1023
29.709217 , 29.5834
12.696985 , 12.7135
24.855263 , 25.4401
2.8083067 , 2.819
13.740467 , 13.7063
3.1589065 , 3.1888
2.3934593 , 2.3936
1.2511873 , 1.2345
10.675258 , 10.7981
6.6676044 , 6.744
6.431363 , 6.3954
6.4876375 , 6.4418
10.849017 , 10.8421
1.8036737 , 1.7833
1.4749298 , 1.4746
36.733177 , 37.2941
24.041977 , 23.8298
10.227535 , 10.2392
7.919996 , 7.9435
0.97771645 , 1.0074
1.4311104 , 1.4071
11.883511 , 11.8705
1.3507853 , 1.3557
13.396584 , 13.1664
1.5514588 , 1.5631
1.6650772 , 1.6383
21.3242 , 21.6715
8.038841 , 8.0863
9.720442 , 9.714
1.2201691 , 1.2181
1.5724163 , 1.5697
2.2608166 , 2.2821
4.754712 , 4.8342
4.1750526 , 4.329
1.4209614 , 1.4636
1.7117772 , 1.73
2.2585535 , 2.2537
1.7278352 , 1.7124
3.0980911 , 3.0165
34.19391 , 34.1665
1.9853511 , 1.9859
1.6144624 , 1.6336
24.115013 , 23.9678
11.347743 , 11.3908
9.855499 , 9.8727
13.991001 , 14.0758
1.1595116 , 1.1629
1.5206461 , 1.516
6.6532807 , 6.6434
1.6395936 , 1.653
1.4507341 , 1.4344
15.94345 , 15.8826
1.8334398 , 1.814
7.5942993 , 7.6202
1.8047829 , 1.7932
2.091845 , 2.082
2.5107918 , 2.5328
7.437541 , 7.418
1.6235919 , 1.6149
1.791873 , 1.8104
1.868021 , 1.8879
1.3365555 , 1.3359
9.215093 , 9.243
5.4880776 , 5.4836
2.0281844 , 2.0077
8.998732 , 8.7339
2.4137 , 2.4055
3.1966105 , 3.2564
18.222485 , 18.2169
8.235888 , 8.2517
12.498034 , 12.474
1.710299 , 1.7166
29.265686 , 28.6943
1.3396854 , 1.3262
1.560967 , 1.5577
1.5145779 , 1.5277
2.7310863 , 2.7969
13.574573 , 13.6514
RMSE:  1.485223995908673  MAPE: 0.04870108407784146
5: ground truth total-  248  predicted total -  248
100: ground truth total-  238  predicted total -  236
 more 100: ground truth total -  10  predicted total -  10



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
1.3418798 , 15.636463
1.1402369 , 4.342653
1.4420452 , 2.007031
1.1442633 , 16.291735
0.9633932 , 0.344199
1.1425924 , 9.395052
0.97085 , 3.666258
1.4667425 , 3.398206
1.263977 , 1.414953
1.0045996 , 3.015366
1.4846344 , 14.229228
1.1165342 , 0.094036
1.3131285 , 5.943754
1.1455593 , 28.659092
1.4131174 , 0.225765
1.2946005 , 0.151563
0.9971409 , 0.279413
1.1241293 , 0.042445
1.4911823 , 8.939355
RMSE:  8.972730544064534  MAPE: 3.305933216393599
