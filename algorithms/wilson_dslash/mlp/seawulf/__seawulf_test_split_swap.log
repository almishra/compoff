['Reduction', 'div_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
2059
19
<class 'numpy.dtype'> float64
2059
train batches:  93  validate samples: 164  test samples: 411
Epoch [0/150], Batch loss: 92.681
Epoch: 0 RMSE:  37.68302065097713  MAPE: 1.5733960757649599  L2+L1 loss: 11.912
Epoch [1/150], Batch loss: 85.924
Epoch: 1 RMSE:  36.9860883762919  MAPE: 2.9399609365266937  L2+L1 loss: 12.277
Epoch [2/150], Batch loss: 86.928
Epoch: 2 RMSE:  36.72128077490464  MAPE: 3.908875046763714  L2+L1 loss: 13.327
Epoch [3/150], Batch loss: 85.933
Epoch: 3 RMSE:  36.645675432037095  MAPE: 4.369324253023627  L2+L1 loss: 13.944
Epoch [4/150], Batch loss: 89.295
Epoch: 4 RMSE:  36.64074418560327  MAPE: 4.407870283622671  L2+L1 loss: 14.002
Epoch [5/150], Batch loss: 90.746
Epoch: 5 RMSE:  36.61115046720027  MAPE: 4.686498356467644  L2+L1 loss: 14.422
Epoch [6/150], Batch loss: 107.124
Epoch: 6 RMSE:  36.63397471403341  MAPE: 4.463602469590283  L2+L1 loss: 14.085
Epoch [7/150], Batch loss: 89.125
Epoch: 7 RMSE:  36.619545082990044  MAPE: 4.596644017305731  L2+L1 loss: 14.287
Epoch [8/150], Batch loss: 84.27
Epoch: 8 RMSE:  36.61581186739842  MAPE: 4.635175185302327  L2+L1 loss: 14.345
Epoch [9/150], Batch loss: 86.552
Epoch: 9 RMSE:  121.33286716227057  MAPE: 91.6527144519169  L2+L1 loss: 37.893
Epoch [10/150], Batch loss: 85.482
Epoch: 10 RMSE:  36.61368957784333  MAPE: 4.658058490954021  L2+L1 loss: 14.379
Epoch [11/150], Batch loss: 87.598
Epoch: 11 RMSE:  36.61700365893448  MAPE: 4.622647637350732  L2+L1 loss: 14.326
Epoch [12/150], Batch loss: 84.6
Epoch: 12 RMSE:  36.64178168402573  MAPE: 4.399623667581858  L2+L1 loss: 13.99
Epoch [13/150], Batch loss: 89.498
Epoch: 13 RMSE:  36.633756208124026  MAPE: 4.4654652287047485  L2+L1 loss: 14.088
Epoch [14/150], Batch loss: 88.939
Epoch: 14 RMSE:  36.627043999172976  MAPE: 4.52477091844987  L2+L1 loss: 14.179
Epoch [15/150], Batch loss: 94.001
Epoch: 15 RMSE:  36.64664291908836  MAPE: 4.361954991104631  L2+L1 loss: 13.933
Epoch [16/150], Batch loss: 111.09
Epoch: 16 RMSE:  18.866008423126537  MAPE: 6.01994574825198  L2+L1 loss: 15.079
Epoch [17/150], Batch loss: 89.546
Epoch: 17 RMSE:  36.61826801357219  MAPE: 4.609594179267189  L2+L1 loss: 14.307
Epoch [18/150], Batch loss: 88.837
Epoch: 18 RMSE:  36.624115547462466  MAPE: 4.552052238660592  L2+L1 loss: 14.22
Epoch [19/150], Batch loss: 86.904
Epoch: 19 RMSE:  36.625237752566726  MAPE: 4.541486888414021  L2+L1 loss: 14.204
Epoch [20/150], Batch loss: 88.017
Epoch: 20 RMSE:  36.613792977983735  MAPE: 4.656925662527989  L2+L1 loss: 14.378
Epoch [21/150], Batch loss: 86.696
Epoch: 21 RMSE:  491.0136047548372  MAPE: 359.2725321373221  L2+L1 loss: 88.922
Epoch [22/150], Batch loss: 101.467
Epoch: 22 RMSE:  36.59696278907691  MAPE: 5.798073892619119  L2+L1 loss: 16.197
Epoch [23/150], Batch loss: 91.667
Epoch: 23 RMSE:  36.58676755378694  MAPE: 5.096267138660202  L2+L1 loss: 15.06
Epoch [24/150], Batch loss: 87.458
Epoch: 24 RMSE:  36.600495674430775  MAPE: 4.823059977250679  L2+L1 loss: 14.628
Epoch [25/150], Batch loss: 88.657
Epoch: 25 RMSE:  36.606834571417984  MAPE: 4.737947540329613  L2+L1 loss: 14.498
Epoch [26/150], Batch loss: 87.255
Epoch: 26 RMSE:  36.60664449121886  MAPE: 4.74031606468915  L2+L1 loss: 14.502
Epoch [27/150], Batch loss: 88.846
Epoch: 27 RMSE:  36.612660223355924  MAPE: 4.669441209270378  L2+L1 loss: 14.396
Epoch [28/150], Batch loss: 89.1
Epoch: 28 RMSE:  36.61275576219561  MAPE: 4.668376530568046  L2+L1 loss: 14.395
Epoch [29/150], Batch loss: 88.546
Epoch: 29 RMSE:  36.607223883925585  MAPE: 4.733125625916878  L2+L1 loss: 14.491
Epoch [30/150], Batch loss: 90.359
Epoch: 30 RMSE:  36.60724483974595  MAPE: 4.732867171304354  L2+L1 loss: 14.491
Epoch [31/150], Batch loss: 90.208
Epoch: 31 RMSE:  36.6079433114085  MAPE: 4.724315452516541  L2+L1 loss: 14.478
Epoch [32/150], Batch loss: 90.962
Epoch: 32 RMSE:  36.60908316811156  MAPE: 4.710611786062854  L2+L1 loss: 14.458
Epoch [33/150], Batch loss: 86.145
Epoch: 33 RMSE:  36.60938840818454  MAPE: 4.706992992872922  L2+L1 loss: 14.452
Epoch [34/150], Batch loss: 87.931
Epoch: 34 RMSE:  36.61069453235513  MAPE: 4.691738598744099  L2+L1 loss: 14.43
Epoch [35/150], Batch loss: 90.784
Epoch: 35 RMSE:  36.61026912934087  MAPE: 4.696666809581221  L2+L1 loss: 14.437
Epoch [36/150], Batch loss: 89.169
Epoch: 36 RMSE:  36.61068517371041  MAPE: 4.6918466096269436  L2+L1 loss: 14.43
Epoch [37/150], Batch loss: 87.224
Epoch: 37 RMSE:  36.61116514709926  MAPE: 4.686330339538774  L2+L1 loss: 14.422
Epoch [38/150], Batch loss: 89.165
Epoch: 38 RMSE:  36.611599503625705  MAPE: 4.681378554897854  L2+L1 loss: 14.414
Epoch [39/150], Batch loss: 93.112
Epoch: 39 RMSE:  36.401542856957285  MAPE: 4.691907982392983  L2+L1 loss: 13.375
Epoch [40/150], Batch loss: 90.48
Epoch: 40 RMSE:  36.6119126464439  MAPE: 4.7239293457774965  L2+L1 loss: 14.484
Epoch [41/150], Batch loss: 88.625
Epoch: 41 RMSE:  36.421711362005574  MAPE: 4.246429698017383  L2+L1 loss: 13.345
Epoch [42/150], Batch loss: 79.303
Epoch: 42 RMSE:  35.86649410683808  MAPE: 2.196682913788841  L2+L1 loss: 9.946
Epoch [43/150], Batch loss: 79.821
Epoch: 43 RMSE:  35.751475839903165  MAPE: 2.054606407933253  L2+L1 loss: 9.339
Epoch [44/150], Batch loss: 75.832
Epoch: 44 RMSE:  35.608701188935406  MAPE: 1.8561242688387822  L2+L1 loss: 8.912
Epoch [45/150], Batch loss: 75.886
Epoch: 45 RMSE:  35.527942803472165  MAPE: 1.9128982738854647  L2+L1 loss: 9.077
Epoch [46/150], Batch loss: 75.322
Epoch: 46 RMSE:  35.428123574405404  MAPE: 1.888007786058362  L2+L1 loss: 8.607
Epoch [47/150], Batch loss: 70.401
Epoch: 47 RMSE:  35.34781240329729  MAPE: 1.8958398899701001  L2+L1 loss: 8.474
Epoch [48/150], Batch loss: 73.425
Epoch: 48 RMSE:  35.264878986078664  MAPE: 1.9114552148723507  L2+L1 loss: 8.399
Epoch [49/150], Batch loss: 73.766
Epoch: 49 RMSE:  35.21531254425361  MAPE: 2.3308058499063313  L2+L1 loss: 9.295
Epoch [50/150], Batch loss: 69.341
Epoch: 50 RMSE:  35.10656625781577  MAPE: 2.073206778563397  L2+L1 loss: 8.402
Epoch [51/150], Batch loss: 72.953
Epoch: 51 RMSE:  35.088940377130875  MAPE: 2.6758141603947503  L2+L1 loss: 9.436
Epoch [52/150], Batch loss: 68.53
Epoch: 52 RMSE:  34.947559016891354  MAPE: 2.0295790725253564  L2+L1 loss: 8.031
Epoch [53/150], Batch loss: 72.729
Epoch: 53 RMSE:  34.81310774295462  MAPE: 1.0842139998340028  L2+L1 loss: 8.551
Epoch [54/150], Batch loss: 71.669
Epoch: 54 RMSE:  34.70466634612044  MAPE: 0.3409065375466066  L2+L1 loss: 7.57
Epoch [55/150], Batch loss: 72.162
Epoch: 55 RMSE:  37.93438849915884  MAPE: 13.634709204558833  L2+L1 loss: 10.156
Epoch [56/150], Batch loss: 74.292
Epoch: 56 RMSE:  53.38040352693062  MAPE: 37.094786671957614  L2+L1 loss: 14.944
Epoch [57/150], Batch loss: 41.977
Epoch: 57 RMSE:  12.844740868223441  MAPE: 2.3174898279221083  L2+L1 loss: 5.415
Epoch [58/150], Batch loss: 32.672
Epoch: 58 RMSE:  10.487378985018653  MAPE: 3.4481313902998596  L2+L1 loss: 4.753
Epoch [59/150], Batch loss: 31.004
Epoch: 59 RMSE:  14.888215977384515  MAPE: 7.795309248916607  L2+L1 loss: 5.931
Epoch [60/150], Batch loss: 24.236
Epoch: 60 RMSE:  7.937556154802561  MAPE: 0.23000416353711936  L2+L1 loss: 2.856
Epoch [61/150], Batch loss: 19.705
Epoch: 61 RMSE:  7.9088182069618105  MAPE: 2.186435134315396  L2+L1 loss: 3.043
Epoch [62/150], Batch loss: 21.804
Epoch: 62 RMSE:  7.259640291397273  MAPE: 0.9018251094076964  L2+L1 loss: 2.713
Epoch [63/150], Batch loss: 18.874
Epoch: 63 RMSE:  6.957411589694677  MAPE: 0.5740035938064507  L2+L1 loss: 2.601
Epoch [64/150], Batch loss: 19.142
Epoch: 64 RMSE:  7.324368100828821  MAPE: 2.1516855341246517  L2+L1 loss: 2.943
Epoch [65/150], Batch loss: 19.222
Epoch: 65 RMSE:  7.1430485264710075  MAPE: 0.6346234846031369  L2+L1 loss: 2.675
Epoch [66/150], Batch loss: 17.339
Epoch: 66 RMSE:  7.468010693918187  MAPE: 2.4332983752366824  L2+L1 loss: 2.767
Epoch [67/150], Batch loss: 16.785
Epoch: 67 RMSE:  6.444120405907096  MAPE: 1.062997222504226  L2+L1 loss: 2.476
Epoch [68/150], Batch loss: 16.725
Epoch: 68 RMSE:  6.006542409251075  MAPE: 1.1378208886385457  L2+L1 loss: 2.372
Epoch [69/150], Batch loss: 15.704
Epoch: 69 RMSE:  5.6323411596453505  MAPE: 1.0013641109815505  L2+L1 loss: 2.334
Epoch [70/150], Batch loss: 15.433
Epoch: 70 RMSE:  5.862784085091075  MAPE: 0.5729662982635366  L2+L1 loss: 2.08
Epoch [71/150], Batch loss: 15.566
Epoch: 71 RMSE:  5.122976064419076  MAPE: 0.3067314918952684  L2+L1 loss: 1.954
Epoch [72/150], Batch loss: 14.719
Epoch: 72 RMSE:  4.697658212904273  MAPE: 0.5074367838318676  L2+L1 loss: 1.853
Epoch [73/150], Batch loss: 13.759
Epoch: 73 RMSE:  5.305522179932185  MAPE: 0.5593528612846187  L2+L1 loss: 1.791
Epoch [74/150], Batch loss: 13.338
Epoch: 74 RMSE:  4.21619504543232  MAPE: 0.5291969070996008  L2+L1 loss: 1.812
Epoch [75/150], Batch loss: 12.779
Epoch: 75 RMSE:  4.062768465564135  MAPE: 1.7221885874090461  L2+L1 loss: 1.768
Epoch [76/150], Batch loss: 12.127
Epoch: 76 RMSE:  3.8555748719076766  MAPE: 0.8135584491827146  L2+L1 loss: 1.52
Epoch [77/150], Batch loss: 10.826
Epoch: 77 RMSE:  3.2796619673424283  MAPE: 0.24265258408019572  L2+L1 loss: 1.314
Epoch [78/150], Batch loss: 10.636
Epoch: 78 RMSE:  4.801407929958054  MAPE: 1.633472568333765  L2+L1 loss: 1.561
Epoch [79/150], Batch loss: 10.334
Epoch: 79 RMSE:  2.7560626222458433  MAPE: 0.6852180881226353  L2+L1 loss: 1.283
Epoch [80/150], Batch loss: 8.164
Epoch: 80 RMSE:  3.7338673692695394  MAPE: 0.8201277097359361  L2+L1 loss: 1.322
Epoch [81/150], Batch loss: 8.592
Epoch: 81 RMSE:  2.966235157477669  MAPE: 0.9395456939330594  L2+L1 loss: 1.608
Epoch [82/150], Batch loss: 7.901
Epoch: 82 RMSE:  2.7928693155372986  MAPE: 0.25890583520043164  L2+L1 loss: 0.941
Epoch [83/150], Batch loss: 7.652
Epoch: 83 RMSE:  2.0979196222298446  MAPE: 0.18386406598668917  L2+L1 loss: 1.185
Epoch [84/150], Batch loss: 7.655
Epoch: 84 RMSE:  2.969011881235033  MAPE: 1.6896848523754189  L2+L1 loss: 1.294
Epoch [85/150], Batch loss: 6.131
Epoch: 85 RMSE:  2.4887295009302237  MAPE: 0.2340040976864975  L2+L1 loss: 1.448
Epoch [86/150], Batch loss: 7.219
Epoch: 86 RMSE:  2.7700660652877533  MAPE: 0.9055024746768479  L2+L1 loss: 1.452
Epoch [87/150], Batch loss: 5.838
Epoch: 87 RMSE:  2.0869046640258393  MAPE: 1.3669163957926203  L2+L1 loss: 1.139
Epoch [88/150], Batch loss: 5.557
Epoch: 88 RMSE:  3.1631153822190834  MAPE: 1.5221837476885451  L2+L1 loss: 1.344
Epoch [89/150], Batch loss: 6.237
Epoch: 89 RMSE:  1.4597539878382142  MAPE: 0.40579635947747517  L2+L1 loss: 1.25
Epoch [90/150], Batch loss: 4.319
Epoch: 90 RMSE:  1.1476899183108957  MAPE: 0.09113550587131086  L2+L1 loss: 0.694
Epoch [91/150], Batch loss: 3.9
Epoch: 91 RMSE:  1.1952448894013492  MAPE: 0.16976903068773908  L2+L1 loss: 0.725
Epoch [92/150], Batch loss: 3.705
Epoch: 92 RMSE:  1.2203123547458599  MAPE: 0.0784818240656226  L2+L1 loss: 0.725
Epoch [93/150], Batch loss: 3.701
Epoch: 93 RMSE:  1.2608816475962268  MAPE: 0.2057535420370833  L2+L1 loss: 0.749
Epoch [94/150], Batch loss: 3.552
Epoch: 94 RMSE:  1.2123430357545961  MAPE: 0.13996564722961338  L2+L1 loss: 0.752
Epoch [95/150], Batch loss: 3.664
Epoch: 95 RMSE:  1.1812043834385033  MAPE: 0.1515484294695223  L2+L1 loss: 0.728
Epoch [96/150], Batch loss: 3.618
Epoch: 96 RMSE:  1.6278158089690382  MAPE: 0.495633246021805  L2+L1 loss: 0.899
Epoch [97/150], Batch loss: 3.676
Epoch: 97 RMSE:  1.222537344252866  MAPE: 0.10532933972186925  L2+L1 loss: 0.722
Epoch [98/150], Batch loss: 3.49
Epoch: 98 RMSE:  1.1847129917323307  MAPE: 0.1124386343997926  L2+L1 loss: 0.737
Epoch [99/150], Batch loss: 3.54
Epoch: 99 RMSE:  1.1177901082407902  MAPE: 0.2747909479455198  L2+L1 loss: 0.689
Epoch [100/150], Batch loss: 3.395
Epoch: 100 RMSE:  1.2736924689529225  MAPE: 0.21017833174104228  L2+L1 loss: 0.764
Epoch [101/150], Batch loss: 3.599
Epoch: 101 RMSE:  1.1250032420167555  MAPE: 0.22059113572451192  L2+L1 loss: 0.711
Epoch [102/150], Batch loss: 3.416
Epoch: 102 RMSE:  1.2295252372166405  MAPE: 0.3262782289367654  L2+L1 loss: 0.785
Epoch [103/150], Batch loss: 3.392
Epoch: 103 RMSE:  1.184566525628224  MAPE: 0.18460831196517766  L2+L1 loss: 0.723
Epoch [104/150], Batch loss: 3.441
Epoch: 104 RMSE:  1.2286162340659987  MAPE: 0.32907305456626396  L2+L1 loss: 0.762
Epoch [105/150], Batch loss: 3.385
Epoch: 105 RMSE:  1.3769044909301995  MAPE: 0.5206835219202743  L2+L1 loss: 0.814
Epoch [106/150], Batch loss: 3.212
Epoch: 106 RMSE:  1.336644254937316  MAPE: 0.08213764468782701  L2+L1 loss: 0.777
Epoch [107/150], Batch loss: 3.365
Epoch: 107 RMSE:  1.2326932498290977  MAPE: 0.4558216475889498  L2+L1 loss: 0.779
Epoch [108/150], Batch loss: 3.447
Epoch: 108 RMSE:  1.2360347119525648  MAPE: 0.1771554165273797  L2+L1 loss: 0.75
Epoch [109/150], Batch loss: 3.302
Epoch: 109 RMSE:  1.0723582991449818  MAPE: 0.21738477976631682  L2+L1 loss: 0.702
Epoch [110/150], Batch loss: 3.379
Epoch: 110 RMSE:  1.0997453793055645  MAPE: 0.10146388999985054  L2+L1 loss: 0.705
Epoch [111/150], Batch loss: 3.235
Epoch: 111 RMSE:  1.0715313705649503  MAPE: 0.26894819483318644  L2+L1 loss: 0.67
Epoch [112/150], Batch loss: 3.254
Epoch: 112 RMSE:  1.2053995660284313  MAPE: 0.5062645566040287  L2+L1 loss: 0.781
Epoch [113/150], Batch loss: 3.203
Epoch: 113 RMSE:  1.0885367814725568  MAPE: 0.13613902871750652  L2+L1 loss: 0.733
Epoch [114/150], Batch loss: 3.167
Epoch: 114 RMSE:  1.1194448881992538  MAPE: 0.13479050497226724  L2+L1 loss: 0.7
Epoch [115/150], Batch loss: 3.07
Epoch: 115 RMSE:  1.124242012824096  MAPE: 0.26255835548471723  L2+L1 loss: 0.736
Epoch [116/150], Batch loss: 3.107
Epoch: 116 RMSE:  1.1939268342684521  MAPE: 0.35315321616488077  L2+L1 loss: 0.798
Epoch [117/150], Batch loss: 3.108
Epoch: 117 RMSE:  1.0672391130859773  MAPE: 0.2604958198287467  L2+L1 loss: 0.644
Epoch [118/150], Batch loss: 3.092
Epoch: 118 RMSE:  1.0780348395965926  MAPE: 0.2686902140164687  L2+L1 loss: 0.697
Epoch [119/150], Batch loss: 3.1
Epoch: 119 RMSE:  1.235664086952774  MAPE: 0.5024588849282916  L2+L1 loss: 0.783
Epoch [120/150], Batch loss: 2.888
Epoch: 120 RMSE:  1.07882074129569  MAPE: 0.24541242169427968  L2+L1 loss: 0.688
Epoch [121/150], Batch loss: 2.927
Epoch: 121 RMSE:  1.0612699716794827  MAPE: 0.2092805740368984  L2+L1 loss: 0.679
Epoch [122/150], Batch loss: 2.836
Epoch: 122 RMSE:  1.0562707179187738  MAPE: 0.22702355527697524  L2+L1 loss: 0.675
Epoch [123/150], Batch loss: 2.842
Epoch: 123 RMSE:  1.0423240623904237  MAPE: 0.17037027723362735  L2+L1 loss: 0.675
Epoch [124/150], Batch loss: 2.902
Epoch: 124 RMSE:  1.0410251133570705  MAPE: 0.19708298327297025  L2+L1 loss: 0.671
Epoch [125/150], Batch loss: 2.901
Epoch: 125 RMSE:  1.065932066493153  MAPE: 0.26803877674339177  L2+L1 loss: 0.686
Epoch [126/150], Batch loss: 2.896
Epoch: 126 RMSE:  1.053563290567937  MAPE: 0.23746344547501413  L2+L1 loss: 0.676
Epoch [127/150], Batch loss: 2.868
Epoch: 127 RMSE:  1.0717357615204315  MAPE: 0.29339178024016654  L2+L1 loss: 0.694
Epoch [128/150], Batch loss: 2.944
Epoch: 128 RMSE:  1.0582858722376363  MAPE: 0.2625285821470447  L2+L1 loss: 0.682
Epoch [129/150], Batch loss: 2.879
Epoch: 129 RMSE:  1.0478293504723104  MAPE: 0.2263610439072959  L2+L1 loss: 0.676
Epoch [130/150], Batch loss: 2.865
Epoch: 130 RMSE:  1.0616746409713158  MAPE: 0.25637329065608483  L2+L1 loss: 0.688
Epoch [131/150], Batch loss: 2.905
Epoch: 131 RMSE:  1.0716860118998353  MAPE: 0.2793020101891729  L2+L1 loss: 0.693
Epoch [132/150], Batch loss: 2.793
Epoch: 132 RMSE:  1.0380785378488737  MAPE: 0.17986965018593956  L2+L1 loss: 0.677
Epoch [133/150], Batch loss: 2.899
Epoch: 133 RMSE:  1.0594261729489467  MAPE: 0.25900511473721477  L2+L1 loss: 0.684
Epoch [134/150], Batch loss: 2.9
Epoch: 134 RMSE:  1.0375659760320322  MAPE: 0.20072320033342403  L2+L1 loss: 0.675
Epoch [135/150], Batch loss: 2.871
Epoch: 135 RMSE:  1.0559552471969478  MAPE: 0.2669924909749069  L2+L1 loss: 0.686
Epoch [136/150], Batch loss: 2.902
Epoch: 136 RMSE:  1.0596986883928379  MAPE: 0.2683536683066932  L2+L1 loss: 0.691
Epoch [137/150], Batch loss: 2.914
Epoch: 137 RMSE:  1.039836750432011  MAPE: 0.16355138418067722  L2+L1 loss: 0.677
Epoch [138/150], Batch loss: 2.942
Epoch: 138 RMSE:  1.0494540641639991  MAPE: 0.2569259037543293  L2+L1 loss: 0.678
Epoch [139/150], Batch loss: 2.843
Epoch: 139 RMSE:  1.0433808436745569  MAPE: 0.208573153378691  L2+L1 loss: 0.676
Epoch [140/150], Batch loss: 2.908
Epoch: 140 RMSE:  1.0464657803170088  MAPE: 0.21708637790567478  L2+L1 loss: 0.677
Epoch [141/150], Batch loss: 2.874
Epoch: 141 RMSE:  1.0450130842597076  MAPE: 0.2419708662522021  L2+L1 loss: 0.675
Epoch [142/150], Batch loss: 2.851
Epoch: 142 RMSE:  1.0491321134952452  MAPE: 0.2713652073419245  L2+L1 loss: 0.676
Epoch [143/150], Batch loss: 2.86
Epoch: 143 RMSE:  1.0421878045329425  MAPE: 0.22490389598834964  L2+L1 loss: 0.673
Epoch [144/150], Batch loss: 2.942
Epoch: 144 RMSE:  1.0433996428211514  MAPE: 0.22868770504568953  L2+L1 loss: 0.677
Epoch [145/150], Batch loss: 2.857
Epoch: 145 RMSE:  1.058605830888817  MAPE: 0.2668064435053889  L2+L1 loss: 0.69
Epoch [146/150], Batch loss: 2.872
Epoch: 146 RMSE:  1.034812229512546  MAPE: 0.17365939129772878  L2+L1 loss: 0.677
Epoch [147/150], Batch loss: 2.859
Epoch: 147 RMSE:  1.047812710812966  MAPE: 0.241948532280428  L2+L1 loss: 0.683
Epoch [148/150], Batch loss: 2.873
Epoch: 148 RMSE:  1.0519999973978456  MAPE: 0.2441322828543915  L2+L1 loss: 0.684
Epoch [149/150], Batch loss: 2.898
Epoch: 149 RMSE:  1.0459914402423376  MAPE: 0.23618572353125852  L2+L1 loss: 0.677


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
1.9184866 , 1.9179
1.5451927 , 1.5094
245.14539 , 244.4858
20.1114 , 20.7374
9.58134 , 9.2968
1.1884289 , 1.1931
9.084934 , 9.3095
25.350323 , 23.5126
7.9024677 , 7.7773
1.4498882 , 1.5554
2.2993631 , 1.9429
6.780308 , 6.8458
9.907127 , 9.7481
1.088417 , 1.0905
5.149437 , 5.1217
1.137003 , 1.0116
1.6484451 , 1.6268
3.1141539 , 2.763
4.054097 , 4.8437
9.329512 , 9.1929
5.2527113 , 5.5101
1.3654842 , 1.4449
2.2142572 , 1.9499
20.497995 , 19.8521
2.4879131 , 2.5344
9.095485 , 9.4317
9.356066 , 9.2204
1.1188354 , 1.1002
2.1834936 , 1.9147
6.0394106 , 6.8008
1.3049183 , 1.3546
13.5762205 , 13.1952
8.036226 , 7.8512
1.0656261 , 1.0512
11.529342 , 11.4398
2.7603512 , 2.7742
1.1311302 , 1.1225
9.786491 , 9.6203
5.848283 , 5.9323
9.6821995 , 9.2091
9.612678 , 9.5491
2.4221287 , 2.0154
9.582943 , 9.6236
12.500673 , 12.2698
9.280657 , 9.3949
1.2949276 , 1.3565
10.916414 , 11.0823
6.8267527 , 7.1256
1.1037827 , 1.0957
1.0372505 , 1.0169
5.741003 , 5.9097
2.155754 , 1.2663
28.377731 , 28.6528
12.585043 , 12.4748
1.5843353 , 1.7729
1.2687454 , 1.2462
0.0 , 0.311502
7.9117413 , 8.0698
7.3078737 , 7.2685
12.224331 , 11.9209
1.7254229 , 1.707
8.813268 , 8.9599
2.071041 , 1.8524
1.3350353 , 1.3941
0.0 , 1.058
9.419477 , 9.6362
1.1503506 , 1.1194
2.3346558 , 2.2306
2.2738438 , 2.0363
2.6889477 , 2.4878
3.3329697 , 4.1211
1.1141567 , 1.0872
19.371418 , 19.7035
1.0730953 , 1.0169
1.2720528 , 1.2429
28.106155 , 27.7509
8.116722 , 8.0312
2.532772 , 2.2121
5.235055 , 5.553
4.0835686 , 4.264
2.7620897 , 2.6755
12.396645 , 12.04
7.801035 , 7.4946
1.3861694 , 1.4286
7.5438166 , 7.2082
8.954272 , 8.8951
1.3271809 , 1.3668
11.009102 , 11.155
1.2563477 , 1.2741
7.200554 , 7.7041
8.254715 , 8.1327
5.6065493 , 6.1799
8.326933 , 8.2838
1.103117 , 1.08
2.2426682 , 2.4635
5.2671432 , 6.3899
7.990615 , 7.8992
1.7517815 , 1.728
1.1338654 , 1.1733
3.8107948 , 4.2769
1.221138 , 1.2029
10.714563 , 10.7858
5.389043 , 5.5693
17.02532 , 16.9554
20.981598 , 20.9332
4.330785 , 3.6251
1.8302698 , 1.7113
5.753793 , 5.7721
9.712658 , 9.9941
1.3318424 , 1.3757
4.847971 , 4.0232
1.1882725 , 1.2864
8.150835 , 8.0265
4.10478 , 4.0369
2.6242476 , 2.5655
37.21659 , 38.1666
1.618763 , 1.647
5.7775097 , 5.7782
1.1512508 , 1.1588
8.884984 , 9.0117
11.287448 , 11.2174
1.399374 , 1.4583
16.92058 , 16.9965
1.8769484 , 1.6716
1.0597744 , 1.1113
9.267133 , 9.1713
1.5860386 , 1.6195
10.050257 , 9.9611
9.324693 , 9.1605
18.99709 , 19.6912
1.1362476 , 1.081
12.20297 , 12.0015
7.3968897 , 7.4297
1.8096027 , 1.633
7.249421 , 7.2535
20.98808 , 20.7412
5.6731462 , 6.1256
1.351675 , 1.321
8.955588 , 8.2931
2.035285 , 1.8652
8.689662 , 8.5851
1.1346054 , 1.0519
4.3856096 , 4.1396
1.3148422 , 1.3793
12.181703 , 12.1057
1.8027687 , 1.7369
6.3429193 , 6.9353
1.3040028 , 1.359
8.727559 , 8.7718
1.217493 , 1.1903
1.0578442 , 1.0547
4.109951 , 4.2456
1.6191864 , 1.6272
9.293534 , 9.1684
2.9929905 , 2.9526
74.35355 , 78.4878
4.4482203 , 4.3415
1.1162891 , 1.0886
4.585188 , 5.2328
1.3001404 , 1.2693
5.629328 , 5.3349
24.199436 , 23.5391
12.632105 , 12.5865
1.3583603 , 1.4247
4.212553 , 4.1801
8.867779 , 8.8905
66.955605 , 70.172
8.254809 , 8.3683
13.802519 , 13.2755
24.344814 , 24.476
5.0974674 , 5.2334
1.0748196 , 1.0827
2.1266994 , 1.8636
14.443768 , 14.1874
10.25274 , 10.2589
1.0043716 , 1.0331
24.748234 , 23.9216
1.058569 , 1.1013
1.157896 , 1.1044
13.928295 , 13.5504
17.650246 , 17.7519
16.240557 , 17.1301
7.6030045 , 6.9519
18.725473 , 20.2622
257.30054 , 258.3909
1.0751572 , 1.0068
5.739235 , 5.9195
1.2183552 , 1.2104
7.0355473 , 6.5821
8.430004 , 8.5889
17.52379 , 17.632
7.8951006 , 7.9334
20.525105 , 20.7407
11.226656 , 11.0296
4.969907 , 5.0938
1.1729794 , 1.2229
10.049355 , 9.8866
1.4228897 , 1.4966
3.2192783 , 4.027843
1.241724 , 1.299
32.903076 , 32.4053
6.3899355 , 6.9539
27.75341 , 28.0384
7.2627172 , 7.0991
1.1882458 , 1.1551
9.311919 , 9.4377
8.336077 , 8.1717
28.609127 , 27.5489
4.147979 , 2.4849
2.2212753 , 2.0654
2.1115265 , 1.9428
12.960733 , 12.9926
1.1383877 , 1.0831
1.6594315 , 1.7495
5.165118 , 5.611
4.612154 , 4.7924
3.277585 , 3.4429
3.6712112 , 4.3406
14.346518 , 14.1995
2.0518818 , 2.6172
10.800237 , 10.7827
1.951869 , 1.9138
12.096852 , 11.926
5.4499006 , 4.8338
3.1361923 , 2.6107
22.310753 , 23.0239
10.7035265 , 10.6738
4.115864 , 5.7859
10.15572 , 10.1648
5.2684975 , 4.2047
6.6360264 , 5.213969
2.730029 , 2.2798
1.1869698 , 1.14
8.827082 , 8.6831
10.796857 , 10.7448
4.680599 , 5.6778
8.224523 , 8.2321
1.270956 , 1.3402
6.2529545 , 6.1947
16.26494 , 15.9479
1.0541363 , 1.0021
97.781586 , 93.7982
1.0662422 , 1.0291
1.3516064 , 1.4017
1.1925087 , 1.2359
1.1903057 , 1.1507
7.965788 , 7.8023
1.3953037 , 1.411
13.276417 , 13.1679
1.6449394 , 1.5675
3.0452938 , 2.9723
1.4659138 , 1.4921
1.2155514 , 1.2246
5.850978 , 5.5283
1.2733154 , 1.321
1.8145514 , 1.7913
7.0167084 , 6.7853
1.2843609 , 1.3242
2.3145847 , 3.0188
5.5366745 , 5.5729
32.198944 , 31.1271
2.177329 , 1.8888
3.3921251 , 4.2797
8.999399 , 8.8437
7.2699146 , 7.2144
6.289777 , 6.036
8.30997 , 8.4818
7.1427517 , 6.9129
1.2220898 , 1.1726
8.118477 , 7.8525
23.017931 , 22.4039
8.94902 , 8.8279
6.9072475 , 6.9505
92.81258 , 93.7605
1.242794 , 1.28
9.701365 , 9.6303
1.2016754 , 1.0605
1.1086445 , 1.0373
24.666027 , 24.5365
14.971631 , 14.8673
10.423838 , 10.0977
1.1120701 , 1.0609
11.391209 , 11.5173
1.7462769 , 1.5356
1.9525795 , 2.1405
2.310546 , 2.1793
1.13484 , 1.061
13.2581 , 12.8039
1.9182587 , 1.8183
1.3557606 , 1.3818
11.552822 , 11.7141
11.104408 , 11.0879
1.0118504 , 1.0142
5.112623 , 5.8973
2.7509308 , 3.0711
4.0865 , 3.7585
7.7854824 , 7.4538
6.923373 , 7.2684
2.7712927 , 2.4776
18.623016 , 19.2598
12.888454 , 12.7578
11.716255 , 11.5108
1.6959019 , 1.6951
7.299718 , 7.148
6.9960537 , 6.9551
2.7196236 , 2.5955
1.112215 , 1.155
21.71411 , 21.15
23.716433 , 23.9247
10.689801 , 10.7164
5.306945 , 6.1987
5.8281775 , 5.9846
24.441446 , 24.5875
1.0908718 , 1.1396
8.426723 , 8.2312
8.215026 , 8.4389
9.830192 , 10.1407
1.4168377 , 1.294
8.092652 , 8.2
1.1520939 , 1.0685
13.139652 , 13.0467
2.37109 , 2.9124
1.3896027 , 1.3972
8.122801 , 7.8377
1.3350887 , 1.3484
19.866007 , 19.0803
1.136467 , 1.158
2.0818377 , 1.5773
4.846074 , 4.9584
4.752885 , 5.5423
12.275621 , 11.9353
11.325048 , 12.2906
9.138767 , 9.3785
11.141212 , 11.084
9.160073 , 9.5127
6.9910297 , 6.9146
1.289938 , 1.3693
14.5309925 , 14.35
9.903122 , 9.7818
2.3314657 , 2.3113
21.90237 , 21.7515
5.730794 , 5.835
1.4730206 , 1.6499
8.151459 , 7.8304
1.1610737 , 1.1732
2.933423 , 2.9224
3.8408737 , 3.847
4.39159 , 3.6169
2.663391 , 2.6187
18.243628 , 18.4566
15.349636 , 15.0697
6.315711 , 6.904
11.033733 , 10.9944
1.4535313 , 1.503
8.627696 , 8.8117
7.333844 , 7.0837
1.1778011 , 1.1898
5.075035 , 4.5619
1.3745766 , 1.3977
16.59166 , 16.8509
1.2695312 , 1.2943
8.63741 , 8.7526
2.0518217 , 2.4864
8.312247 , 8.3828
5.7232685 , 6.2009
4.9211826 , 0.299123
1.4082279 , 1.4985
6.6920414 , 6.9202
8.917525 , 8.6225
2.4103642 , 3.4239
14.315886 , 13.8846
9.793917 , 9.7428
1.2546921 , 1.2863
12.122092 , 11.9208
9.674537 , 9.394
16.146152 , 16.1532
1.1525555 , 1.1507
3.4764805 , 6.302282
3.8431664 , 3.7415
8.940668 , 9.1658
6.69786 , 6.8386
3.2229366 , 3.1497
8.660692 , 8.7355
5.9002256 , 5.9809
9.883332 , 9.7694
2.6192904 , 2.5684
7.694316 , 7.5768
4.0964003 , 3.3787
7.2376356 , 7.1357
9.454083 , 9.3259
1.4865379 , 1.5388
11.1384325 , 11.1123
18.990002 , 19.5255
6.67293 , 6.7339
10.89818 , 10.4544
2.0518818 , 3.0899
15.673576 , 15.5747
10.407324 , 10.8351
1.0248661 , 1.0527
5.817277 , 5.833
1.3405628 , 1.4207
9.85405 , 9.7905
1.4594746 , 1.4957
14.758596 , 14.3232
2.1127481 , 1.961
7.779269 , 7.8967
2.0518618 , 1.7566
10.693377 , 10.6723
4.9584684 , 5.0936
8.45109 , 8.6198
17.447454 , 17.5204
RMSE:  0.5627195983770154  MAPE: 0.09145662872347274
5: ground truth total-  184  predicted total -  183
100: ground truth total-  225  predicted total -  225
 more 100: ground truth total -  2  predicted total -  2



evaluating data from wilson d-slash kernel
predicted_runtime, ground_truth
3.9081821 , 2.468374
5.294199 , 5.194488
4.165415 , 4.528264
4.6541843 , 0.265589
4.85441 , 2.266598
4.348032 , 7.319704
3.5351658 , 0.153871
4.489603 , 8.471678
5.3006687 , 2.841807
5.973276 , 4.042319
4.346018 , 0.22826
5.7403355 , 7.603322
4.9761753 , 6.13126
2.1493015 , 0.077513
2.9600315 , 0.116175
5.6651096 , 3.435019
4.718973 , 3.738982
4.2791233 , 1.678649
3.9814548 , 0.190473
RMSE:  2.6682843893309744  MAPE: 7.083482689199817
