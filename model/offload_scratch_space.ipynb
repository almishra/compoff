{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3725df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler as mm_scaler\n",
    "from sklearn.preprocessing import StandardScaler as std_scaler\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d11326",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompData(Dataset):\n",
    "    def __init__(self, X, y, train=True, scaler=True, task_num=3, num_sets=100, meta_train_batch=10, meta_test_batch=10, test_batch=32):\n",
    "        self.task_num = task_num       \n",
    "        self.scaler = scaler\n",
    "        if train:\n",
    "            self.meta_train_batch = meta_train_batch\n",
    "            self.meta_test_batch = meta_test_batch\n",
    "        else:\n",
    "            self.test_batch = test_batch\n",
    "        \n",
    "        if scaler:\n",
    "            X = mm_scaler().fit_transform(X)\n",
    "        \n",
    "        ### create sets. support_x has 10000 sets of 5/ 25 images each. Total ~1000 sets (for 1000 iterations)\n",
    "        ### create set with 32 rows (#meta_train_number) and 32 rows (#meta_test_number) and append to train and test lists\n",
    "        #convert pandas dataframe to numpy array\n",
    "        # print(type(X), type(y))\n",
    "        #X = X.to_numpy()\n",
    "        #y = y.to_numpy()\n",
    "        \n",
    "        entire_data = np.column_stack((X,y))\n",
    "        \n",
    "        #num_rows = len(X) # for index sampling\n",
    "        #self.selected_sample_indices = np.random.choice(num_rows,\\\n",
    "        #                                           size=task_num*(meta_train_batch+meta_test_batch),\\\n",
    "        #                                           replace=False)\n",
    "        \n",
    "        total_rows_required = num_sets*(meta_train_batch+meta_test_batch)\n",
    "        # we get shuffled data so directly pick total rows required\n",
    "        total_train_rows = entire_data[:total_rows_required,:]\n",
    "        \n",
    "        train_rows = total_train_rows[:num_sets*meta_train_batch,:]\n",
    "        test_rows = total_train_rows[num_sets*meta_train_batch:,:]\n",
    "        \n",
    "        #train_rows = np.hsplit(train_rows, num_sets)\n",
    "        #test_rows = np.hsplit(test_rows, num_sets)\n",
    "        \n",
    "        num_features = len(train_rows[0]) #### DEFINE\n",
    "        \n",
    "        ### final np arrays with data and runtimes for num_set rows \n",
    "        train_rows_data = train_rows[:, :num_features-1]\n",
    "        train_rows_runtime = train_rows[:,num_features-1:]\n",
    "        \n",
    "        test_rows_data = test_rows[:,:num_features-1]\n",
    "        test_rows_runtime = test_rows[:,num_features-1:]\n",
    "        \n",
    "        train_rows_data = np.vsplit(train_rows_data, num_sets)\n",
    "        train_rows_runtime = np.vsplit(train_rows_runtime, num_sets)\n",
    "        test_rows_data = np.vsplit(test_rows_data, num_sets)\n",
    "        test_rows_runtime = np.vsplit(test_rows_runtime, num_sets)\n",
    "        \n",
    "        #create sets here:\n",
    "        final_sets = [] ## list of list. each list row will have train_rows_data/runtime, test_data/runtime\n",
    "        for i in range(num_sets):\n",
    "            temp = [train_rows_data[i]]+[train_rows_runtime[i]]+[test_rows_data[i]]+[test_rows_runtime[i]]\n",
    "            final_sets.append(temp)\n",
    "        self.final_sets = final_sets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_sets)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "                \n",
    "        #zip sample without replacement from X\n",
    "        temp_store = self.final_sets[index]\n",
    "        train_row_data = temp_store[0]\n",
    "        train_row_runtime = temp_store[1]\n",
    "        test_row_data = temp_store[2]\n",
    "        test_row_runtime = temp_store[3]\n",
    "        \n",
    "        if np.asarray(train_row_data) is train_row_data:\n",
    "            train_row_data = np.asarray(train_row_data)\n",
    "            train_row_runtime = np.asarray(train_row_runtime)\n",
    "            test_row_data = np.asarray(test_row_data)\n",
    "            test_row_runtime = np.asarray(test_row_runtime)\n",
    "        \n",
    "        ## convert numpy array to torch tensor\n",
    "        #if not torch.is_tensor(X):\n",
    "        train_row_data = train_row_data.astype(np.float32)\n",
    "        train_row_runtime = train_row_runtime.astype(np.float32)\n",
    "        test_row_data = test_row_data.astype(np.float32)\n",
    "        test_row_runtime = test_row_runtime.astype(np.float32)\n",
    "        \n",
    "        train_row_data = torch.from_numpy(train_row_data)\n",
    "        train_row_runtime = torch.from_numpy(train_row_runtime)\n",
    "        test_row_data = torch.from_numpy(test_row_data)\n",
    "        test_row_runtime = torch.from_numpy(test_row_runtime)\n",
    "        #if not torch.is_tensor(y):\n",
    "        #    self.train_y = torch.from_numpy(y)               \n",
    "        \n",
    "        \n",
    "        return train_row_data, train_row_runtime, test_row_data, test_row_runtime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdff557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58ab5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### write parameter initialization in script:: kaiming or xavier; check c.b.finn's work\n",
    "class OffloadModel(torch.nn.Module):\n",
    "    def __init__(self, ip_features, num_hidden, op_features=1):\n",
    "        super(OffloadModel, self).__init__()\n",
    "        \n",
    "        self.mod1 = nn.Sequential(OrderedDict([\n",
    "            ('lin1', nn.Linear(ip_features, num_hidden)),\n",
    "            ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "        self.mod2 = nn.Sequential(OrderedDict([\n",
    "            ('lin2', nn.Linear(num_hidden,num_hidden*2)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('drop1', nn.Dropout(p=0.25)),          \n",
    "            ('lin3', nn.Linear(num_hidden*2, num_hidden)),\n",
    "            ('relu3', nn.ReLU())\n",
    "           \n",
    "        ]))\n",
    "        self.mod3 = nn.Sequential(OrderedDict([\n",
    "            ('lin4', nn.Linear(num_hidden, num_hidden)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('drop2', nn.Dropout(p=0.25)),\n",
    "            ('lin5', nn.Linear(num_hidden,op_features)),\n",
    "            ('sig1', nn.Sigmoid())             \n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        op = self.mod1(x)\n",
    "        x = self.mod2(op)\n",
    "        x += op\n",
    "        x = self.mod3(x)\n",
    "        return x\n",
    "    \n",
    "    def var_forward(self, x, weights):\n",
    "        op = F.relu(F.linear(x, weights[0], weights[1]))\n",
    "        x = F.relu(F.linear(op, weights[2], weights[3]))\n",
    "        x = F.dropout(x, p=0.25)\n",
    "        x = F.relu(F.linear(x, weights[4], weights[5]))\n",
    "        x += op\n",
    "        x = F.relu(F.linear(x, weights[6], weights[7]))\n",
    "        x = F.dropout(x, p=0.25)\n",
    "        x = F.sigmoid(F.linear(x, weights[8], weights[9]))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fef22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OffloadModel(\n",
       "  (mod1): Sequential(\n",
       "    (lin1): Linear(in_features=53, out_features=106, bias=True)\n",
       "    (relu1): ReLU()\n",
       "  )\n",
       "  (mod2): Sequential(\n",
       "    (lin2): Linear(in_features=106, out_features=212, bias=True)\n",
       "    (relu2): ReLU()\n",
       "    (drop1): Dropout(p=0.25, inplace=False)\n",
       "    (lin3): Linear(in_features=212, out_features=106, bias=True)\n",
       "    (relu3): ReLU()\n",
       "  )\n",
       "  (mod3): Sequential(\n",
       "    (lin4): Linear(in_features=106, out_features=106, bias=True)\n",
       "    (relu4): ReLU()\n",
       "    (drop2): Dropout(p=0.25, inplace=False)\n",
       "    (lin5): Linear(in_features=106, out_features=1, bias=True)\n",
       "    (sig1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet = OffloadModel(53,106)\n",
    "mnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903da0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 10, 106]           5,724\n",
      "              ReLU-2              [-1, 10, 106]               0\n",
      "            Linear-3              [-1, 10, 212]          22,684\n",
      "              ReLU-4              [-1, 10, 212]               0\n",
      "           Dropout-5              [-1, 10, 212]               0\n",
      "            Linear-6              [-1, 10, 106]          22,578\n",
      "              ReLU-7              [-1, 10, 106]               0\n",
      "            Linear-8              [-1, 10, 106]          11,342\n",
      "              ReLU-9              [-1, 10, 106]               0\n",
      "          Dropout-10              [-1, 10, 106]               0\n",
      "           Linear-11                [-1, 10, 1]             107\n",
      "          Sigmoid-12                [-1, 10, 1]               0\n",
      "================================================================\n",
      "Total params: 62,435\n",
      "Trainable params: 62,435\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mnet.to(device)\n",
    "summary(mnet, (10,53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1f2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_columns = ['kernel','Compiler','Cluster','gpu_name','outer','inner','var_decl','ref_expr','int_literal','float_literal','mem_to',\\\n",
    "            'mem_from','add_sub_int','add_sub_double','mul_int','mul_double','div_int','div_double','assign_int','assign_double']\n",
    "\n",
    "dataset_root=\"\"\n",
    "df = pd.read_csv(dataset_root+\"matrix_multiplication.csv\")   \n",
    "df = df.drop(columns=dr_columns)\n",
    "\n",
    "#sys.exit(\"please check the dataset path and file names\")\n",
    "\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "train_eval_split=0.6\n",
    "split_seed=43\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_eval_split, random_state=split_seed, shuffle=True)\n",
    "\n",
    "train_sets = CompData(X_train,y_train, scaler=False,train=True, task_num=3, num_sets=20, meta_train_batch=10, meta_test_batch=5)\n",
    "\n",
    "# print(train_sets.__getitem__(0))\n",
    "#test_sets = CompData(X_test, y_test, train=False, test_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3503677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls = DataLoader(train_sets, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711ac773",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_factor = 0.1\n",
    "def train(model, x_train, y_train, x_test, y_test):\n",
    "    original_model_copy = copy.deepcopy(model)\n",
    "    loss_tasks = 0\n",
    "    for k in range(task_num):\n",
    "        print(k)\n",
    "        temp_weights=[w.clone() for w in list(original_model_copy.parameters())]\n",
    "        \n",
    "        outputs = original_model_copy.var_forward(x_train[k], temp_weights)\n",
    "        loss = criterion(outputs, y_train[k])\n",
    "        #print(type(y_train[k]))\n",
    "        #print(loss, type(loss))\n",
    "        #print(type(loss), type(temp_weights))\n",
    "        grad = torch.autograd.grad(loss, temp_weights)\n",
    "        # temporary update weights \n",
    "        temp_weights = [w - update_factor*g for w,g in zip(temp_weights, grad)]\n",
    "        \n",
    "        ## run updated weights on meta-test batch\n",
    "        new_outputs = original_model_copy.var_forward(x_test[k], temp_weights)\n",
    "        new_loss = criterion(new_outputs, y_test[k])\n",
    "        \n",
    "        loss_tasks += new_loss\n",
    "    \n",
    "    return loss_tasks        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd55fe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outer_epochs = 10\n",
    "task_num = 3\n",
    "global_model = OffloadModel(53,106)\n",
    "meta_optim = torch.optim.Adam(global_model.parameters(), lr=1e-3)\n",
    "#### add some lr decay: cosine or step or lambda\n",
    "global_model = global_model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for idx in range(outer_epochs):\n",
    "    train_set_loader = DataLoader(train_sets, batch_size=task_num, drop_last=True)\n",
    "    for i, (x_train, y_train, x_test, y_test) in enumerate(train_set_loader):\n",
    "        # print((x_train[0]))\n",
    "        task_num_, set_size, cols = x_train.shape #<--verify\n",
    "        #print(task_num_, set_size, cols)\n",
    "        x_train, y_train, x_test, y_test = x_train.to(device), y_train.to(device), x_test.to(device), y_test.to(device)\n",
    "        \n",
    "        # print(type(x_train))\n",
    "        ### train should return and accuracies???\n",
    "        total_loss = train(global_model, x_train, y_train, x_test, y_test) #<-- returns loss\n",
    "        \n",
    "        meta_optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        meta_optim.step()\n",
    "    \n",
    "        ### do some validation or call the actual test function? (diff data set & loader)\n",
    "        ### no grad calculation in whatever that happens next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c9a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e642d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OffloadModel(\n",
       "  (hidden1): Linear(in_features=53, out_features=106, bias=True)\n",
       "  (hidden2): Linear(in_features=106, out_features=212, bias=True)\n",
       "  (hidden3): Linear(in_features=212, out_features=106, bias=True)\n",
       "  (hidden4): Linear(in_features=106, out_features=106, bias=True)\n",
       "  (op_run): Linear(in_features=106, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet = OffloadModel(53,106)\n",
    "mnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "xip = torch.randn((10,53), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde86b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 10, 106]           5,724\n",
      "            Linear-2              [-1, 10, 212]          22,684\n",
      "           Dropout-3              [-1, 10, 212]               0\n",
      "            Linear-4              [-1, 10, 106]          22,578\n",
      "            Linear-5              [-1, 10, 106]          11,342\n",
      "           Dropout-6              [-1, 10, 106]               0\n",
      "            Linear-7                [-1, 10, 1]             107\n",
      "================================================================\n",
      "Total params: 62,435\n",
      "Trainable params: 62,435\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.30\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Smeet\\anaconda3\\envs\\offload\\lib\\site-packages\\torch\\nn\\functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "summary(mnet, (10,53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43265f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.column_stack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320cd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#### Added to dataset_other on origin-local\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler as mm_scaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class CompData(Dataset):\n",
    "    def __init__(self, X, y, train=True, scaler=True, task_num=3, num_sets=100. meta_train_batch=10, meta_test_batch=10, test_batch=32):\n",
    "        self.task_num = task_num\n",
    "        self.scaler = scaler\n",
    "        if train:\n",
    "            self.meta_train_batch = meta_train_batch\n",
    "            self.meta_test_batch = meta_test_batch\n",
    "        else:\n",
    "            self.test_batch = test_batch\n",
    "\n",
    "        if scaler:\n",
    "            X = mm_scaler().fit_transform(X)\n",
    "\n",
    "        ### create sets. support_x has 10000 sets of 5/ 25 images each. Total ~1000 sets (for 1000 iterations)\n",
    "        ### create set with 32 rows (#meta_train_number) and 32 rows (#meta_test_number) and append to train and test lists\n",
    "        #convert pandas dataframe to numpy array\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        entire_data = np.hstack((X,y))\n",
    "\n",
    "        #num_rows = len(X) # for index sampling\n",
    "        #self.selected_sample_indices = np.random.choice(num_rows,\\\n",
    "        #                                           size=task_num*(meta_train_batch+meta_test_batch),\\\n",
    "        #                                           replace=False)\n",
    "\n",
    "        total_rows_required = num_sets*(meta_train_batch+meta_test_batch)\n",
    "        # we get shuffled data so directly pick total rows required\n",
    "        total_train_rows = entire_data[:total_rows_required]\n",
    "\n",
    "        train_rows = total_train_rows[:num_sets*meta_train_batch]\n",
    "        test_rows = total_train_rows[num_threats*meta_train_batch:]\n",
    "\n",
    "        train_rows = np.hsplit(train_rows, num_sets)\n",
    "        test_rows = np.hsplit(test_rows, num_sets)\n",
    "\n",
    "        num_features = len(train_rows[0])#### DEFINE\n",
    "\n",
    "        ### final np arrays with data and runtimes for num_set rows\n",
    "        train_rows_data = train_rows[:, :num_features-1]\n",
    "        train_rows_runtime = train_rows[:,num_features-1:]\n",
    "\n",
    "        test_rows_data = test_rows[:,:num_features-1]\n",
    "        test_rows_runtime = test_rows[:,num_features-1:]\n",
    "        #create sets here:\n",
    "        final_sets = [] ## list of list. each list row will have train_rows_data/runtime, test_data/runtime\n",
    "        for j in range(num_tasks):\n",
    "            tr_row_data = list()\n",
    "            tr_row_run = list()\n",
    "            te_row_data = list()\n",
    "            te_row_run - list()\n",
    "            for i in range(num_sets):\n",
    "                tr_row_data.append(train_rows_data[i])\n",
    "                tr_row_run.append(train_rows_runtime[i])\n",
    "                te_row_data.append(test_rows_data[i])\n",
    "                te_row_run.append(test_rows_runtime[i])\n",
    "\n",
    "            temp = [tr_row_data]+[tr_row_run]+[te_row_data]]+[te_row_run]\n",
    "            final_sets.append(temp)\n",
    "\n",
    "        self.final_sets = final_sets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.final_sets)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        #zip sample without replacement from X\n",
    "        #for i in range(self.num_tasks):\n",
    "        train_row_data = self.final_sets[index,:,0]\n",
    "        train_row_runtime = self.final_sets[index,:,1]\n",
    "        test_row_data = self.final_sets[index,:,2]\n",
    "        test_row_runtime = self.final_set[index,:,3]\n",
    "\n",
    "        #for i in range(self.num_tasks):\n",
    "        #if np.asarray(train_row_data) is train_row_data:\n",
    "        train_row_data = np.asarray(train_row_data)\n",
    "        train_row_runtime = np.asarray(train_row_runtime)\n",
    "        test_row_data = np.asarray(test_row_data)\n",
    "        test_row_runtime = np.asarray(test_row_runtime)\n",
    "\n",
    "        ## convert numpy array to torch tensor\n",
    "        #if not torch.is_tensor(X):\n",
    "        train_row_data = torch.from_numpy(train_row_data)\n",
    "        train_row_runtime = torch.from_numpy(train_row_runtime)\n",
    "        test_row_data = torch.from_numpy(test_row_data)\n",
    "        test_row_runtime = torch.from_numpy(test_row_runtime)\n",
    "        #if not torch.is_tensor(y):\n",
    "        #    self.train_y = torch.from_numpy(y)\n",
    "\n",
    "\n",
    "        return train_row_data, train_row_runtime, test_row_data, test_row_runtime\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
